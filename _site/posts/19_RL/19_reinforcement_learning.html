<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Derek Sollberger">
<meta name="dcterms.date" content="2025-04-07">

<title>19: Reinforcement Learning – SML 301 Slides</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">SML 301 Slides</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Lecture Sessions</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#session-19-reinforcement-learning" id="toc-session-19-reinforcement-learning" class="nav-link active" data-scroll-target="#session-19-reinforcement-learning">Session 19: Reinforcement Learning</a>
  <ul class="collapse">
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link" data-scroll-target="#learning-objectives">Learning objectives:</a></li>
  </ul></li>
  <li><a href="#rl-in-the-news" id="toc-rl-in-the-news" class="nav-link" data-scroll-target="#rl-in-the-news">RL in the News</a></li>
  <li><a href="#gym-mountain-car" id="toc-gym-mountain-car" class="nav-link" data-scroll-target="#gym-mountain-car">Gym: Mountain Car</a></li>
  <li><a href="#case-study-where-to-eat" id="toc-case-study-where-to-eat" class="nav-link" data-scroll-target="#case-study-where-to-eat">Case Study: Where to Eat?</a>
  <ul class="collapse">
  <li><a href="#neural-network" id="toc-neural-network" class="nav-link" data-scroll-target="#neural-network">Neural Network</a></li>
  <li><a href="#forward-propagation" id="toc-forward-propagation" class="nav-link" data-scroll-target="#forward-propagation">Forward Propagation</a></li>
  <li><a href="#cross-entropy" id="toc-cross-entropy" class="nav-link" data-scroll-target="#cross-entropy">Cross Entropy</a></li>
  <li><a href="#chain-rule" id="toc-chain-rule" class="nav-link" data-scroll-target="#chain-rule">Chain Rule</a></li>
  <li><a href="#rewards-1" id="toc-rewards-1" class="nav-link" data-scroll-target="#rewards-1">Rewards</a></li>
  <li><a href="#update" id="toc-update" class="nav-link" data-scroll-target="#update">Update</a></li>
  <li><a href="#second-epoch" id="toc-second-epoch" class="nav-link" data-scroll-target="#second-epoch">Second Epoch</a></li>
  <li><a href="#many-epochs" id="toc-many-epochs" class="nav-link" data-scroll-target="#many-epochs">Many Epochs</a></li>
  </ul></li>
  <li><a href="#sar" id="toc-sar" class="nav-link" data-scroll-target="#sar">SAR</a></li>
  <li><a href="#mdp" id="toc-mdp" class="nav-link" data-scroll-target="#mdp">MDP</a>
  <ul class="collapse">
  <li><a href="#return" id="toc-return" class="nav-link" data-scroll-target="#return">Return</a></li>
  </ul></li>
  <li><a href="#ethics-corner-training-data-with-human-subjects" id="toc-ethics-corner-training-data-with-human-subjects" class="nav-link" data-scroll-target="#ethics-corner-training-data-with-human-subjects">Ethics Corner: Training Data with Human Subjects</a></li>
  <li><a href="#bellman-equations" id="toc-bellman-equations" class="nav-link" data-scroll-target="#bellman-equations">Bellman Equations</a>
  <ul class="collapse">
  <li><a href="#state-and-action-functions" id="toc-state-and-action-functions" class="nav-link" data-scroll-target="#state-and-action-functions">State and Action Functions</a></li>
  <li><a href="#optimal-policy" id="toc-optimal-policy" class="nav-link" data-scroll-target="#optimal-policy">Optimal Policy</a></li>
  <li><a href="#forming-the-bellman-equations" id="toc-forming-the-bellman-equations" class="nav-link" data-scroll-target="#forming-the-bellman-equations">Forming the Bellman Equations</a></li>
  </ul></li>
  <li><a href="#policy-improvement" id="toc-policy-improvement" class="nav-link" data-scroll-target="#policy-improvement">Policy Improvement</a>
  <ul class="collapse">
  <li><a href="#definition" id="toc-definition" class="nav-link" data-scroll-target="#definition">Definition</a></li>
  <li><a href="#gpi" id="toc-gpi" class="nav-link" data-scroll-target="#gpi">GPI</a></li>
  </ul></li>
  <li><a href="#gym-frozen-lake" id="toc-gym-frozen-lake" class="nav-link" data-scroll-target="#gym-frozen-lake">Gym: Frozen Lake</a></li>
  <li><a href="#quo-vadimus" id="toc-quo-vadimus" class="nav-link" data-scroll-target="#quo-vadimus">Quo Vadimus?</a></li>
  <li><a href="#footnotes" id="toc-footnotes" class="nav-link" data-scroll-target="#footnotes">Footnotes</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">19: Reinforcement Learning</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Derek Sollberger </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 7, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="session-19-reinforcement-learning" class="level1">
<h1>Session 19: Reinforcement Learning</h1>
<section id="learning-objectives" class="level2">
<h2 class="anchored" data-anchor-id="learning-objectives">Learning objectives:</h2>
<div class="columns">
<div class="column" style="width:45%;">
<ul>
<li>Introduce reinforcement learning</li>
<li>Discuss Markov Decision Processes</li>
</ul>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:45%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="RL_robot.png" class="img-fluid figure-img"></p>
<figcaption>Reinforcement Learning</figcaption>
</figure>
</div>
<ul>
<li>image credit: <a href="https://databasecamp.de/en/ml/q-learnings">Data Base Camp</a></li>
</ul>
</div>
</div>
</section>
</section>
<section id="rl-in-the-news" class="level1">
<h1>RL in the News</h1>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">Checkers</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">Chess</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-3" role="tab" aria-controls="tabset-1-3" aria-selected="false">Go</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-4-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-4" role="tab" aria-controls="tabset-1-4" aria-selected="false">Starcraft</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="columns">
<div class="column" style="width:45%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="checkers_1994.png" class="img-fluid figure-img"></p>
<figcaption>Chinook, 1994</figcaption>
</figure>
</div>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:45%;">
<p><a href="https://www.ijcai.org/Proceedings/05/Papers/0515.pdf">Solving Checkers</a> by Schaeffer, et al., 2005</p>
</div>
</div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="columns">
<div class="column" style="width:45%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="chess_1997.png" class="img-fluid figure-img"></p>
<figcaption>Deep Blue, 1997</figcaption>
</figure>
</div>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:45%;">
<p><a href="https://www.chess.com/terms/deep-blue-chess-computer">Deep Blue</a> defeated Garry Kasparov in 1997</p>
</div>
</div>
</div>
<div id="tabset-1-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-3-tab">
<div class="columns">
<div class="column" style="width:45%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="go_2016.png" class="img-fluid figure-img"></p>
<figcaption>AlphaGo, 2016</figcaption>
</figure>
</div>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:45%;">
<p><a href="https://gomagic.org/alphago-and-lee-sedol/">AlphaGo</a> defeated Lee Sedol in 2016</p>
</div>
</div>
</div>
<div id="tabset-1-4" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-4-tab">
<div class="columns">
<div class="column" style="width:45%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="alphastar_2019.png" class="img-fluid figure-img"></p>
<figcaption>AlphaStar, 2019</figcaption>
</figure>
</div>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:45%;">
<p><a href="https://deepmind.google/discover/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii/">AlphaStar</a> won competitive matches against LiquidTLO in 2019.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="gym-mountain-car" class="level1">
<h1>Gym: Mountain Car</h1>
<div class="columns">
<div class="column" style="width:45%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mountain_car.gif" class="img-fluid figure-img"></p>
<figcaption>Mountain Car</figcaption>
</figure>
</div>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:45%;">
<ul>
<li>states: <span class="math inline">\(x\)</span> coordinate</li>
<li>actions: push left or push right</li>
<li>reward: given upon reaching top of hill</li>
</ul>
</div>
</div>
</section>
<section id="case-study-where-to-eat" class="level1">
<h1>Case Study: Where to Eat?</h1>
<div class="columns">
<div class="column" style="width:35%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="food_choices.png" class="img-fluid figure-img"></p>
<figcaption>food choices</figcaption>
</figure>
</div>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:55%;">
<ul>
<li><span class="math inline">\(F\)</span>: choosing to eat at Frist Campus Center</li>
<li><span class="math inline">\(R\)</span>: choosing to eat at your residential college</li>
<li>Input: hunger</li>
</ul>
<p><span class="math display">\[H \in [0,1]\]</span></p>
<ul>
<li><p>Reward: net satisfaction</p>
<ul>
<li>meal size</li>
<li>distance to food</li>
</ul></li>
</ul>
<p>Example adapted from examples by Professor Josh Starmer</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What if we don’t already have training data?
</div>
</div>
<div class="callout-body-container callout-body">
<p>In previous sessions, we trained a model with training data. In particular, we were able to compute a loss function (such as the difference between predictions and observed outputs).</p>
<p>Here, instead, we will use <strong>reinforcement learning</strong> to employ <em>rewards</em> to help provide signals for the backpropagation.</p>
</div>
</div>
<section id="neural-network" class="level2">
<h2 class="anchored" data-anchor-id="neural-network">Neural Network</h2>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR

input[hunger]
activation[sigmoid]
output[residential college]

input -- wH + b --&gt; activation
activation --&gt; output
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<ul>
<li>linear transformation: <span class="math inline">\(L = wh + b\)</span> in the fully connected layer</li>
<li>output: probability of choosing to eat at your residential college (instead of Frist)</li>
<li>learning rate (hyperparameter): 0.301</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Sigmoid instead of ReLU?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Here, I am using the sigmoid activation function instead of the ReLU simply because it is easier in the slides presentation.</p>
<ul>
<li>sigmoid</li>
</ul>
<p><span class="math display">\[A(x) = \frac{1}{1 + e^{-x}} \text{ with }\frac{dA}{dx} = \sigma(x)(1 - \sigma(x))\]</span> * ReLU</p>
<p><span class="math display">\[f(x) = \text{max}(x,0) \text{ with } f'(x) = \begin{cases} 1, &amp; x &gt; 0 \\ 0, &amp; x \leq 0 \end{cases}\]</span></p>
</div>
</div>
</div>
</section>
<section id="forward-propagation" class="level2">
<h2 class="anchored" data-anchor-id="forward-propagation">Forward Propagation</h2>
<p>Suppose that you are not hungry (<span class="math inline">\(H = 0\)</span>) but you still want to eat some dinner at this time.</p>
<ul>
<li>initialized as <span class="math inline">\(w = 20\)</span> and <span class="math inline">\(b = 0\)</span></li>
<li>we will train for the bias value</li>
<li>we don’t know the meal satisfaction in advance</li>
</ul>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR

input[H = 0]
activation[sigmoid]
output[residential college]

input -- 20H + 0 --&gt; activation
activation --&gt; output
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>So far, our <em>distribution</em> of choices is</p>
<ul>
<li><span class="math inline">\(P(\text{residential college}) = A(0) = \frac{1}{1 + e^{-0}} = 0.5\)</span></li>
<li><span class="math inline">\(P(\text{Frist}) = 1 - P(\text{residential college}) = 0.5\)</span></li>
</ul>
<p>whose <span class="math inline">\([0,1]\)</span> probability space can be mapped as</p>
<ul>
<li><span class="math inline">\([0, 0.5)\)</span>: go to Frist</li>
<li><span class="math inline">\((0.5, 1.0]\)</span>: go to your residential college</li>
</ul>
<p>Now, suppose that we used a random number generator and obtained 0.2025. Thus, we are visiting Frist.</p>
</section>
<section id="cross-entropy" class="level2">
<h2 class="anchored" data-anchor-id="cross-entropy">Cross Entropy</h2>
<p>We will use <em>cross entropy</em> to measure the loss.</p>
<p><span class="math display">\[C_{\text{res}} = -\ln(P(\text{res}))\]</span> <span class="math display">\[C_{\text{Frist}} = -\ln(1 - P(\text{res}))\]</span></p>
</section>
<section id="chain-rule" class="level2">
<h2 class="anchored" data-anchor-id="chain-rule">Chain Rule</h2>
<p>Toward using the output to train the bias, we examine the chain rule to apply the change in the cross entropy with respect to the bias.</p>
<p><span class="math display">\[\begin{array}{rcl}
\frac{dC_{\text{Frist}}}{db} &amp; = &amp; \frac{dC_{\text{Frist}}}{dP(\text{res})} \cdot \frac{dP(\text{res})}{dL} \cdot \frac{dL}{db} \\
~ &amp; = &amp; \frac{1}{1 - P(\text{res})} \cdot P(\text{res}) \cdot (1 - P(\text{res})) \cdot (1) \\
~ &amp; = &amp; 0.5 \\
\end{array}\]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Signs
</div>
</div>
<div class="callout-body-container callout-body">
<p>If we were training a simple neural network</p>
<ul>
<li>negative derivative –&gt; decrease parameter</li>
<li>positive derivative –&gt; increase parameter</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Rewards
</div>
</div>
<div class="callout-body-container callout-body">
<p>However, we need to incorporate the possible <strong>rewards</strong></p>
<p><span class="math display">\[\text{updated derivative} = \text{derivative}*\text{reward}\]</span></p>
</div>
</div>
</section>
<section id="rewards-1" class="level2">
<h2 class="anchored" data-anchor-id="rewards-1">Rewards</h2>
<p>In this scenario, let us assign the <strong>rewards</strong> as follows</p>
<ul>
<li><p>If hunger = 0 and we choose Frist: reward = 1.0</p>
<ul>
<li>perhaps valued location over quantity of food</li>
</ul></li>
<li><p>If hunger = 0 and we choose the residential college: reward = -1.0</p></li>
</ul>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
How are reward amounts chosen?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For now, we note that reward amounts do not have to be <span class="math inline">\(\pm 1.0\)</span>. Amounts can be <em>weighted</em> to give more priority to certain outcomes.</p>
</div>
</div>
</div>
<p>Thus, in our scenario here, our updated derivative is</p>
<p><span class="math display">\[\begin{array}{rcl}
\text{updated derivative} &amp; = &amp; \text{derivative}*\text{reward} \\
~ &amp; = &amp; (0.5)(1.0) \\
~ &amp; = &amp; 0.5
\end{array}\]</span></p>
</section>
<section id="update" class="level2">
<h2 class="anchored" data-anchor-id="update">Update</h2>
<p><span class="math display">\[\begin{array}{rcl}
\text{step size} &amp; = &amp;\text{learning rate}*\text{updated derivative} \\
~ &amp; = &amp; (0.301)(0.5) \\
~ &amp; = &amp; 0.1505 \\
\end{array}\]</span></p>
<p><span class="math display">\[\begin{array}{rcl}
\text{updated bias} &amp; = &amp; \text{old bias} - \text{step size} \\
~ &amp; = &amp; 0.0 - 0.1505 \\
~ &amp; = &amp; -0.1505 \\
\end{array}\]</span></p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR

input[H = 0]
activation[sigmoid]
output[residential college]

input -- 20H - 0.1505 --&gt; activation
activation --&gt; output
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>Now, our distribution of choices is</p>
<ul>
<li><span class="math inline">\(P(\text{residential college}) = A(-0.1505) = \frac{1}{1 + e^{0.1505}} \approx 0.4624\)</span></li>
<li><span class="math inline">\(P(\text{Frist}) = 1 - P(\text{residential college}) \approx 0.5376\)</span></li>
</ul>
<p>whose <span class="math inline">\([0,1]\)</span> probability space can be mapped as</p>
<ul>
<li><span class="math inline">\([0, 0.5376)\)</span>: go to Frist</li>
<li><span class="math inline">\((0.5376, 1.0]\)</span>: go to your residential college</li>
</ul>
</section>
<section id="second-epoch" class="level2">
<h2 class="anchored" data-anchor-id="second-epoch">Second Epoch</h2>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true">Start</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false">CE</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-3" role="tab" aria-controls="tabset-2-3" aria-selected="false">Rewards</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-4-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-4" role="tab" aria-controls="tabset-2-4" aria-selected="false">Update</a></li></ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<p>Now, suppose that we used a random number generator and obtained 0.678. Thus, we are visiting your residential college.</p>
<ul>
<li><span class="math inline">\(P(\text{residential college}) = A(-0.1505) = \frac{1}{1 + e^{0.1505}} \approx 0.4624\)</span></li>
<li><span class="math inline">\(P(\text{Frist}) = 1 - P(\text{residential college}) \approx 0.5376\)</span></li>
</ul>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<p><span class="math display">\[\begin{array}{rcl}
\frac{dC_{\text{res}}}{db} &amp; = &amp; \frac{dC_{\text{res}}}{dP(\text{res})} \cdot \frac{dP(\text{res})}{dL} \cdot \frac{dL}{db} \\
~ &amp; = &amp; \frac{-1}{P(\text{res})} \cdot P(\text{res}) \cdot (1 - P(\text{res})) \cdot (1) \\
~ &amp; = &amp; -0.5376 \\
\end{array}\]</span></p>
</div>
<div id="tabset-2-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-3-tab">
<p>But eating at the residential college when you are not hungry was assigned a reward value of -1.0.</p>
<p><span class="math display">\[\begin{array}{rcl}
\text{updated derivative} &amp; = &amp; \text{derivative}*\text{reward} \\
~ &amp; = &amp; (-0.5376)(-1.0) \\
~ &amp; = &amp; 0.5376
\end{array}\]</span></p>
</div>
<div id="tabset-2-4" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-4-tab">
<p><span class="math display">\[\begin{array}{rcl}
\text{step size} &amp; = &amp;\text{learning rate}*\text{updated derivative} \\
~ &amp; = &amp; (0.301)(0.5376) \\
~ &amp; \approx &amp; 0.1618 \\
\end{array}\]</span></p>
<p><span class="math display">\[\begin{array}{rcl}
\text{updated bias} &amp; = &amp; \text{old bias} - \text{step size} \\
~ &amp; = &amp; -0.1505 - 0.1618 \\
~ &amp; = &amp; -0.3123 \\
\end{array}\]</span></p>
</div>
</div>
</div>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR

input[H = 0]
activation[sigmoid]
output[residential college]

input -- 20H - 0.3123 --&gt; activation
activation --&gt; output
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>After the second epoch, our distribution of choices is</p>
<ul>
<li><span class="math inline">\(P(\text{residential college}) = A(-0.3123) = \frac{1}{1 + e^{0.3123}} \approx 0.4226\)</span></li>
<li><span class="math inline">\(P(\text{Frist}) = 1 - P(\text{residential college}) \approx 0.5774\)</span></li>
</ul>
<p>whose <span class="math inline">\([0,1]\)</span> probability space can be mapped as</p>
<ul>
<li><span class="math inline">\([0, 0.5774)\)</span>: go to Frist</li>
<li><span class="math inline">\((0.5774, 1.0]\)</span>: go to your residential college</li>
</ul>
<p>That is, in situations without hunger, your chance of choosing dining at your residential college is <em>decreasing</em>.</p>
</section>
<section id="many-epochs" class="level2">
<h2 class="anchored" data-anchor-id="many-epochs">Many Epochs</h2>
<p>Assuming that we can get data from many dinners (and/or many students), the neural network would be trained over</p>
<ul>
<li>many epochs</li>
<li>many values for hunger (<span class="math inline">\(H \in [0,1]\)</span>)</li>
</ul>
<p>and would perhaps converage toward</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR

input[H]
activation[sigmoid]
output[residential college]

input -- 20H - 10 --&gt; activation
activation --&gt; output
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p><span class="math display">\[\begin{array}{rcl}
  H \in [0, 0.5) &amp; \rightarrow &amp; \text{prefer Frist} \\
  H \in (0.5, 1.0] &amp; \rightarrow &amp; \text{prefer residential college} \\
\end{array}\]</span></p>
</section>
</section>
<section id="sar" class="level1">
<h1>SAR</h1>
<p>To rigorously formulate the concepts of reinforcement learning, we think of states, actions, and rewards.</p>
<p>For some terminology,</p>
<ul>
<li><span class="math inline">\(\mathcal{S}\)</span>: set of states</li>
<li><span class="math inline">\(\mathcal{A}\)</span>: set of actions</li>
<li><em>distribution</em>: <span class="math inline">\(P(s'|s,a)\)</span></li>
<li><em>reward</em>: <span class="math inline">\(r(s,a)\)</span></li>
</ul>
</section>
<section id="mdp" class="level1">
<h1>MDP</h1>
<p>Together we have a <strong>Markov decision process</strong> (MDP).</p>
<p><span class="math display">\[\text{MDP}: (\mathcal{S}, \mathcal{A}, T, r)\]</span></p>
<p>Starting at an <em>initial state</em> <span class="math inline">\(s_{0}\)</span>, a <strong>trajectory</strong> happens over time <span class="math inline">\(t\)</span></p>
<p><span class="math display">\[t \in \{0, 1, 2, ... , T\}\]</span></p>
<p>(where <span class="math inline">\(T\)</span> is the <strong>terminal time</strong>) and looks like</p>
<p><span class="math display">\[\tau = (s_{0}, a_{0}, r_{0}, s_{1}, a_{1}, r_{1}, s_{2}, a_{2}, r_{2}, ...)\]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Markov assumption
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Recall at for a Markov process, we assume that the present iteration (i.e.&nbsp;its probabilities) depend only on the previous iteration</p>
<p><span class="math display">\[P(s_{t}|s_{t-1}, a_{t-1})\]</span></p>
</div>
</div>
</div>
<p>In terms of random variables, the trajectories are</p>
<p><span class="math display">\[\tau \in (S_{0}, A_{0}, R_{0}, S_{1}, A_{1}, R_{1}, S_{2}, A_{2}, R_{2}, ...)\]</span></p>
<section id="return" class="level2">
<h2 class="anchored" data-anchor-id="return">Return</h2>
<p>The <strong>return</strong> of a trajectory is the total of the rewards</p>
<p><span class="math display">\[r_{0} + \gamma r_{1} + \gamma^{2}r_{2} + \gamma_{3}r_{3} + ... = \displaystyle\sum_{t = 0}^{T} \gamma^{t}r_{t}\]</span></p>
<p>where <span class="math inline">\(\gamma \in [0,1]\)</span> is the <strong>discount rate</strong>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Goal
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <strong>goal</strong> in reinforcement learning is to seek out a <strong>policy</strong> <span class="math inline">\(\pi\)</span> that maximizes the return.</p>
</div>
</div>
<p>From a later moment in time (i.e.&nbsp;not the initial state), we can think of the return as</p>
<p><span class="math display">\[G_{t} = \displaystyle\sum_{k = t+1}^{T} \gamma^{k - t - 1}R_{k}\]</span></p>
<p>and then the goal can be expressed as</p>
<p><span class="math display">\[\text{max}_{\pi} \text{E}_{\pi}[G_{t}]\]</span></p>
</section>
</section>
<section id="ethics-corner-training-data-with-human-subjects" class="level1">
<h1>Ethics Corner: Training Data with Human Subjects</h1>
<div class="columns">
<div class="column" style="width:45%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ai_xray_bias.png" class="img-fluid figure-img"></p>
<figcaption>AI x-ray bias</figcaption>
</figure>
</div>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:45%;">
<p><em>AI models miss disease in Black and female patients</em></p>
<ul>
<li><a href="https://www.science.org/content/article/ai-models-miss-disease-black-female-patients">Science Magazine</a>, March 26, 2025</li>
</ul>
</div>
</div>
</section>
<section id="bellman-equations" class="level1">
<h1>Bellman Equations</h1>
<section id="state-and-action-functions" class="level2">
<h2 class="anchored" data-anchor-id="state-and-action-functions">State and Action Functions</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
State Value Function
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math display">\[v_{\pi}(s) = \text{E}[G_{t}|S_{t} = s]\]</span></p>
<p>is the value of being at state <span class="math inline">\(s\)</span>, and is defined as the expected value of the return given that we are at state <span class="math inline">\(s\)</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Action Value Function
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math display">\[q_{\pi}(s,a) = \text{E}[G_{t}|S_{t} = s, A_{t} = a]\]</span></p>
<p>is the expected return given that we are at state <span class="math inline">\(s\)</span> and take action <span class="math inline">\(a\)</span>.</p>
</div>
</div>
</section>
<section id="optimal-policy" class="level2">
<h2 class="anchored" data-anchor-id="optimal-policy">Optimal Policy</h2>
<p>There exists an optimal policy <span class="math inline">\(\pi_{*}\)</span> where</p>
<p><span class="math display">\[\begin{array}{rclc}
  v_{\pi_{*}}(s) &amp; \geq &amp; v_{\pi}(s) &amp; \forall s \forall \pi \\
  q_{\pi_{*}}(s,a) &amp; \geq &amp; q_{\pi}(s,a) &amp; \forall s \forall a \forall \pi \\
\end{array}\]</span></p>
<p>To iterate toward an optimal policy, we try dynamic programming.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Dynamic Programming
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Here, dynamic programming will assume <em>complete</em> knowledge of the Markov decision process, where</p>
<ul>
<li>state space <span class="math inline">\(S\)</span> is finite</li>
<li>state space is discrete</li>
<li>temporal space is finite (i.e.&nbsp;<span class="math inline">\(T &lt; \infty\)</span>)</li>
</ul>
<p>Dynamic programming can rely on knowing distribution</p>
<p><span class="math display">\[P(s', r| s,a)\]</span></p>
</div>
</div>
</div>
<p><em>Bellman optimality</em> says that the agent must choose an action that has the maximum value. Computing total probabilities yields</p>
</section>
<section id="forming-the-bellman-equations" class="level2">
<h2 class="anchored" data-anchor-id="forming-the-bellman-equations">Forming the Bellman Equations</h2>
<p><span class="math display">\[\begin{array}{c|rcl}
  (1) &amp; v_{\pi}(s) &amp; = &amp; \displaystyle\sum_{a\in\mathcal{A}(s)} \pi(a|s)q_{\pi}(s,a) \\
  (2) &amp; q_{\pi}(s,a) &amp; = &amp; \displaystyle\sum_{\begin{array}{c} s'\in\mathcal{S} \\ r\in\mathcal{R}\end{array}} p(s',r|s,a)[r + \gamma v_{\pi}(s')]
\end{array}\]</span></p>
<ul>
<li>If we substitute (2) into (1), we can form the <strong>Bellman Equation for state values</strong> that relates any state value from any state value one step away:</li>
</ul>
<p><span class="math display">\[v_{\pi}(s) = \displaystyle\sum_{a\in\mathcal{A}(s)} \pi(a|s)\displaystyle\sum_{\begin{array}{c} s'\in\mathcal{S} \\ r\in\mathcal{R}\end{array}} p(s',r|s,a)[r + \gamma v_{\pi}(s')]\]</span></p>
<ul>
<li>If we substitute (1) into (2), we can form the <strong>Bellman Equation for action values</strong> that relates any action value to any action value one step away:</li>
</ul>
<p><span class="math display">\[q_{\pi}(s,a) = \displaystyle\sum_{\begin{array}{c} s'\in\mathcal{S} \\ r\in\mathcal{R}\end{array}} p(s',r|s,a)\left[r + \gamma \displaystyle\sum_{a'\in\mathcal{A}(s')} \pi(a'|s')q_{\pi}(s',a')\right] \]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Bellman’s contribution
</div>
</div>
<div class="callout-body-container callout-body">
<p>For a Markov decision process (MDP) where we know the distribution of possible states, the Bellman equations above all us to compute any state value or any action value from all of the other information.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Recursion
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The derivation of equation (2) above involves how the returns</p>
<p><span class="math display">\[G_{t} = R_{t+1} + \gamma G_{t+1}\]</span> Then, the expected values are</p>
<p><span class="math display">\[\begin{array}{rcl}
\text{E}[G_{t}|s,a] &amp; = &amp; \text{E}[R_{t+1} + \gamma G_{t+1}|s,a] \\
~ &amp; = &amp; \text{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1})|s,a] \\
\end{array}\]</span></p>
<p>One cannot say that distributions <span class="math inline">\(G_{t+1} = v_{\pi}(S_{t+1})\)</span>, but we are allowed to equate the expected values!</p>
</div>
</div>
</div>
</section>
</section>
<section id="policy-improvement" class="level1">
<h1>Policy Improvement</h1>
<section id="definition" class="level2">
<h2 class="anchored" data-anchor-id="definition">Definition</h2>
<p>If we have the state value function <span class="math inline">\(v_{\pi}(s)\)</span>, and assuming a deterministic policy (i.e.&nbsp;<span class="math inline">\(a = \pi(s)\)</span>), an optimal policy is defined as</p>
<p><span class="math display">\[\pi_{*}(s) = \text{argmax}_{a} q_{*}(s,a)\]</span> We define a new policy <span class="math inline">\(\pi'\)</span> as</p>
<p><span class="math display">\[\pi'(s) = \text{argmax}_{a} q_{\pi}(s,a)\]</span></p>
<p>and due to the Policy Improvement Theorem, we are assured that</p>
<p><span class="math display">\[v_{\pi'}(s) \geq v_{\pi}(s) \quad \forall s \in \mathcal{S}\]</span></p>
</section>
<section id="gpi" class="level2">
<h2 class="anchored" data-anchor-id="gpi">GPI</h2>
<p>For <strong>Generalized Policy Iteration</strong>, we hope to converge to the optimal policy <span class="math inline">\(\pi_{*}\)</span> and optimal state value function <span class="math inline">\(v_{*}\)</span>.</p>
<p>“Almost all reinforcement learning methods are well described as GPI” — Simon and Barto</p>
</section>
</section>
<section id="gym-frozen-lake" class="level1">
<h1>Gym: Frozen Lake</h1>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-3-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-1" role="tab" aria-controls="tabset-3-1" aria-selected="true">Scene</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-2" role="tab" aria-controls="tabset-3-2" aria-selected="false">Search</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-3" role="tab" aria-controls="tabset-3-3" aria-selected="false">Findings</a></li></ul>
<div class="tab-content">
<div id="tabset-3-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-3-1-tab">
<div class="columns">
<div class="column" style="width:45%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="frozen_lake_start.png" class="img-fluid figure-img"></p>
<figcaption>Frozen Lake</figcaption>
</figure>
</div>
<ul>
<li>image credit: <a href="https://aleksandarhaber.com/policy-iteration-algorithm-in-python-and-tests-with-frozen-lake-openai-gym-environment-reinforcement-learning-tutorial/">Alexandar Haber</a></li>
</ul>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:45%;">
<ul>
<li>start at square “S”</li>
<li>can move up, down, left, or right from <em>frozen</em> squares “F”</li>
<li>trajectory ends at holes (“H”) with negative reward</li>
<li>trajectory ends at finish with positive reward</li>
</ul>
<p>Iterate until policy converges.</p>
</div>
</div>
</div>
<div id="tabset-3-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-2-tab">
<div class="columns">
<div class="column" style="width:45%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="frozen_lake.gif" class="img-fluid figure-img"></p>
<figcaption>Frozen Lake</figcaption>
</figure>
</div>
<ul>
<li>image credit: <a href="https://www.gymlibrary.dev/environments/toy_text/frozen_lake/">Gym Library</a></li>
</ul>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:45%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Q_learning_FrozenLake_33_0.png" class="img-fluid figure-img"></p>
<figcaption>policy improvement</figcaption>
</figure>
</div>
<ul>
<li>image credit: <a href="https://gsverhoeven.github.io/post/frozenlake-qlearning-convergence/">Gertjan Verhoeven</a></li>
</ul>
</div>
</div>
</div>
<div id="tabset-3-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-3-tab">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="output_value-iter_b30603_3_0.png" class="img-fluid figure-img"></p>
<figcaption>optimal policy</figcaption>
</figure>
</div>
<ul>
<li>image credit: <a href="https://d2l.ai/chapter_reinforcement-learning/value-iter.html">Dive into Deep Learning</a></li>
</ul>
</div>
</div>
</div>
</section>
<section id="quo-vadimus" class="level1">
<h1>Quo Vadimus?</h1>
<div class="columns">
<div class="column" style="width:45%;">
<ul>
<li><p>due this Friday (April 11):</p>
<ul>
<li>Precept 9</li>
<li>Data Glimpse</li>
</ul></li>
</ul>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:45%;">
<ul>
<li>semester projects will be due May 10</li>
</ul>
</div>
</div>
</section>
<section id="footnotes" class="level1">
<h1>Footnotes</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-13-contents" aria-controls="callout-13" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
(optional) Additional Resources and References
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-13" class="callout-13-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><p><a href="https://d2l.ai/chapter_reinforcement-learning/index.html">Reinforcement Learning</a> from the <em>Dive into Deep Learning</em> book</p></li>
<li><p>Reinforcement Learning with Neural Networks by Josh Starmer</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=xyfSvzZuk5Y">essentials</a></li>
<li><a href="https://www.youtube.com/watch?v=DVGmsnxB2UQ">math details</a></li>
</ul></li>
<li><p><a href="https://medium.com/@MachineLearningYearning/simple-reinforcement-learning-in-python-2340c4df8c04">Simple Reinforcement Learning</a> by Machine Yearner</p></li>
<li><p><a href="https://isaac-the-man.dev/posts/reinforcement-learning-mountain-car/">Train an AI Agent to play Mountain Car with RL</a> by Yu-Kai “Steven” Wang</p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-14-contents" aria-controls="callout-14" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Session Info
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-14" class="callout-14-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sessionInfo</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>R version 4.4.2 (2024-10-31 ucrt)
Platform: x86_64-w64-mingw32/x64
Running under: Windows 10 x64 (build 19045)

Matrix products: default


locale:
[1] LC_COLLATE=English_United States.utf8 
[2] LC_CTYPE=English_United States.utf8   
[3] LC_MONETARY=English_United States.utf8
[4] LC_NUMERIC=C                          
[5] LC_TIME=English_United States.utf8    

time zone: America/New_York
tzcode source: internal

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

loaded via a namespace (and not attached):
 [1] htmlwidgets_1.6.4 compiler_4.4.2    fastmap_1.2.0     cli_3.6.3        
 [5] tools_4.4.2       htmltools_0.5.8.1 rstudioapi_0.17.1 yaml_2.3.10      
 [9] rmarkdown_2.29    knitr_1.49        jsonlite_1.8.9    xfun_0.50        
[13] digest_0.6.37     rlang_1.1.5       evaluate_1.0.3   </code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="columns">
<div class="column" style="width:45%;">

</div><div class="column" style="width:10%;">

</div><div class="column" style="width:45%;">

</div>
</div>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"></ul>
<div class="tab-content">

</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>