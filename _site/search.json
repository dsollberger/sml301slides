[
  {
    "objectID": "posts/301_01_introduction/01_introduction.html",
    "href": "posts/301_01_introduction/01_introduction.html",
    "title": "1: Introductions",
    "section": "",
    "text": "Goal: Introduce course\nObjective: Explore some Python codes\n\n\n\n\n\ntextbooks"
  },
  {
    "objectID": "posts/301_01_introduction/01_introduction.html#start",
    "href": "posts/301_01_introduction/01_introduction.html#start",
    "title": "1: Introductions",
    "section": "",
    "text": "Goal: Introduce course\nObjective: Explore some Python codes\n\n\n\n\n\ntextbooks"
  },
  {
    "objectID": "posts/301_01_introduction/01_introduction.html#data-intelligence-modern-data-science-methods",
    "href": "posts/301_01_introduction/01_introduction.html#data-intelligence-modern-data-science-methods",
    "title": "1: Introductions",
    "section": "Data Intelligence: Modern Data Science Methods",
    "text": "Data Intelligence: Modern Data Science Methods\n\nSpring 2025\nMonday, Wednesday, 11 AM to 1250 PM\nLecturer: Derek\n\nI go by “Derek” or “teacher”\n\n\n\n\n\n\n\n\nCourse Description\n\n\n\n\n\nThis course provides the training for students to be independent in modern data analysis. The course emphasizes the rigorous treatment of data and the programming skills and conceptual understanding required for dealing with modern datasets. The course examines data analysis through the lens of statistics and machine learning methods. Students verify their understanding by working with real datasets. The course also covers supporting topics such as experiment design, ethical data use, best practices for statistical and machine learning methods, reproducible research, writing a quantitative research paper, and presenting research results."
  },
  {
    "objectID": "posts/301_01_introduction/01_introduction.html#lecturer",
    "href": "posts/301_01_introduction/01_introduction.html#lecturer",
    "title": "1: Introductions",
    "section": "Lecturer",
    "text": "Lecturer"
  },
  {
    "objectID": "posts/301_01_introduction/01_introduction.html#current-research-in-pedagogy",
    "href": "posts/301_01_introduction/01_introduction.html#current-research-in-pedagogy",
    "title": "1: Introductions",
    "section": "Current Research in Pedagogy",
    "text": "Current Research in Pedagogy\n\n\n\n\n\nactive learning\ncomputer programming\nflipped classrooms"
  },
  {
    "objectID": "posts/301_01_introduction/01_introduction.html#textbooks",
    "href": "posts/301_01_introduction/01_introduction.html#textbooks",
    "title": "1: Introductions",
    "section": "Textbooks",
    "text": "Textbooks\n\nList1234\n\n\n\nAn Introduction to Statistical Learning by James, Witten, Hastie, Tibshirani, Taylor\nDeep Learning Illustrated by Jon Krohn\nHow AI Works by Ronald T Kneusel\nProbabilistic Machine Learning by Kevin Patrick Murphy\n\n\n\n\n\n\nISLP\n\n\n\nAn Introduction to Statistical Learning by James, Witten, Hastie, Tibshirani, Taylor\n\n\n\n\n\n\nDeep Learning\n\n\n\nDeep Learning Illustrated by Jon Krohn\n\n\n\n\n\n\nHow AI Works\n\n\n\nHow AI Works by Ronald T Kneusel\n\n\n\n\n\n\nProbabilistic Machine Learning\n\n\n\nProbabilistic Machine Learning by Kevin Patrick Murphy"
  },
  {
    "objectID": "posts/301_01_introduction/01_introduction.html#cooperative-classroom",
    "href": "posts/301_01_introduction/01_introduction.html#cooperative-classroom",
    "title": "1: Introductions",
    "section": "Cooperative Classroom",
    "text": "Cooperative Classroom\nLearning in a cooperative environment should be stimulating, demanding, and fair. Because this approach to learning is different from the competitive classroom structure that many other courses used to be based on, it is important for us to be clear about mutual expectations. Below are my expectations for students in this class. This set of expectations is intended to maximize debate and exchange of ideas in an atmosphere of mutual respect while preserving individual ownership of ideas and written words. If you feel you do not understand or cannot agree to these expectations, you should discuss this with your instructor and classmates.\n\nStudents are expected to work cooperatively with other members of the class and show respect for the ideas and contributions of other people.\nWhen working as part of a group, students should strive to be good contributors to the group, listen to others, not dominate, and recognize the contributions of others. Students should try to ensure that everyone in the group is welcome to contribute and recognize that everyone contributes in different ways to a group process.\nStudents should explore data, make observations, and develop inferences as part of a group. If you use material from published sources, you must provide appropriate attribution.\n\n\n\n(Students will be asked to acknowledge this document in an online form.)\nThis document has been adapted from Scientific Teaching by Jo Handelsman, Sarah Miller, and Christine Pfund\n\n\n\n\nScientific Teaching"
  },
  {
    "objectID": "posts/301_01_introduction/01_introduction.html#pep-talk",
    "href": "posts/301_01_introduction/01_introduction.html#pep-talk",
    "title": "1: Introductions",
    "section": "Pep Talk",
    "text": "Pep Talk\nLearning R can be difficult at first—it is like learning a new language, just like Spanish, French, or Chinese. Hadley Wickham—the chief data scientist at RStudio and the author of some amazing R packages you will be using like ggplot2—made this wise observation:\n\n\n\n\n\n\nWisdom from Hadley Wickham\n\n\n\n\n\nIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n\n\n\nIf you are finding yourself taking way too long hitting your head against a wall and not understanding, take a break, talk to classmates, ask questions … e-mail [Derek], etc. I promise you can do this.\n—Andrew Heiss, Georgia State University"
  },
  {
    "objectID": "posts/301_01_introduction/01_introduction.html#inclusion-statement",
    "href": "posts/301_01_introduction/01_introduction.html#inclusion-statement",
    "title": "1: Introductions",
    "section": "Inclusion Statement",
    "text": "Inclusion Statement\nI value all students regardless of their background, country of origin, race, religion, ethnicity, gender, sexual orientation, disability status, etc. and am committed to providing a climate of excellence and inclusiveness within all aspects of the course. If there are aspects of your culture or identity that you would like to share with me as they relate to your success in this class, I am happy to meet to discuss. Likewise, if you have any concerns in this area or facing any special issues or challenges, you are encouraged to discuss the matter with me (set up a meeting by e-mail) with an assurance of full confidentiality (only exception being mandatory reporting of academic integrity code violations or sexual harassment)."
  },
  {
    "objectID": "posts/301_01_introduction/01_introduction.html#tensorflow-playground",
    "href": "posts/301_01_introduction/01_introduction.html#tensorflow-playground",
    "title": "1: Introductions",
    "section": "TensorFlow Playground",
    "text": "TensorFlow Playground\n\nlink: https://playground.tensorflow.org\nexplore the various menus and buttons\nfeel free to run a simulation"
  },
  {
    "objectID": "posts/301_02_convergence/02_convergence.html",
    "href": "posts/301_02_convergence/02_convergence.html",
    "title": "2: Convergence",
    "section": "",
    "text": "Goal: Discuss convergence\nObjective: Explore some Python codes about root finding and stochastic processes\n\n\nAs we get started, try to load a session in Google Colab"
  },
  {
    "objectID": "posts/301_02_convergence/02_convergence.html#start",
    "href": "posts/301_02_convergence/02_convergence.html#start",
    "title": "2: Convergence",
    "section": "",
    "text": "Goal: Discuss convergence\nObjective: Explore some Python codes about root finding and stochastic processes\n\n\nAs we get started, try to load a session in Google Colab"
  },
  {
    "objectID": "posts/301_02_convergence/02_convergence.html#tensorflow-playground",
    "href": "posts/301_02_convergence/02_convergence.html#tensorflow-playground",
    "title": "2: Convergence",
    "section": "TensorFlow Playground",
    "text": "TensorFlow Playground\n\nlink: https://playground.tensorflow.org\nexplore the various menus and buttons\nfeel free to run a simulation"
  },
  {
    "objectID": "posts/301_02_convergence/02_convergence.html#sequences",
    "href": "posts/301_02_convergence/02_convergence.html#sequences",
    "title": "2: Convergence",
    "section": "Sequences",
    "text": "Sequences\n\nindex: \\(n \\in \\mathbb{N} = \\{1, 2, 3, 4, 5, ...\\}\\)\nformulaic: f(n) = 2n - 1\n\n\\[1, 3, 5, 7, 9, ...\\]\n\nconstructive:\n\n\\[3, 3.1, 3.14, 3.141, 3.1415, 3.14159, ...\\]\n\nshapes:\n\n\n\n\ntriangular numbers\n\n\n\nimage source: BYJUs"
  },
  {
    "objectID": "posts/301_02_convergence/02_convergence.html#random-variables",
    "href": "posts/301_02_convergence/02_convergence.html#random-variables",
    "title": "2: Convergence",
    "section": "Random Variables",
    "text": "Random Variables\nA random variable has no set value, but rather represents an element of chance. We can better understand a random variable through statistics like\n\nmean\nvariance\ndistribution\n\n\n\n\n\n\n\nStochastic Process\n\n\n\n\n\nA stochastic process is a sequence of random variables"
  },
  {
    "objectID": "posts/301_02_convergence/02_convergence.html#application-dinner-choices",
    "href": "posts/301_02_convergence/02_convergence.html#application-dinner-choices",
    "title": "2: Convergence",
    "section": "Application: Dinner Choices",
    "text": "Application: Dinner Choices\nSuppose that we have a Princeton student whose behavior includes eating only three types of dinner:\n\\[S = \\{\\text{ramen}, \\text{pizza}, \\text{sushi}\\}\\]\nwith transition matrix\n\\[P = \\left(\\begin{array}{ccc}\n0.2 & 0.4 & 0.4 \\\\\n0.3 & 0.4 & 0.3 \\\\\n0.2 & 0.2 & 0.6\n\\end{array}\\right)\\]\n\n\n\ndinner choices network\n\n\n\n\n\n\n\n\nNetwork terminology\n\n\n\n\n\n\ndirected versus undirected graphs\ncyclic versus acyclic graphs\n\nLater studies focus on DAGs: directed, acyclic network graphs\n\n\n\nSuppose that, on a Monday, the student’s preferences are\n\\[x_0 = \\left(\\begin{array}{ccc} 0.5 & 0.25 & 0.25 \\end{array}\\right)\\]\n\nWhat is the probability that the student will eat ramen on Tuesday (i.e. the next day)?\nWhat is the probability that the student will eat pizza on Wednesday (i.e. two days later)?\nWhat is the long-term dinner-choice behavior of this student?"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Lecture Sessions",
    "section": "",
    "text": "5: Support Vector Machines\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2025\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n4: Classification\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2025\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n3: Regression\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2025\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n2: Convergence\n\n\n\n\n\n\n\n\n\n\n\nJan 29, 2025\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n1: Introductions\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2025\n\n\nDerek Sollberger\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SML 301 (Spring 2025)",
    "section": "",
    "text": "5: Support Vector Machines\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2025\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n4: Classification\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2025\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n3: Regression\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2025\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n2: Convergence\n\n\n\n\n\n\n\n\n\n\n\nJan 29, 2025\n\n\nDerek Sollberger\n\n\n\n\n\n\n\n\n\n\n\n\n1: Introductions\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2025\n\n\nDerek Sollberger\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/03_regression/03_regression.html",
    "href": "posts/03_regression/03_regression.html",
    "title": "3: Regression",
    "section": "",
    "text": "Goal: Discuss the bias-variance trade-ff\nObjective: Explore linear regression\n\n\nAs we get started, try to install the ISLP package in your Python software"
  },
  {
    "objectID": "posts/03_regression/03_regression.html#start",
    "href": "posts/03_regression/03_regression.html#start",
    "title": "3: Regression",
    "section": "",
    "text": "Goal: Discuss the bias-variance trade-ff\nObjective: Explore linear regression\n\n\nAs we get started, try to install the ISLP package in your Python software"
  },
  {
    "objectID": "posts/03_regression/03_regression.html#residuals",
    "href": "posts/03_regression/03_regression.html#residuals",
    "title": "3: Regression",
    "section": "Residuals",
    "text": "Residuals\nA residual is the difference between a predicted value and its true value."
  },
  {
    "objectID": "posts/03_regression/03_regression.html#method-of-least-squares",
    "href": "posts/03_regression/03_regression.html#method-of-least-squares",
    "title": "3: Regression",
    "section": "Method of Least Squares",
    "text": "Method of Least Squares\nIdea: The best-fit line is where the sum-of-squared residuals is minimized.\n\\[E(a,b) = \\sum_{i=1}^{n} (y_{i} - a - bx_{i})^{2}\\]\nClaim: \\[a = \\frac{ (\\sum y_{i})(\\sum x_{i}^{2}) - (\\sum x_{i})(\\sum x_{i}y_{i}) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} }, \\quad b = \\frac{ n\\sum x_{i}y_{i} - (\\sum x_{i})(\\sum y_{i}) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} }\\]\n\n\n\n\n\n\n(optional) Proof\n\n\n\n\n\nSearch for a critical point by setting the partial derivatives (along with the Chain Rule) equal to zero.\n\\[0 = \\frac{\\partial E}{\\partial a} = -2\\sum_{i = 1}^{n} (y_{i} - a - bx_{i}) = 2an + 2b\\sum_{i = 1}^{n}x_{i} - 2\\sum_{i = 1}^{n} y_{i}\\] \\[0 = \\frac{\\partial E}{\\partial b} = -2\\sum_{i = 1}^{n} (y_{i} - a - bx_{i})x_{i} = 2a\\sum_{i = 1}^{n}x_{i} + 2b\\sum_{i = 1}^{n}x_{i}^{2} - 2\\sum_{i = 1}^{n} x_{i}y_{i}\\]\nCreate a matrix system of equations.\n\\[\\left[  \\begin{array}{cc}\n  n & \\sum_{i = 1}^{n}x_{i} \\\\\n  \\sum_{i = 1}^{n}x_{i} & \\sum_{i = 1}^{n}x_{i}^{2} \\\\\n  \\end{array}\\right]\n  \\left[  \\begin{array}{c}  a \\\\ b \\end{array}\\right]\n  =\n  \\left[  \\begin{array}{c}  \\sum_{i = 1}^{n} y_{i} \\\\ \\sum_{i = 1}^{n} x_{i}y_{i} \\end{array}\\right]\n  \\]\nEmploy a matrix inverse.\n$$\n\\[\\begin{array}{rcl}\n  \\left[  \\begin{array}{c}  a \\\\ b \\end{array}\\right] & = &\n  \\left[  \\begin{array}{cc}\n  n & \\sum_{i = 1}^{n}x_{i} \\\\\n  \\sum_{i = 1}^{n}x_{i} & \\sum_{i = 1}^{n}x_{i}^{2} \\\\\n  \\end{array}\\right]^{-1}\\left[  \\begin{array}{c}  \\sum_{i = 1}^{n} y_{i} \\\\ \\sum_{i = 1}^{n} x_{i}y_{i} \\end{array}\\right] \\\\\n  \n  ~ & ~ & ~ \\\\\n  \n  \\left[  \\begin{array}{c}  a \\\\ b \\end{array}\\right] & = & \\frac{1}{n\\sum x_{i}^{2} - (\\sum x_{i})^{2}} \\left[  \\begin{array}{cc}\n  \\sum_{i = 1}^{n}x_{i}^{2} & -\\sum_{i = 1}^{n}x_{i} \\\\\n  -\\sum_{i = 1}^{n}x_{i} & n \\\\\n  \\end{array}\\right]  \\left[  \\begin{array}{c}  \\sum_{i = 1}^{n} y_{i} \\\\ \\sum_{i = 1}^{n} x_{i}y_{i} \\end{array}\\right] \\\\\n  \n  ~ & ~ & ~ \\\\\n  \n  \\left[  \\begin{array}{c}  a \\\\ b \\end{array}\\right] & = & \\frac{1}{n\\sum x_{i}^{2} - (\\sum x_{i})^{2}}\n  \\left[  \\begin{array}{c}  (\\sum y_{i})(\\sum x_{i}^{2}) - (\\sum x_{i})(\\sum x_{i}y_{i}) \\\\  n\\sum x_{i}y_{i} - (\\sum x_{i})(\\sum y_{i}) \\end{array}\\right] \\\\\n\\end{array}\\]\n$$"
  },
  {
    "objectID": "posts/03_regression/03_regression.html#multiple-linear-regression",
    "href": "posts/03_regression/03_regression.html#multiple-linear-regression",
    "title": "3: Regression",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\\[\\hat{y} = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{3} + ...\\]\nis likewise solved by ordinary least squares"
  },
  {
    "objectID": "posts/03_regression/03_regression.html#adaboost",
    "href": "posts/03_regression/03_regression.html#adaboost",
    "title": "3: Regression",
    "section": "Adaboost",
    "text": "Adaboost\n\nA Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting (1997—published as abstract in 1995), Freund and Schapire"
  },
  {
    "objectID": "posts/03_regression/03_regression.html#alexnet",
    "href": "posts/03_regression/03_regression.html#alexnet",
    "title": "3: Regression",
    "section": "AlexNet",
    "text": "AlexNet\n\nImageNet Classification with Deep Convolutional Neural Networks (2012)"
  },
  {
    "objectID": "posts/03_regression/03_regression.html#dropout",
    "href": "posts/03_regression/03_regression.html#dropout",
    "title": "3: Regression",
    "section": "DropOut",
    "text": "DropOut\n\nDropout: A Simple Way to Prevent Neural Networks from Overfitting (2014), Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov"
  },
  {
    "objectID": "posts/03_regression/03_regression.html#gans",
    "href": "posts/03_regression/03_regression.html#gans",
    "title": "3: Regression",
    "section": "GANs",
    "text": "GANs\n\nGeneral Adversarial Nets (2014), Goodfellow et al."
  },
  {
    "objectID": "posts/03_regression/03_regression.html#tensorflow",
    "href": "posts/03_regression/03_regression.html#tensorflow",
    "title": "3: Regression",
    "section": "TensorFlow",
    "text": "TensorFlow\n\nTensorFlow: A system for large-scale machine learning (2016), Abadi et al."
  },
  {
    "objectID": "posts/03_regression/03_regression.html#word2vec",
    "href": "posts/03_regression/03_regression.html#word2vec",
    "title": "3: Regression",
    "section": "Word2Vec",
    "text": "Word2Vec\n\nEfficient Estimation of Word Representations in Vector Space (2013), Mikolov, Chen, Corrado, and Dean"
  },
  {
    "objectID": "posts/03_regression/03_regression.html#bias-variance-trade-off",
    "href": "posts/03_regression/03_regression.html#bias-variance-trade-off",
    "title": "3: Regression",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off\nWithin a hypothesis class of similar modeling functions, we are concerned with the bias-variance tradeoff in model selection.\n\n\n\nbias-variance tradeoff\n\n\nimage source: Scott Fortmann-Roe"
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html",
    "href": "posts/01_introduction/01_introduction.html",
    "title": "1: Introductions",
    "section": "",
    "text": "Goal: Introduce course\nObjective: Explore some Python codes\n\n\n\n\n\ntextbooks"
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#start",
    "href": "posts/01_introduction/01_introduction.html#start",
    "title": "1: Introductions",
    "section": "",
    "text": "Goal: Introduce course\nObjective: Explore some Python codes\n\n\n\n\n\ntextbooks"
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#data-intelligence-modern-data-science-methods",
    "href": "posts/01_introduction/01_introduction.html#data-intelligence-modern-data-science-methods",
    "title": "1: Introductions",
    "section": "Data Intelligence: Modern Data Science Methods",
    "text": "Data Intelligence: Modern Data Science Methods\n\nSpring 2025\nMonday, Wednesday, 11 AM to 1250 PM\nLecturer: Derek\n\nI go by “Derek” or “teacher”\n\n\n\n\n\n\n\n\nCourse Description\n\n\n\n\n\nThis course provides the training for students to be independent in modern data analysis. The course emphasizes the rigorous treatment of data and the programming skills and conceptual understanding required for dealing with modern datasets. The course examines data analysis through the lens of statistics and machine learning methods. Students verify their understanding by working with real datasets. The course also covers supporting topics such as experiment design, ethical data use, best practices for statistical and machine learning methods, reproducible research, writing a quantitative research paper, and presenting research results."
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#lecturer",
    "href": "posts/01_introduction/01_introduction.html#lecturer",
    "title": "1: Introductions",
    "section": "Lecturer",
    "text": "Lecturer"
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#current-research-in-pedagogy",
    "href": "posts/01_introduction/01_introduction.html#current-research-in-pedagogy",
    "title": "1: Introductions",
    "section": "Current Research in Pedagogy",
    "text": "Current Research in Pedagogy\n\n\n\n\n\nactive learning\ncomputer programming\nflipped classrooms"
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#textbooks",
    "href": "posts/01_introduction/01_introduction.html#textbooks",
    "title": "1: Introductions",
    "section": "Textbooks",
    "text": "Textbooks\n\nList1234\n\n\n\nAn Introduction to Statistical Learning by James, Witten, Hastie, Tibshirani, Taylor\nDeep Learning Illustrated by Jon Krohn\nHow AI Works by Ronald T Kneusel\nProbabilistic Machine Learning by Kevin Patrick Murphy\n\n\n\n\n\n\nISLP\n\n\n\nAn Introduction to Statistical Learning by James, Witten, Hastie, Tibshirani, Taylor\n\n\n\n\n\n\nDeep Learning\n\n\n\nDeep Learning Illustrated by Jon Krohn\n\n\n\n\n\n\nHow AI Works\n\n\n\nHow AI Works by Ronald T Kneusel\n\n\n\n\n\n\nProbabilistic Machine Learning\n\n\n\nProbabilistic Machine Learning by Kevin Patrick Murphy"
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#cooperative-classroom",
    "href": "posts/01_introduction/01_introduction.html#cooperative-classroom",
    "title": "1: Introductions",
    "section": "Cooperative Classroom",
    "text": "Cooperative Classroom\nLearning in a cooperative environment should be stimulating, demanding, and fair. Because this approach to learning is different from the competitive classroom structure that many other courses used to be based on, it is important for us to be clear about mutual expectations. Below are my expectations for students in this class. This set of expectations is intended to maximize debate and exchange of ideas in an atmosphere of mutual respect while preserving individual ownership of ideas and written words. If you feel you do not understand or cannot agree to these expectations, you should discuss this with your instructor and classmates.\n\nStudents are expected to work cooperatively with other members of the class and show respect for the ideas and contributions of other people.\nWhen working as part of a group, students should strive to be good contributors to the group, listen to others, not dominate, and recognize the contributions of others. Students should try to ensure that everyone in the group is welcome to contribute and recognize that everyone contributes in different ways to a group process.\nStudents should explore data, make observations, and develop inferences as part of a group. If you use material from published sources, you must provide appropriate attribution.\n\n\n\n(Students will be asked to acknowledge this document in an online form.)\nThis document has been adapted from Scientific Teaching by Jo Handelsman, Sarah Miller, and Christine Pfund\n\n\n\n\nScientific Teaching"
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#pep-talk",
    "href": "posts/01_introduction/01_introduction.html#pep-talk",
    "title": "1: Introductions",
    "section": "Pep Talk",
    "text": "Pep Talk\nLearning R can be difficult at first—it is like learning a new language, just like Spanish, French, or Chinese. Hadley Wickham—the chief data scientist at RStudio and the author of some amazing R packages you will be using like ggplot2—made this wise observation:\n\n\n\n\n\n\nWisdom from Hadley Wickham\n\n\n\n\n\nIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n\n\n\nIf you are finding yourself taking way too long hitting your head against a wall and not understanding, take a break, talk to classmates, ask questions … e-mail [Derek], etc. I promise you can do this.\n—Andrew Heiss, Georgia State University"
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#inclusion-statement",
    "href": "posts/01_introduction/01_introduction.html#inclusion-statement",
    "title": "1: Introductions",
    "section": "Inclusion Statement",
    "text": "Inclusion Statement\nI value all students regardless of their background, country of origin, race, religion, ethnicity, gender, sexual orientation, disability status, etc. and am committed to providing a climate of excellence and inclusiveness within all aspects of the course. If there are aspects of your culture or identity that you would like to share with me as they relate to your success in this class, I am happy to meet to discuss. Likewise, if you have any concerns in this area or facing any special issues or challenges, you are encouraged to discuss the matter with me (set up a meeting by e-mail) with an assurance of full confidentiality (only exception being mandatory reporting of academic integrity code violations or sexual harassment)."
  },
  {
    "objectID": "posts/01_introduction/01_introduction.html#tensorflow-playground",
    "href": "posts/01_introduction/01_introduction.html#tensorflow-playground",
    "title": "1: Introductions",
    "section": "TensorFlow Playground",
    "text": "TensorFlow Playground\n\nlink: https://playground.tensorflow.org\nexplore the various menus and buttons\nfeel free to run a simulation"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "SML 301 Spring 2025"
  },
  {
    "objectID": "posts/02_convergence/02_convergence.html",
    "href": "posts/02_convergence/02_convergence.html",
    "title": "2: Convergence",
    "section": "",
    "text": "Goal: Discuss convergence\nObjective: Explore some Python codes about root finding and stochastic processes\n\n\nAs we get started, try to load a session in Google Colab"
  },
  {
    "objectID": "posts/02_convergence/02_convergence.html#start",
    "href": "posts/02_convergence/02_convergence.html#start",
    "title": "2: Convergence",
    "section": "",
    "text": "Goal: Discuss convergence\nObjective: Explore some Python codes about root finding and stochastic processes\n\n\nAs we get started, try to load a session in Google Colab"
  },
  {
    "objectID": "posts/02_convergence/02_convergence.html#tensorflow-playground",
    "href": "posts/02_convergence/02_convergence.html#tensorflow-playground",
    "title": "2: Convergence",
    "section": "TensorFlow Playground",
    "text": "TensorFlow Playground\n\nlink: https://playground.tensorflow.org\nexplore the various menus and buttons\nfeel free to run a simulation"
  },
  {
    "objectID": "posts/02_convergence/02_convergence.html#sequences",
    "href": "posts/02_convergence/02_convergence.html#sequences",
    "title": "2: Convergence",
    "section": "Sequences",
    "text": "Sequences\n\nindex: \\(n \\in \\mathbb{N} = \\{1, 2, 3, 4, 5, ...\\}\\)\nformulaic: f(n) = 2n - 1\n\n\\[1, 3, 5, 7, 9, ...\\]\n\nconstructive:\n\n\\[3, 3.1, 3.14, 3.141, 3.1415, 3.14159, ...\\]\n\nshapes:\n\n\n\n\ntriangular numbers\n\n\n\nimage source: BYJUs"
  },
  {
    "objectID": "posts/02_convergence/02_convergence.html#random-variables",
    "href": "posts/02_convergence/02_convergence.html#random-variables",
    "title": "2: Convergence",
    "section": "Random Variables",
    "text": "Random Variables\nA random variable has no set value, but rather represents an element of chance. We can better understand a random variable through statistics like\n\nmean\nvariance\ndistribution\n\n\n\n\n\n\n\nStochastic Process\n\n\n\n\n\nA stochastic process is a sequence of random variables"
  },
  {
    "objectID": "posts/02_convergence/02_convergence.html#application-dinner-choices",
    "href": "posts/02_convergence/02_convergence.html#application-dinner-choices",
    "title": "2: Convergence",
    "section": "Application: Dinner Choices",
    "text": "Application: Dinner Choices\nSuppose that we have a Princeton student whose behavior includes eating only three types of dinner:\n\\[S = \\{\\text{ramen}, \\text{pizza}, \\text{sushi}\\}\\]\nwith transition matrix\n\\[P = \\left(\\begin{array}{ccc}\n0.2 & 0.4 & 0.4 \\\\\n0.3 & 0.4 & 0.3 \\\\\n0.2 & 0.2 & 0.6\n\\end{array}\\right)\\]\n\n\n\ndinner choices network\n\n\n\n\n\n\n\n\nNetwork terminology\n\n\n\n\n\n\ndirected versus undirected graphs\ncyclic versus acyclic graphs\n\nLater studies focus on DAGs: directed, acyclic network graphs\n\n\n\nSuppose that, on a Monday, the student’s preferences are\n\\[x_0 = \\left(\\begin{array}{ccc} 0.5 & 0.25 & 0.25 \\end{array}\\right)\\]\n\nWhat is the probability that the student will eat ramen on Tuesday (i.e. the next day)?\nWhat is the probability that the student will eat pizza on Wednesday (i.e. two days later)?\nWhat is the long-term dinner-choice behavior of this student?"
  },
  {
    "objectID": "posts/04_classification/04_classification.html",
    "href": "posts/04_classification/04_classification.html",
    "title": "4: Classification",
    "section": "",
    "text": "Goal: Start to discuss classification methods\nObjective: Explore logistic regression and Naive Bayes classifiers\n\n\nAs we get started, try to install the palmerpenguins package in your Python software"
  },
  {
    "objectID": "posts/04_classification/04_classification.html#start",
    "href": "posts/04_classification/04_classification.html#start",
    "title": "4: Classification",
    "section": "",
    "text": "Goal: Start to discuss classification methods\nObjective: Explore logistic regression and Naive Bayes classifiers\n\n\nAs we get started, try to install the palmerpenguins package in your Python software"
  },
  {
    "objectID": "posts/04_classification/04_classification.html#palmer-penguins-example",
    "href": "posts/04_classification/04_classification.html#palmer-penguins-example",
    "title": "4: Classification",
    "section": "Palmer Penguins Example",
    "text": "Palmer Penguins Example\n\n\n\n\n\n\n\n\n\n\n\nR Code\n\n\nadelie_color = \"#fb7504\"\nchinstrap_color = \"#c65ccc\"\ngentoo_color = \"#067476\"\n\npenguin_class_df &lt;- penguins |&gt;\n  na.omit() |&gt;\n  mutate(chinstrap_bool = ifelse(species == \"Chinstrap\", 1, 0)) |&gt;\n  mutate(across(chinstrap_bool, as.factor)) #https://stackoverflow.com/questions/33180058/coerce-multiple-columns-to-factors-at-once\n\npenguin_class_df |&gt;\nggplot(aes(x = flipper_length_mm, y = bill_length_mm, \n           color = chinstrap_bool)) + \n  geom_point(size = 3) + \n  labs(title = \"Classification Task\",\n       subtitle = \"Finding the &lt;span style = 'color:#c65ccc'&gt;Chinstrap&lt;/span&gt; penguins among n = 333 penguins\",\n       caption = \"SML 301\") +\n  scale_color_manual(values = c(\"gray70\", chinstrap_color)) +\n  theme_minimal() +\n  theme(plot.title = element_markdown(face = \"bold\", size = 24),\n        plot.subtitle = element_markdown(size = 16))\n\n\n\nGeneralized Linear Models\n\nlogistic_model &lt;- stats::glm(chinstrap_bool ~ flipper_length_mm + bill_length_mm,\n                      data = penguin_class_df,\n                      family = binomial) #makes logistic regression\n\n\n\n\n\n\n\n\n\n\n\n\nR code\n\n\n# https://stats.stackexchange.com/questions/6206/how-to-plot-decision-boundary-in-r-for-logistic-regression-model\nbeta_0 &lt;- coef(logistic_model)[1]\nbeta_1 &lt;- coef(logistic_model)[2]\nbeta_2 &lt;- coef(logistic_model)[3]\nboundary_slope &lt;- -1.0 * beta_1 / beta_2\nboundary_intercept &lt;- -1.0 * beta_0 / beta_2\n\npenguin_pred_df &lt;- penguin_class_df |&gt;\n  mutate(species_pred = ifelse(\n    bill_length_mm &gt; boundary_intercept + boundary_slope * flipper_length_mm,\n    1,0)) |&gt;\n  mutate(across(species_pred, as.factor))\n\npenguin_pred_df |&gt;\nggplot(aes(x = flipper_length_mm, y = bill_length_mm, \n           color = species_pred)) + \n  geom_point(size = 3) + \n  geom_abline(intercept = boundary_intercept,\n              slope = boundary_slope,\n              color = adelie_color,\n              linewidth = 2,\n              linetype = 2) +\n  labs(title = \"&lt;span style = 'color:#fb7504'&gt;Decision Boundary&lt;/span&gt;\",\n       subtitle = \"where logit a = 0\",\n       caption = \"SML 301\") +\n  scale_color_manual(values = c(\"gray70\", chinstrap_color)) +\n  theme_minimal() +\n  theme(plot.title = element_markdown(face = \"bold\", size = 24),\n        plot.subtitle = element_markdown(size = 16))\n\n\n\npenguin_pred_df |&gt;\n  janitor::tabyl(chinstrap_bool, species_pred) |&gt;\n  janitor::adorn_totals(c(\"row\", \"col\"))\n\n chinstrap_bool   0  1 Total\n              0 258  7   265\n              1   8 60    68\n          Total 266 67   333\n\n\n\naccuracy: 0.9550\nsensitivity: 0.8824\nspecificity: 0.9736"
  },
  {
    "objectID": "posts/04_classification/04_classification.html#adaboost",
    "href": "posts/04_classification/04_classification.html#adaboost",
    "title": "4: Classification",
    "section": "Adaboost",
    "text": "Adaboost\n\nA Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting (1997—published as abstract in 1995), Freund and Schapire"
  },
  {
    "objectID": "posts/04_classification/04_classification.html#alexnet",
    "href": "posts/04_classification/04_classification.html#alexnet",
    "title": "4: Classification",
    "section": "AlexNet",
    "text": "AlexNet\n\nImageNet Classification with Deep Convolutional Neural Networks (2012)"
  },
  {
    "objectID": "posts/04_classification/04_classification.html#dropout",
    "href": "posts/04_classification/04_classification.html#dropout",
    "title": "4: Classification",
    "section": "DropOut",
    "text": "DropOut\n\nDropout: A Simple Way to Prevent Neural Networks from Overfitting (2014), Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov"
  },
  {
    "objectID": "posts/04_classification/04_classification.html#gans",
    "href": "posts/04_classification/04_classification.html#gans",
    "title": "4: Classification",
    "section": "GANs",
    "text": "GANs\n\nGeneral Adversarial Nets (2014), Goodfellow et al."
  },
  {
    "objectID": "posts/04_classification/04_classification.html#tensorflow",
    "href": "posts/04_classification/04_classification.html#tensorflow",
    "title": "4: Classification",
    "section": "TensorFlow",
    "text": "TensorFlow\n\nTensorFlow: A system for large-scale machine learning (2016), Abadi et al."
  },
  {
    "objectID": "posts/04_classification/04_classification.html#word2vec",
    "href": "posts/04_classification/04_classification.html#word2vec",
    "title": "4: Classification",
    "section": "Word2Vec",
    "text": "Word2Vec\n\nEfficient Estimation of Word Representations in Vector Space (2013), Mikolov, Chen, Corrado, and Dean"
  },
  {
    "objectID": "posts/04_classification/04_classification.html#data-palmer-penguins",
    "href": "posts/04_classification/04_classification.html#data-palmer-penguins",
    "title": "4: Classification",
    "section": "Data: Palmer Penguins",
    "text": "Data: Palmer Penguins\nThere exist multiple penguin species throughout Antarctica, including the Adelie, Chinstrap, and Gentoo. When encountering one of these penguins on an Antarctic trip, we might classify its species\n\\[Y = \\begin{cases} A & \\text{Adelie} \\\\ C & \\text{Chinstrap} \\\\ G & \\text{Gentoo} \\end{cases}\\]\n\n\n\nthree species\n\n\nExample comes from chapter 14 of Bayes Rules!\n\n\n\nBayes Rules! textbook\n\n\n\\(X_{1}\\) categorical variable: whether the penguin weighs more than the average 4200 grams\n\\[X_{1} = \\begin{cases} 1 & \\text{above-average weight} \\\\ 0 & \\text{below-average weight} \\end{cases}\\]\n\n\n\nAKA culmen length and depth\n\n\nNumerical variables:\n\\[\\begin{array}{rcl}\n  X_{2} & = & \\text{bill length (mm)} \\\\\n  X_{3} & = & \\text{flipper length (mm)} \\\\\n\\end{array}\\]\n\ndata(penguins_bayes)\npenguins &lt;- penguins_bayes\n\nadelie_color = \"#fb7504\"\nchinstrap_color = \"#c65ccc\"\ngentoo_color = \"#067476\"\n\npenguins |&gt;\n  tabyl(species)\n\n   species   n   percent\n    Adelie 152 0.4418605\n Chinstrap  68 0.1976744\n    Gentoo 124 0.3604651"
  },
  {
    "objectID": "posts/04_classification/04_classification.html#motivation",
    "href": "posts/04_classification/04_classification.html#motivation",
    "title": "4: Classification",
    "section": "Motivation",
    "text": "Motivation\nHere, we have three categories, whereas logistic regression is limited to classifying binary response variables. As an alternative, naive Bayes classification\n\ncan classify categorical response variables \\(Y\\) with two or more categories\ndoesn’t require much theory beyond Bayes’ Rule\nit’s computationally efficient, i.e., doesn’t require MCMC simulation\n\nBut why is it called “naive”?"
  },
  {
    "objectID": "posts/04_classification/04_classification.html#one-categorical-predictor",
    "href": "posts/04_classification/04_classification.html#one-categorical-predictor",
    "title": "4: Classification",
    "section": "One Categorical Predictor",
    "text": "One Categorical Predictor\nSuppose an Antarctic researcher comes across a penguin that weighs less than 4200g with a 195mm-long flipper and 50mm-long bill. Our goal is to help this researcher identify the species of this penguin: Adelie, Chinstrap, or Gentoo\n\n\n\n\n\n\n\n\n\n\n\nimage code\n\n\npenguins |&gt;\n  drop_na(above_average_weight) |&gt;\n  ggplot(aes(fill = above_average_weight, x = species)) + \n  geom_bar(position = \"fill\") + \n  labs(title = \"&lt;span style = 'color:#067476'&gt;For which species is a&lt;br&gt;below-average weight most likely?&lt;/span&gt;\",\n       subtitle = \"(focus on the &lt;span style = 'color:#c65ccc'&gt;below-average&lt;/span&gt; category)\",\n       caption = \"SML 301\") +\n  scale_fill_manual(values = c(\"#c65ccc\", \"#fb7504\")) +\n  theme_minimal() +\n  theme(plot.title = element_markdown(face = \"bold\", size = 24),\n        plot.subtitle = element_markdown(size = 16))\n\n\n\nRecall: Bayes Rule\n\\[f(y|x_{1}) = \\frac{\\text{prior}\\cdot\\text{likelihood}}{\\text{normalizing constant}} = \\frac{f(y) \\cdot L(y|x_{1})}{f(x_{1})}\\] where, by the Law of Total Probability,\n\\[\\begin{array}{rcl}\nf(x_{1} & = & \\displaystyle\\sum_{\\text{all } y'} f(y')L(y'|x_{1}) \\\\\n~ & = & f(y' = A)L(y' = A|x_{1}) + f(y' = C)L(y' = C|x_{1}) + f(y' = G)L(y' = G|x_{1}) \\\\\n\\end{array}\\]\nover our three penguin species.\n\n\nCalculation\n\npenguins |&gt; \n  select(species, above_average_weight) |&gt; \n  na.omit() |&gt; \n  tabyl(species, above_average_weight) |&gt; \n  adorn_totals(c(\"row\", \"col\"))\n\n   species   0   1 Total\n    Adelie 126  25   151\n Chinstrap  61   7    68\n    Gentoo   6 117   123\n     Total 193 149   342\n\n\nPrior probabilities:\n\\[f(y = A) = \\frac{151}{342}, \\quad f(y = C) = \\frac{68}{342}, \\quad f(y = G) = \\frac{123}{342}\\]\nLikelihoods:\n\\[\\begin{array}{rcccl}\n  L(y = A | x_{1} = 0) & = & \\frac{126}{151} & \\approx & 0.8344 \\\\\n  L(y = C | x_{1} = 0) & = & \\frac{61}{68} & \\approx & 0.8971 \\\\\n  L(y = G | x_{1} = 0) & = & \\frac{6}{123} & \\approx & 0.0488 \\\\\n\\end{array}\\]\nTotal probability:\n\\[f(x_{1} = 0) = \\frac{151}{342}\\cdot\\frac{126}{151} + \\frac{68}{342}\\cdot\\frac{61}{68} + \\frac{123}{342}\\cdot\\frac{6}{123} = \\frac{193}{342}\\]\nBayes’ Rules:\n\\[\\begin{array}{rcccccl}\n  f(y = A | x_{1} = 0) & = & \\frac{f(y = A) \\cdot L(y = A | x_{1} = 0)}{f(x_{1} = 0)} = \\frac{\\frac{151}{342}\\cdot\\frac{126}{151}}{\\frac{193}{342}} & \\approx & 0.6528 \\\\\n  f(y = C | x_{1} = 0) & = & \\frac{f(y = A) \\cdot L(y = C | x_{1} = 0)}{f(x_{1} = 0)} = \\frac{\\frac{68}{342}\\cdot\\frac{61}{68}}{\\frac{193}{342}} & \\approx & 0.3161 \\\\\n  f(y = G | x_{1} = 0) & = & \\frac{f(y = A) \\cdot L(y = G | x_{1} = 0)}{f(x_{1} = 0)} = \\frac{\\frac{123}{342}\\cdot\\frac{6}{123}}{\\frac{193}{342}} & \\approx & 0.0311 \\\\\n\\end{array}\\]\nThe posterior probability that this penguin is an Adelie is more than double that of the other two species"
  },
  {
    "objectID": "posts/04_classification/04_classification.html#one-numerical-predictor",
    "href": "posts/04_classification/04_classification.html#one-numerical-predictor",
    "title": "4: Classification",
    "section": "One Numerical Predictor",
    "text": "One Numerical Predictor\nLet’s ignore the penguin’s weight for now and classify its species using only the fact that it has a 50mm-long bill\n\n\n\n\n\n\n\n\n\n\n\nimage code\n\n\npenguins|&gt;\n  ggplot(aes(x = bill_length_mm, fill = species)) + \n  geom_density(alpha = 0.7) + \n  geom_vline(xintercept = 50, linetype = \"dashed\", linewidth = 3) + \n  labs(title = \"&lt;span style = 'color:#c65ccc'&gt;For which species is a&lt;br&gt;50mm-long bill the most common?&lt;/span&gt;\",\n       subtitle = \"one numerical predictor\",\n       caption = \"SML 301\") +\n  scale_fill_manual(values = c(adelie_color, chinstrap_color, gentoo_color)) +\n  theme_minimal() +\n  theme(plot.title = element_markdown(face = \"bold\", size = 24),\n        plot.subtitle = element_markdown(size = 16))\n\n\nOur data points to our penguin being a Chinstrap\n\nwe must weigh this data against the fact that Chinstraps are the rarest of these three species\ndifficult to compute likelihood \\(L(y = A | x_{2} = 50)\\)\n\nThis is where one “naive” part of naive Bayes classification comes into play. The naive Bayes method typically assumes that any quantitative predictor, here \\(X_{2}\\), is continuous and conditionally normal:\n\\[\\begin{array}{rcl}\n  X_{2} | (Y = A) & \\sim & N(\\mu_{A}, \\sigma_{A}^{2}) \\\\\n  X_{2} | (Y = C) & \\sim & N(\\mu_{C}, \\sigma_{C}^{2}) \\\\\n  X_{2} | (Y = G) & \\sim & N(\\mu_{G}, \\sigma_{G}^{2}) \\\\\n\\end{array}\\]\n\nPrior Probability Distributions\n\n# Calculate sample mean and sd for each Y group\npenguins |&gt; \n  group_by(species) |&gt; \n  summarize(mean = mean(bill_length_mm, na.rm = TRUE), \n            sd = sd(bill_length_mm, na.rm = TRUE))\n\n# A tibble: 3 × 3\n  species    mean    sd\n  &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie     38.8  2.66\n2 Chinstrap  48.8  3.34\n3 Gentoo     47.5  3.08\n\n\n\npenguins |&gt;\n  ggplot(aes(x = bill_length_mm, color = species)) + \n  stat_function(fun = dnorm, args = list(mean = 38.8, sd = 2.66), \n                aes(color = \"Adelie\"), linewidth = 3) +\n  stat_function(fun = dnorm, args = list(mean = 48.8, sd = 3.34),\n                aes(color = \"Chinstrap\"), linewidth = 3) +\n  stat_function(fun = dnorm, args = list(mean = 47.5, sd = 3.08),\n                aes(color = \"Gentoo\"), linewidth = 3) +\n  ...\n\n\n\n\n\n\n\n\n\n\n\n\nimage code\n\n\npenguins |&gt;\n  ggplot(aes(x = bill_length_mm, color = species)) + \n  stat_function(fun = dnorm, args = list(mean = 38.8, sd = 2.66), \n                aes(color = \"Adelie\"), linewidth = 3) +\n  stat_function(fun = dnorm, args = list(mean = 48.8, sd = 3.34),\n                aes(color = \"Chinstrap\"), linewidth = 3) +\n  stat_function(fun = dnorm, args = list(mean = 47.5, sd = 3.08),\n                aes(color = \"Gentoo\"), linewidth = 3) + \n  geom_vline(xintercept = 50, linetype = \"dashed\") + \n  labs(title = \"&lt;span style = 'color:#c65ccc'&gt;Prior Probabilities&lt;/span&gt;\",\n       subtitle = \"conditionally normal\",\n       caption = \"SML 301\") +\n  scale_color_manual(values = c(adelie_color, chinstrap_color, gentoo_color)) +\n  theme_minimal() +\n  theme(plot.title = element_markdown(face = \"bold\", size = 24),\n        plot.subtitle = element_markdown(size = 16))\n\n\nComputing the likelihoods in R:\n\n# L(y = A | x_2 = 50) = 2.12e-05\ndnorm(50, mean = 38.8, sd = 2.66)\n\n# L(y = C | x_2 = 50) = 0.112\ndnorm(50, mean = 48.8, sd = 3.34)\n\n# L(y = G | x_2 = 50) = 0.09317\ndnorm(50, mean = 47.5, sd = 3.08)\n\nTotal probability:\n\\[f(x_{2} = 50) = \\frac{151}{342} \\cdot 0.0000212 + \\frac{68}{342} \\cdot 0.112 + \\frac{123}{342} \\cdot 0.09317 \\approx 0.05579\\]\nBayes’ Rules:\n\\[\\begin{array}{rcccccl}\n  f(y = A | x_{2} = 50) & = & \\frac{f(y = A) \\cdot L(y = A | x_{1} = 0)}{f(x_{1} = 0)} = \\frac{\\frac{151}{342} \\cdot 0.0000212}{0.05579} & \\approx & 0.0002 \\\\\n  f(y = C | x_{2} = 50) & = & \\frac{f(y = A) \\cdot L(y = C | x_{1} = 0)}{f(x_{1} = 0)} = \\frac{\\frac{68}{342} \\cdot 0.112}{0.05579} & \\approx & 0.3992 \\\\\n  f(y = G | x_{2} = 50) & = & \\frac{f(y = A) \\cdot L(y = G | x_{1} = 0)}{f(x_{1} = 0)} = \\frac{\\frac{123}{342} \\cdot 0.09317}{0.05579} & \\approx & 0.6006 \\\\\n\\end{array}\\]\nThough a 50mm-long bill is relatively less common among Gentoo than among Chinstrap, it follows that our naive Bayes classification, based on our prior information and penguin’s bill length alone, is that this penguin is a Gentoo – it has the highest posterior probability.\nWe’ve now made two naive Bayes classifications of our penguin’s species, one based solely on the fact that our penguin has below-average weight and the other based solely on its 50mm-long bill (in addition to our prior information). And these classifications disagree: we classified the penguin as Adelie in the former analysis and Gentoo in the latter. This discrepancy indicates that there’s room for improvement in our naive Bayes classification method."
  },
  {
    "objectID": "posts/04_classification/04_classification.html#two-predictor-variables",
    "href": "posts/04_classification/04_classification.html#two-predictor-variables",
    "title": "4: Classification",
    "section": "Two Predictor Variables",
    "text": "Two Predictor Variables\n\n\n\n\n\n\n\n\n\n\n\nimage code\n\n\npenguins |&gt;\nggplot(aes(x = flipper_length_mm, y = bill_length_mm, \n           color = species)) + \n  geom_point(size = 3) + \n  geom_segment(aes(x = 195, y = 30, xend = 195, yend = 50),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  geom_segment(aes(x = 170, y = 50, xend = 195, yend = 50),\n               color = \"black\", linetype = 2, linewidth = 2) +\n  labs(title = \"&lt;span style = 'color:#c65ccc'&gt;Two Predictor Variables&lt;/span&gt;\",\n       subtitle = \"50mm-long bill and 195mm-long flipper\",\n       caption = \"SML 301\") +\n  scale_color_manual(values = c(adelie_color, chinstrap_color, gentoo_color)) +\n  theme_minimal() +\n  theme(plot.title = element_markdown(face = \"bold\", size = 24),\n        plot.subtitle = element_markdown(size = 16))\n\n\nGeneralizing Bayes’ Rule:\n\\[f(y | x_{2}, x_{3}) = \\frac{f(y) \\cdot L(y | x_{2}, x_{3})}{\\sum_{y'} f(y') \\cdot L(y' | x_{2}, x_{3})}\\]\nAnother “naive” assumption of conditionally independent:\n\\[L(y | x_{2}, x_{3}) = f(x_{2}, x_{3} | y) = f(x_{2} | y) \\cdot f(x_{3} | y)\\]\n\nmathematically efficient\nbut what about correlation?\n\n\n# sample statistics of x_3: flipper length\npenguins |&gt; \n  group_by(species) |&gt; \n  summarize(mean = mean(flipper_length_mm, na.rm = TRUE), \n            sd = sd(flipper_length_mm, na.rm = TRUE))\n\n# A tibble: 3 × 3\n  species    mean    sd\n  &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie     190.  6.54\n2 Chinstrap  196.  7.13\n3 Gentoo     217.  6.48\n\n\nLikelihoods of a flipper length of 195 mm:\n\n# L(y = A | x_3 = 195) = 0.04554\ndnorm(195, mean = 190, sd = 6.54)\n\n# L(y = C | x_3 = 195) = 0.05541\ndnorm(195, mean = 196, sd = 7.13)\n\n# L(y = G | x_3 = 195) = 0.0001934\ndnorm(195, mean = 217, sd = 6.48)\n\nTotal probability:\n\\[f(x_{2} = 50, x_{3} = 195) = \\frac{151}{342} \\cdot 0.0000212 \\cdot 0.04554 + \\frac{68}{342} \\cdot 0.112 \\cdot 0.05541 + \\frac{123}{342} \\cdot 0.09317 \\cdot 0.0001931 \\approx 0.001241\\]\nBayes’ Rules:\n\\[\\begin{array}{rcccl}\n  f(y = A | x_{2} = 50, x_{3} = 195) & = & \\frac{\\frac{151}{342} \\cdot 0.0000212 \\cdot 0.04554}{0.0001931} & \\approx & 0.0003 \\\\\n  f(y = C | x_{2} = 50, x_{3} = 195) & = & \\frac{\\frac{68}{342} \\cdot 0.112 \\cdot 0.05541}{0.0001931} & \\approx & 0.9944 \\\\\n  f(y = G | x_{2} = 50, x_{3} = 195) & = & \\frac{\\frac{123}{342} \\cdot 0.09317 \\cdot 0.0001931}{0.0001931} & \\approx & 0.0052 \\\\\n\\end{array}\\]\nIn conclusion, our penguin is almost certainly a Chinstrap."
  },
  {
    "objectID": "posts/04_classification/SML_301_session_4.html",
    "href": "posts/04_classification/SML_301_session_4.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "#%pip install palmerpenguins #https://github.com/mcnakhaee/palmerpenguins\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom palmerpenguins import load_penguins\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\npenguins = load_penguins()\ntype(penguins)\npenguins['chinstrap_bool'] = np.where(penguins['species'] == 'Chinstrap', 1, 0)\npenguins.head()\n# remember to remove missing values before a machine learning algorithm\npenguins_subset = penguins[['chinstrap_bool', 'flipper_length_mm', 'bill_length_mm']].dropna()\npenguins_subset.head()"
  },
  {
    "objectID": "posts/04_classification/SML_301_session_4.html#one-predictor-variable",
    "href": "posts/04_classification/SML_301_session_4.html#one-predictor-variable",
    "title": "Data Cleaning",
    "section": "One predictor variable",
    "text": "One predictor variable\n\nX = penguins_subset[['flipper_length_mm']] #explanatory variable\ny = penguins_subset['chinstrap_bool'] #response variable (Boolean)\n\n\n#main code for logistic regression\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=301)\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\n\n# make predictions\ny_pred = logreg.predict(X_test)"
  },
  {
    "objectID": "posts/04_classification/SML_301_session_4.html#confusion-matrix",
    "href": "posts/04_classification/SML_301_session_4.html#confusion-matrix",
    "title": "Data Cleaning",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\nconfusion_mat = confusion_matrix(y_test, y_pred)\nprint(confusion_mat)\n\n\n# metrics from a confusion matrix\nprint(classification_report(y_test, y_pred))"
  },
  {
    "objectID": "posts/04_classification/SML_301_session_4.html#preview-roc-curves",
    "href": "posts/04_classification/SML_301_session_4.html#preview-roc-curves",
    "title": "Data Cleaning",
    "section": "Preview: ROC Curves",
    "text": "Preview: ROC Curves\nROC curves (receiver operating characteristic) are very popular for helping judge the quality of a classification computation.\n\nlogit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()"
  },
  {
    "objectID": "posts/04_classification/SML_301_session_4.html#two-predictor-variables",
    "href": "posts/04_classification/SML_301_session_4.html#two-predictor-variables",
    "title": "Data Cleaning",
    "section": "Two predictor variables",
    "text": "Two predictor variables\n\nX = penguins_subset[['flipper_length_mm', 'bill_length_mm']] #explanatory variables\ny = penguins_subset['chinstrap_bool'] #response variable (Boolean)\n\n\n#main code for logistic regression\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=301)\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\n\n# make predictions\ny_pred = logreg.predict(X_test)\n\n\nconfusion_mat = confusion_matrix(y_test, y_pred)\nprint(confusion_mat)\n\n\n# metrics from a confusion matrix\nprint(classification_report(y_test, y_pred))\n\n\nlogit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()"
  },
  {
    "objectID": "posts/04_classification/SML_301_session_4.html#one-categorical-predictor",
    "href": "posts/04_classification/SML_301_session_4.html#one-categorical-predictor",
    "title": "Data Cleaning",
    "section": "One categorical predictor",
    "text": "One categorical predictor\n\nmean_weight = penguins['body_mass_g'].mean()\npenguins['above_average_weight'] = np.where(penguins['body_mass_g'] &gt; mean_weight, 1, 0)\npenguins_subset = penguins[['species', 'above_average_weight']].dropna()\npenguins_subset.head()\nX = penguins_subset[['above_average_weight']] #explanatory variable\ny = penguins_subset['species'] #response variable (Boolean)\n\n\n# main code for Naive Bayes\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=301)\nNB_model = GaussianNB()\nNB_model.fit(X_train, y_train)\ny_pred = NB_model.predict(X_test)\n\n\n# make one prediction\nprint(NB_model.predict([[0]])) #here, \"0\" for \"below-average weight\"\n\n\nconfusion_mat = confusion_matrix(y_test, y_pred)\nsns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', cbar=False)\n\n\n# metrics from a confusion matrix\nprint(classification_report(y_test, y_pred))"
  },
  {
    "objectID": "posts/04_classification/SML_301_session_4.html#two-predictor-variables-1",
    "href": "posts/04_classification/SML_301_session_4.html#two-predictor-variables-1",
    "title": "Data Cleaning",
    "section": "Two predictor variables",
    "text": "Two predictor variables\n\npenguins_subset = penguins[['species', 'bill_length_mm', 'flipper_length_mm']].dropna()\npenguins_subset.head()\nX = penguins_subset[['bill_length_mm', 'flipper_length_mm']] #explanatory variables\ny = penguins_subset['species'] #response variable (Boolean)\n\n\n# main code for Naive Bayes\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=301)\nNB_model = GaussianNB()\nNB_model.fit(X_train, y_train)\ny_pred = NB_model.predict(X_test)\n\n\n# make one prediction\nprint(NB_model.predict([[50, 195]]))\n\n\nconfusion_mat = confusion_matrix(y_test, y_pred)\nsns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', cbar=False)\n\n\n# metrics from a confusion matrix\nprint(classification_report(y_test, y_pred))"
  },
  {
    "objectID": "posts/05_svm/05_svm.html",
    "href": "posts/05_svm/05_svm.html",
    "title": "5: Support Vector Machines",
    "section": "",
    "text": "Implement a binary classification model using a maximal margin classifier.\nImplement a binary classification model using a support vector classifier.\nImplement a binary classification model using a support vector machine (SVM).\nGeneralize SVM models to multi-class cases.\n\n\n\n\n\n\n\nsupport vector machines!\n\n\n\n\n\n\n\n\nMMC to SVMMermaid code\n\n\nSupport vector machine (SVM), an approach for classification developed in 1990. SVM is a generalizaion of classifiers methods, in particular:\n\nmaximal margin classifier (it requires that the classes be separable by a linear boundary).\nsupport vector classifier\nsupport vector machine: binary classification setting with two classes\n\n\n\n\n\n\n\n\n\n(optional for this course)\n\n# DiagrammR package\nmermaid(\"\ngraph TB\n  A[SVM&lt;br&gt;support vector machine&lt;br&gt;non-linear class boundaries]\n\n  B[MMC&lt;br&gt;maximal margin classifier&lt;br&gt;linear class boundaries] \n  B--&gt;C[SVC&lt;br&gt;support vector classifier&lt;br&gt;applied in a broader range of cases]\n  A--&gt;C\n\n\")"
  },
  {
    "objectID": "posts/05_svm/05_svm.html#learning-objectives",
    "href": "posts/05_svm/05_svm.html#learning-objectives",
    "title": "5: Support Vector Machines",
    "section": "",
    "text": "Implement a binary classification model using a maximal margin classifier.\nImplement a binary classification model using a support vector classifier.\nImplement a binary classification model using a support vector machine (SVM).\nGeneralize SVM models to multi-class cases.\n\n\n\n\n\n\n\nsupport vector machines!"
  },
  {
    "objectID": "posts/05_svm/05_svm.html#overview",
    "href": "posts/05_svm/05_svm.html#overview",
    "title": "5: Support Vector Machines",
    "section": "",
    "text": "MMC to SVMMermaid code\n\n\nSupport vector machine (SVM), an approach for classification developed in 1990. SVM is a generalizaion of classifiers methods, in particular:\n\nmaximal margin classifier (it requires that the classes be separable by a linear boundary).\nsupport vector classifier\nsupport vector machine: binary classification setting with two classes\n\n\n\n\n\n\n\n\n\n(optional for this course)\n\n# DiagrammR package\nmermaid(\"\ngraph TB\n  A[SVM&lt;br&gt;support vector machine&lt;br&gt;non-linear class boundaries]\n\n  B[MMC&lt;br&gt;maximal margin classifier&lt;br&gt;linear class boundaries] \n  B--&gt;C[SVC&lt;br&gt;support vector classifier&lt;br&gt;applied in a broader range of cases]\n  A--&gt;C\n\n\")"
  },
  {
    "objectID": "posts/05_svm/05_svm.html#separating-hyperplane",
    "href": "posts/05_svm/05_svm.html#separating-hyperplane",
    "title": "5: Support Vector Machines",
    "section": "Separating Hyperplane",
    "text": "Separating Hyperplane\n\nConsider a matrix X of dimensions \\(n*p\\), and a \\(y_{i} \\in \\{-1, 1\\}\\). We have a new observation, \\(x^*\\), which is a vector \\(x^* = (x^*_{1}...x^*_{p})^T\\) which we wish to classify to one of two groups.\nWe will use a separating hyperplane to classify the observation.\n\n\n\nWe can label the blue observations as \\(y_{i} = 1\\) and the pink observations as \\(y_{i} = -1\\).\nThus, a separating hyperplane has the property s.t. \\(\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} ... + \\beta_{p}X_{ip} &gt; 0\\) if \\(y_{i} =1\\) and \\(\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} ... + \\beta_{p}X_{ip} &lt; 0\\) if \\(y_{i} = -1\\).\nIn other words, a separating hyperplane has the property s.t. \\(y_{i}(\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} ... + \\beta_{p}X_{ip}) &gt; 0\\) for all \\(i = 1...n\\).\nConsider also the magnitude of \\(f(x^*)\\). If it is far from zero, we are confident in its classification, whereas if it is close to 0, then \\(x^*\\) is located near the hyperplane, and we are less confident about its classification.\n\n\n\n\n\n\n\nConceptual Task 1B\n\n\n\n\n\n\nISLR, Chapter 9, Conceptual Task 1B\n\n\nOn the same plot, sketch the hyperplane \\(-2 + X_{1} + 2X_{2} = 0\\). Indicate the set of points for which \\(-2 + X_{1} + 2X_{2} &gt; 0\\), as well as the set of points for which \\(-2 + X_{1} + 2X_{2} &lt; 0\\).\n\n\nblue: \\(-2 + X_{1} + 2X_{2} &gt; 0\\)\nred: \\(-2 + X_{1} + 2X_{2} &lt; 0\\)"
  },
  {
    "objectID": "posts/05_svm/05_svm.html#maximal-margin-classifier",
    "href": "posts/05_svm/05_svm.html#maximal-margin-classifier",
    "title": "5: Support Vector Machines",
    "section": "Maximal Margin Classifier",
    "text": "Maximal Margin Classifier\n\n\nGenerally, if data can be perfectly separated using a hyperplane, an infinite amount of such hyperplanes exist.\nAn intuitive choice is the maximal margin hyperplane, which is the hyperplane that is farthest from the training data.\nWe compute the perpendicular distance from each training observation to the hyperplane. The smallest of these distances is known as the margin.\nThe maximal margin hyperplane is the hyperplane for which the margin is maximized. We can classify a test observation based on which side of the maximal margin hyperplane it lies on, and this is known as the maximal margin classifier.\nThe maximal margin classifier classifies \\(x^*\\) based on the sign of \\(f(x^*) = \\beta_{0} + \\beta_{1}x^*_{1} + ... + \\beta_{p}x^*_{p}\\).\n\n\n\nNote the 3 training observations that lie on the margin and are equidistant from the hyperplane. These are the support vectors (vectors in \\(p\\)-dimensional space; in this case \\(p=2\\)).\nThey support the hyperplane because if their location was changed, the hyperplane would change.\nThe maximal margin hyperplane depends on these observations, but not the others (unless the other observations were moved at or within the margin).\n\n\n\n\n\n\n\nConceptual Task 3\n\n\n\n\n\n\n\n  obs xvals yvals class_label\n1   1     3     4         Red\n2   2     2     2         Red\n3   3     4     4         Red\n4   4     1     4         Red\n5   5     2     1        Blue\n6   6     4     3        Blue\n7   7     4     1        Blue\n\n\n\nWe are given \\(n = 7\\) observations in \\(p = 2\\) dimensions. For each observation, there is an associated class label.\n\n\n\n\n\n\n\n\n\n\n\nSketch the optimal separating hyperplane, and provide the equation for this hyperplane\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nblue: \\(0.5 - X_{1} + X_{2} &lt; 0\\)\nred: \\(0.5 - X_{1} + X_{2} &gt; 0\\)\n\n\nmaximal margin in indicated by the dashed lines, with margin\n\n\\[M = \\frac{0.5}{\\sqrt{2}} \\approx 0.3536\\]\n\nIndicate the support vectors for the maximal margin classifier.\n\n\n\n\n\n\n\n\n\n\n\nArgue that a slight movement of the seventh observation would not affect the maximal margin hyperplane.\nSketch a hyperplane that is not the optimal separating hyperplane, and provide the equation for this hyperplane.\n\n\n\n\n\n\n\n\n\n\n\nDraw an additional observation on the plot so that the two classes are no longer separable by a hyperplane."
  },
  {
    "objectID": "posts/05_svm/05_svm.html#mathematics-of-the-mmc",
    "href": "posts/05_svm/05_svm.html#mathematics-of-the-mmc",
    "title": "5: Support Vector Machines",
    "section": "Mathematics of the MMC",
    "text": "Mathematics of the MMC\n\nConsider constructing an MMC based on the training observations \\(x_{1}...x_{n} \\in \\mathbb{R}^p\\). This is the solution to the optimization problem:\n\n\\[\\text{max}_{\\beta_{0}...\\beta_{p}, M} \\space M\\] \\[\\text{subject to } \\sum_{j=1}^{p}\\beta_{j}^2 = 1\\] \\[y_{i}(\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} ... + \\beta_{p}X_{ip}) \\geq M \\quad \\forall i = 1...n\\]\n\n\\(M\\) is the margin, and the \\(\\beta\\) coeffients are chosen to maximize \\(M\\).\nThe constraint (3rd equation) ensures that each observation will be correctly classified, as long as M is positive.\n\n\n\nThe 2nd and 3rd equations ensure that each data point is on the correct side of the hyperplane and at least M-distance away from the hyperplane.\nThe perpendicular distance to the hyperplane is given by \\(y_{i}(\\beta_{0} + \\beta_{1}x_{i1} + \\beta_{2}x_{i2} ... + \\beta_{p}x_{ip})\\).\n\n\nBut what if our data is not separable by a linear hyperplane?\n\n\n\nIndividual data points greatly affect formation of the maximal margin classifier"
  },
  {
    "objectID": "posts/05_svm/05_svm.html#mathematics-of-the-svc",
    "href": "posts/05_svm/05_svm.html#mathematics-of-the-svc",
    "title": "5: Support Vector Machines",
    "section": "Mathematics of the SVC",
    "text": "Mathematics of the SVC\n\nThe SVC classifies a test observation based on which side of the hyperplane it lies.\n\n\\[\\text{max}_{\\beta_{0}...\\beta_{p}, \\epsilon_{1}...\\epsilon_{n}, M} \\space M\\] \\[\\text{subject to } \\sum_{j=1}^{p}\\beta_{j}^2 = 1\\] \\[y_{i}(\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} ... + \\beta_{p}X_{ip}) \\geq M(1 - \\epsilon_{i})\\] \\[\\epsilon_{i} \\geq 0, \\quad \\sum_{i=1}^{n}\\epsilon_{i} \\leq C\\]\n\n\\(C\\) is a nonnegative tuning parameter, typically chosen through cross-validation, and can be thought of as the budget for margin violation by the observations.\nThe \\(\\epsilon_{i}\\) are slack variables that allow individual observations to be on the wrong side of the margin or hyperplane. The \\(\\epsilon_{i}\\) indicates where the \\(i^{\\text{th}}\\) observation is located with regards to the margin and hyperplane.\n\nIf \\(\\epsilon_{i} = 0\\), the observation is on the correct side of the margin.\nIf \\(\\epsilon_{i} &gt; 0\\), the observation is on the wrong side of margin\nIf \\(\\epsilon_{i} &gt; 1\\), the observation is on the wrong side of the hyperplane.\n\nSince \\(C\\) constrains the sum of the \\(\\epsilon_{i}\\), it determines the number and magnitude of violations to the margin. If \\(C=0\\), there is no margin for violation, thus all the \\(\\epsilon_{1},...,\\epsilon_{n} = 0\\).\nNote that if \\(C&gt;0\\), no more than \\(C\\) observations can be on wrong side of hyperplane, since in these cases \\(\\epsilon_{i} &gt; 1\\)."
  },
  {
    "objectID": "posts/05_svm/05_svm.html#tuning-parameter",
    "href": "posts/05_svm/05_svm.html#tuning-parameter",
    "title": "5: Support Vector Machines",
    "section": "Tuning Parameter",
    "text": "Tuning Parameter\n\n\nA property of the classifier is that only data points which lie on or violate the margin will affect the hyperplane. These data points are known as support vectors.\n\\(C\\) controls the bias-variance tradeoff of the classifier.\n\nWhen \\(C\\) is large: high bias, low variance\nWhen \\(C\\) is small: low bias, high variance\n\nThe property of the SVC solely being dependent on certain observations in classification differs from other classification methods such as LDA (depends on mean of all observations in each class, as well as each class’s covariance matrix using all observations).\nHowever, logistic regression is more similar to SVC in that it has low sensitivity to observations far from the decision boundary."
  },
  {
    "objectID": "posts/05_svm/05_svm.html#nonlinear-classification",
    "href": "posts/05_svm/05_svm.html#nonlinear-classification",
    "title": "5: Support Vector Machines",
    "section": "Nonlinear Classification",
    "text": "Nonlinear Classification\n\nMany decision boundaries are not linear.\nWe could fit an SVC to the data using \\(2p\\) features (in the case of \\(p\\) features and using a quadratic form).\n\n\\[X_{1}, X_{1}^{2}, \\quad X_{2}, X_{2}^{2}, \\quad\\cdots, \\quad X_{p}, X_{p}^{2}\\]\n\\[\\text{max}_{\\beta_{0},\\beta_{11},\\beta_{12},\\dots,\\beta_{p1},\\beta_{p2} \\epsilon_{1},\\dots,\\epsilon_{n}, M} \\space M\\] \\[\\text{subject to }  y_{i}\\left(\\beta_{0} + \\sum_{j=1}^{p} \\beta_{ji}x_{ji} + \\sum_{j=1}^{p} \\beta_{ji}x_{ji}^{2}\\right) \\geq M(1 - \\epsilon_{i})\\]\n\\[\\epsilon_{i} \\geq 0, \\quad \\sum_{i=1}^{n}\\epsilon_{i} \\leq C, \\quad \\sum_{j=1}^{p}\\sum_{k=1}^{2} \\beta_{jk}^{2} = 1\\]\n\nNote that in the enlarged feature space (here, with the quadratic terms), the decision boundary is linear. But in the original feature space, it is quadratic \\(q(x) = 0\\) (in this example), and generally the solutions are not linear.\nOne could also include interaction terms, higher degree polynomials, etc., and thus the feature space could enlarge quickly and entail unmanageable computations.\n\n\n\n\n\n\n\nConceptual Task 2\n\n\n\n\n\nWe now investigate a non-linear decision boundary.\n\nblue: \\((1 + X_{1})^{2} + (2 - X_{2})^{2} &gt; 4\\)\nred: \\((1 + X_{1})^{2} + (2 - X_{2})^{2} &lt; 4\\)\n\n\n\n\n\n\n\n\n\n\n\nTo what class is the observation (0, 0) classified? (−1, 1)? (2, 2)? (3, 8)?\n\n\nifelse(euclidean_distance(0, 0, -1, 2) &gt; 4, \"blue\", \"red\")\n\n[1] \"red\"\n\nifelse(euclidean_distance(-1, 1, -1, 2) &gt; 4, \"blue\", \"red\")\n\n[1] \"red\"\n\nifelse(euclidean_distance(2, 2, -1, 2) &gt; 4, \"blue\", \"red\")\n\n[1] \"red\"\n\nifelse(euclidean_distance(3, 8, -1, 2) &gt; 4, \"blue\", \"red\")\n\n[1] \"blue\"\n\n\n\nWhile the decision boundary\n\n\\[(1 + X_{1})^{2} + (2 - X_{2})^{2} = 4\\]\nis not linear in \\(X_{1}\\) and \\(X_{2}\\), it is linear in terms of \\(X_{1}\\), \\(X_{1}^{2}\\), \\(X_{2}\\), \\(X_{2}^{2}\\)\n\\[\\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{1}^{2} + \\beta_{4}X_{2}^{2} = 0\\]\nwith \\(\\beta_{0} = 1\\), \\(\\beta_{1} = 2\\), \\(\\beta_{2} = -4\\), \\(\\beta_{3} = 1\\), and \\(\\beta_{4} = 1\\)."
  },
  {
    "objectID": "posts/05_svm/05_svm.html#radial-kernels",
    "href": "posts/05_svm/05_svm.html#radial-kernels",
    "title": "5: Support Vector Machines",
    "section": "Radial Kernels",
    "text": "Radial Kernels\n\n\n\nAKA: Gaussians, image credit: Manin Bocss\n\n\n\nThere are other options besides polynomial kernel functions, and a popular one is a radial kernel.\n\n\\[K(x, x_{i}) = \\text{exp}\\left(-\\gamma\\sum_{j=1}^p(x_{ij} - x_{i'j})^2\\right), \\quad \\gamma &gt; 0\\]\n\nFor a given test observations \\(x^*\\), if it is far from \\(x_{i}\\), then \\(K(x^*, x_{i})\\) will be small given the negative \\(\\gamma\\) and large \\(\\sum_{j=1}^p(x^*_{j} - x_{ij})^2)\\).\nThus, \\(x_{i}\\) will play little role in \\(f(x^*)\\).\nThe predicted class for \\(x^*\\) is based on the sign of \\(f(x^*)\\), so training observations far from a given test point play little part in determining the label for a test observation.\nThe radial kernel therefore exhibits local behavior with respect to other observations."
  },
  {
    "objectID": "posts/05_svm/05_svm.html#svm-with-radial-kernels",
    "href": "posts/05_svm/05_svm.html#svm-with-radial-kernels",
    "title": "5: Support Vector Machines",
    "section": "SVM with Radial Kernels",
    "text": "SVM with Radial Kernels\n\n\n\nimage credit: Manin Bocss\n\n\n\nThe advantage of using a kernel rather than simply enlarging feature space is computational, since it is only necessary to compute \\(\\binom{n}{2}\\) kernel functions.\nFor radial kernels, the feature space is implicit and infinite dimensional, so we could not do the computations in such a space anyways."
  }
]