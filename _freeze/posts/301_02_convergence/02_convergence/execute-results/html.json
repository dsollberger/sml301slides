{
  "hash": "2e2f93ae287e98470b67c94c38b7e9c6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"2: Convergence\"\nauthor: \"Derek Sollberger\"\ndate: \"2025-01-29\"\nformat:\n  html:\n    toc: true\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n\n# Session 2: Convergence\n\n## Start\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n* **Goal**: Discuss convergence\n\n* **Objective**: Explore some Python codes about root finding and stochastic processes\n:::\n\n::: {.column width=\"40%\"}\nAs we get started, try to load a session in Google Colab\n:::\n\n::::\n\n# Activity: TF Playground\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n## TensorFlow Playground\n\n* link: [https://playground.tensorflow.org](https://playground.tensorflow.org)\n* explore the various menus and buttons\n* feel free to run a simulation\n\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"30%\"}\n![TensorFlow Playground](Tensorflow_playground.PNG)\n:::\n\n::::\n\n# Big Idea\n\nEventually we will discuss upon how neural network weights are found via stochastic gradient descent\n\n* stochastic?\n* gradient?\n* descent?\n\n# Little Perturbations\n\n::::: {.panel-tabset}\n\n## rounding\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n### 2 + 2 = 5\n\n... for extremely large values of 2\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![tshirt](2_2_5.png)\n:::\n\n::::\n\n## truncation\n\n![truncation error](truncation-errors.png)\n\n* image source: [BYJUs](https://byjus.com/maths/truncation-errors/)\n\n## consideration\n\nNumerical analysis: many computational algorithms have been designed to mitigate the possible propogation of rounding and truncation errors.\n\n* example: [the quadratic formula](https://zingale.github.io/comp_astro_tutorial/basics/floating-point/numerical_error.html)\n\n:::::\n\n::: {.callout-caution collapse=\"true\"}\n## Dev Corner: Binary\n\nResearch teams are still affected by these rounding and truncation errors\n\n* different operating systems (Mac, Windows, Linux)\n* different processor chips (Intel, AMD)\n\nTip: data engineers can store computation results in their pure binary representations\n\n:::\n\n\n# Stochastic\n\n* **Stochastic**: word of Greek origin, meaning to guess at\n\n## Sequences\n\n* index: $n \\in \\mathbb{N} = \\{1, 2, 3, 4, 5, ...\\}$\n* formulaic: f(n) = 2n - 1\n\n$$1, 3, 5, 7, 9, ...$$\n\n* constructive:\n\n$$3, 3.1, 3.14, 3.141, 3.1415, 3.14159, ...$$\n\n* shapes:\n\n![triangular numbers](triangular-numbers.png)\n\n* image source: [BYJUs](https://byjus.com/maths/triangular-numbers/)\n\n\n## Random Variables\n\nA **random variable** has no set value, but rather represents an element of chance.  We can better understand a random variable through statistics like\n\n* mean\n* variance\n* distribution\n\n::: {.callout-tip collapse=\"true\"}\n## Stochastic Process\n\nA **stochastic process** is a *sequence* of *random variables*\n:::\n\n\n# Gradient\n\nFor a multivariate function $f(\\vec{x})$, the **gradient** is a vector of partial derivatives\n\n$$\\nabla f = \\left[ \\frac{\\partial}{\\partial x_{1}}, \\frac{\\partial}{\\partial x_{2}}, \\frac{\\partial}{\\partial x_{3}}, ..., \\frac{\\partial}{\\partial x_{n}}  \\right]$$\n\n\n# Descent\n\n::::: {.panel-tabset}\n\n## Scenario\n\nFind the roots of the function\n\n$$f(x) = x^{3} - 12x^{2} + 44x - 48$$\n\nIn other words, find the solutions of the equation\n\n$$f(x) = x^{3} - 12x^{2} + 44x - 48 = 0$$\n\n## Bisection\n\n![Bisection Method](bisection_method.png)\n\n* image source: [Python Numerical Methods](https://pythonnumericalmethods.studentorg.berkeley.edu/notebooks/chapter19.03-Bisection-Method.html)\n\n## code\n\n```\ndef bisection_method(f, a, b, tol):\n  # tries to find a root of f between a and b (i.e. f(root) = 0)\n  # Inputs: real numbers a, b; tolerance\n  # Output: root\n\n  # check if root is in the interval (i.e. Intermediate Value Theorem)\n    if np.sign(f(a)) == np.sign(f(b)):\n        raise Exception(\"The endpoints do not contain a root\")\n\n  # initialization\n    iter_num = 1\n\n    while (b - a) > tol:\n      c = (a + b) / 2\n\n      print(\"iter_num: \" + str(iter_num) + \", midpoint: \" + str(c))\n\n      if f(c) * f(a) < 0:\n          b = c\n      else:\n          a = c\n\n      iter_num += 1\n\n    print(\"Root:\", c)\n\n```\n\n```\nf = lambda x: x**3 - 12*x**2  + 44*x - 48\n```\n\n\nTry calling the `bisection_method` with different initial values for a and b, such as\n\n*    [0, 3]\n*    [3, 7]\n*    [5, 10]\n\n## Newton's\n\n![Newton's Method](Newtons_Method.png)\n\n* image source: [Paul's Online Notes](https://tutorial.math.lamar.edu/classes/calci/newtonsmethod.aspx)\n\n## code\n\n```\ndef Newton_Method(f, f_prime, x_0, tol, max_iter):\n  # tries to find a root of f between a and b (i.e. f(root) = 0)\n  # Inputs: function f, derivative function f_prime,\n  # initial guess x_0, tolerance tol, and maximum number of iterations\n  # Output: root\n\n  # initialization\n    iter_num = 1\n    x_n = x_0\n\n    while (abs(f(x_n)) > tol) & (iter_num <= max_iter):\n      print(\"iter_num: \" + str(iter_num) + \", guess: \" + str(x_n))\n      f_x = f(x_n)\n      f_prime_x = f_prime(x_n)\n\n      x_n = x_n - f_x / f_prime_x\n      iter_num += 1\n\n    print(\"Root:\", x_n)\n```\n\n```\nf_prime = lambda x: 3*x**2 - 24*x + 44\n```\n\nTry calling Newton_Method with a few different guesses for the initial value\n\n## Cauchy\n\nAn infinite sequence $\\{x_{i}\\}_{i=1}^{\\infty}$ is **Cauchy convergent** if\n\n$$\\forall \\epsilon \\exists N \\text{ such that } \\forall m,n > N$$\n$$|x_{m} - x_{n}| < \\epsilon$$\n\n:::::\n\n\n# Markov Chains\n\nA **Markov chain** is a stochastic process whose state depends only on the immediate previous iteration.\n\n$$P_{ij} = P(X_{n} = j | X_{n-1} = i)$$\n\n## Application: Dinner Choices\n\nSuppose that we have a Princeton student whose behavior includes eating only three types of dinner: \n\n$$S = \\{\\text{ramen}, \\text{pizza}, \\text{sushi}\\}$$\n\nwith transition matrix\n\n$$P = \\left(\\begin{array}{ccc}\n0.2 & 0.4 & 0.4 \\\\\n0.3 & 0.4 & 0.3 \\\\\n0.2 & 0.2 & 0.6\n\\end{array}\\right)$$\n\n![dinner choices network](dinner_network.drawio.png)\n\n::: {.callout-note collapse=\"true\"}\n## Network terminology\n\n* directed versus undirected graphs\n* cyclic versus acyclic graphs\n\nLater studies focus on **DAGs**: directed, acyclic network graphs\n:::\n\nSuppose that, on a Monday, the student's preferences are\n\n$$x_0 = \\left(\\begin{array}{ccc} 0.5 & 0.25 & 0.25 \\end{array}\\right)$$\n\n* What is the probability that the student will eat ramen on Tuesday (i.e. the next day)?\n* What is the probability that the student will eat pizza on Wednesday (i.e. two days later)?\n* What is the long-term dinner-choice behavior of this student?\n\n\n# Quo Vadimus?\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n* Please read the weekly announcement in Canvas\n* due this Friday (5 PM):\n\n    - Precept 1\n    - CLO Assessment (survey)\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n* tip: talk about your career and research goals with your instructors\n* (optional) If you have seen the Bisection and Newton's Methods before, let me know which class(es) cover that material.\n\n:::\n\n::::\n\n\n# Footnotes\n\n::: {.callout-note collapse=\"true\"}\n\n## (optional) Additional Resources\n\n* [Bisection Method](https://flexiple.com/python/bisection-method-python) by Harsh Pandey\n* [Newton's Method](https://flexiple.com/python/newton-raphson-method-python) by Harsh Pandey\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.3 forcats_1.0.0   stringr_1.5.1   dplyr_1.1.4    \n [5] purrr_1.0.2     readr_2.1.5     tidyr_1.3.1     tibble_3.2.1   \n [9] ggplot2_3.5.1   tidyverse_2.0.0 gt_0.11.1      \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6      jsonlite_1.8.9    compiler_4.4.2    tidyselect_1.2.1 \n [5] xml2_1.3.6        scales_1.3.0      yaml_2.3.10       fastmap_1.2.0    \n [9] R6_2.5.1          generics_0.1.3    knitr_1.49        htmlwidgets_1.6.4\n[13] munsell_0.5.1     pillar_1.10.1     tzdb_0.4.0        rlang_1.1.5      \n[17] stringi_1.8.4     xfun_0.50         timechange_0.3.0  cli_3.6.3        \n[21] withr_3.0.2       magrittr_2.0.3    digest_0.6.37     grid_4.4.2       \n[25] rstudioapi_0.17.1 hms_1.1.3         lifecycle_1.0.4   vctrs_0.6.5      \n[29] evaluate_1.0.3    glue_1.8.0        colorspace_2.1-1  rmarkdown_2.29   \n[33] tools_4.4.2       pkgconfig_2.0.3   htmltools_0.5.8.1\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}