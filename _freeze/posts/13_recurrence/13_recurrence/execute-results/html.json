{
  "hash": "c6e42539c694ae8a942124929d7f4773",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"13: Recurrence\"\nauthor: \"Derek Sollberger\"\ndate: \"2025-10-22\"\nformat:\n  html:\n    toc: true\n---\n\n::: {.cell}\n\n:::\n\n\n\n# Session 13: Recurrence\n\n## Learning objectives:\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- RNN\n- LSTM\n- GRU\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n![Washington Ave crosswalk](crosswalk.png)\n:::\n\n::::\n\n# More Neural Network Concepts\n\n## Cross Entropy\n\nRecall, a step size in back propagation is\n\n$$\\text{step size} = \\text{derivative} \\cdot \\text{learning rate}$$\n\n::: {.callout-note}\n## Cross Entropy Loss\n\nWhen classifying among $C$ classes, for an array of predictions (after computing a softmax) $\\vec{s}$ and its associated vector of true observations $\\vec{y}$, the **cross entropy loss** is calculated as\n\n$$L(\\vec{s}, \\vec{y}) = -\\sum_{i=1}^{C} y_{i}\\log s_{i}$$\n:::\n\n![cross entropy loss](cross_entropy_graph.png)\n\n* logarithm of softmax\n* larger derivative values for larger misclassification values\n* larger step sizes for larger misclassification values\n* image source: [Machine Learning Glossary](https://ml-cheatsheet.readthedocs.io/en/latest/index.html)\n\n::: {.callout-tip}\n## Cross Entropy Inference\n\nAs a logarithm (a monotonic transformation) of softmax and similar to the sum-of-squared residuals (SSR), we are likewise seeking lower values of errors.\n\n* smaller cross entropy $\\rightarrow$ better network\n:::\n\n\n\n\n\n\n\n\n\n\n\n## Math Example\n\nWe start with a sequence of natural numbers\n\n$$\\{8, 9, 10, ..., 100\\}$$\nand we will use a neural network to try to classify the numbers as prime numbers or composite numbers. In this simple example, the inputs are indicator variables\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n* `div2`: number is divisible by 2\n* `div3`: number is divisible by 3\n* `div5`: number is divisible by 5\n* `div7`: number is divisible by 7\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\nand our network looks like\n\n\n\n```{mermaid}\nflowchart LR\n\ninput_1[div2]\ninput_2[div3]\ninput_3[div5]\ninput_4[div7]\noutput_1[composite]\noutput_2[prime]\n\ninput_1 --> output_1\ninput_1 --> output_2\ninput_2 --> output_1\ninput_2 --> output_2\ninput_3 --> output_1\ninput_3 --> output_2\ninput_4 --> output_1\ninput_4 --> output_2\n```\n\n\n:::\n\n::::\n\n> What do you think will happen to the *weights* and bias values upon training the network?\n\nAfter running the code for 100 epochs, the network calculations so far look like\n\n$$Wx + b$$\n\n$$\\left[\\begin{array}{rrrr}\n  0.92 & 0.34 & 0.79 & 0.68 \\\\\n  -0.40 & -0.45 & 0.80 & -0.36 \\\\\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n  x_{1} \\\\ x_{2} \\\\ x_{3} \\\\ x_{4} \\\\\n\\end{array}\\right]\n+\n\\left[\\begin{array}{c}\n  0.92 \\\\ 0.42 \\\\\n\\end{array}\\right]$$\n\n\n## Hidden Layer\n\nTo try to capture abstract notions with a neural network, we can employ a **hidden layer** between the input and output layers.\n\n::: {.callout-note collapse=\"true\"}\n## Python code\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nclass NN1H(pl.LightningModule):\n  # Neural Network object\n  # assumes one hidden layer (and default learning rate: 0.01)\n  # inheirited class from LightningModule (for faster computations)\n\n    def __init__(self, input_layer_size, hidden_layer_size,\n                 output_layer_size, learning_rate = None):\n        super().__init__()\n        self.input_layer_size = input_layer_size #input layer size (number of explanatory variables)\n        self.hidden_layer_size = hidden_layer_size\n        self.learning_rate = learning_rate if learning_rate is not None else 0.01\n        self.output_layer_size = output_layer_size #output layer size: 3 (number of penguin species)\n        self.fc1 = nn.Linear(input_layer_size, hidden_layer_size)\n        self.fc2 = nn.Linear(hidden_layer_size, output_layer_size)\n        self.test_step_outputs = []\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n    def training_step(self, batch, batch_idx): # what happen in each train step\n        x, y = batch\n        output = self(x)\n        loss = F.cross_entropy(output, y)\n        # self.log('train_loss', loss, on_epoch=True) # use this for logging (e.g. using TensorBoard)\n        return {'loss':loss}\n\n    def test_step(self, batch, batch_idx): # what happen in each test step\n        x, y = batch\n        output = self(x)\n        loss = F.cross_entropy(output, y)\n        self.test_step_outputs.append(loss)\n        return {'loss':loss}\n\n    def on_test_epoch_end(self):\n        epoch_average = torch.stack(self.test_step_outputs).mean()\n        self.log(\"test_epoch_average\", epoch_average)\n        self.test_step_outputs.clear()  # free memory\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.SGD(self.parameters(), lr = lr)\n        return optimizer\n```\n:::\n\n\n:::\n\nOnce again, let us deploy the `palmerpenguins`:\n\n* $x_{1}$: bill length (mm)\n* $x_{2}$: bill depth (mm)\n* $x_{3}$: flipper length (mm)\n* $x_{4}$: body mass (g)\n\n![neural network](NN_4_2_3.png)\n\n$$\\text{min-max normalization} \\rightarrow Wx + b \\rightarrow \\text{ReLU} \\rightarrow Wx + b \\rightarrow \\text{cross entropy}$$\n\nAfter 100 epochs, our weights and bias values are\n\n$$\\begin{array}{c}\n\\text{min-max normalization} \\\\ \n\\downarrow \\\\\n\\left[\\begin{array}{rrrr}\n  0.04 & -1.19 & 0.96 & 0.91 \\\\\n  -1.59 & 0.65 & 0.02 & 0.16 \\\\\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n  x_{1} \\\\ x_{2} \\\\ x_{3} \\\\ x_{4} \\\\\n\\end{array}\\right]\n+\n\\left[\\begin{array}{r}\n  0.55 \\\\ 0.62 \\\\\n\\end{array}\\right] \\\\\n\\downarrow \\\\\n\\text{ReLU} \\\\\n\\downarrow \\\\\n\\left[\\begin{array}{rr}\n  -1.00 & 1.17 \\\\\n  -1.07 & -1.21 \\\\\n  1.08 & -0.74 \\\\\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n  x_{1} \\\\ x_{2} \\\\ \n\\end{array}\\right]\n+\n\\left[\\begin{array}{r}\n  -0.29 \\\\ 1.28 \\\\ -1.38 \\\\\n\\end{array}\\right] \\\\\n\\downarrow \\\\\n\\text{cross entropy}\n\\end{array}$$\n\n\n## Hidden Layer Size\n\n::: {.callout-note collapse=\"true\"}\n## How do we decide on a hidden layer size?\n\nIndustry professionals and writers suggest powers of two:\n\n$$d_{\\text{model}} = 2, 4, 8, 16, ...$$\n\nand choose a model with a low cross-entropy result (i.e. when increasing the hidden layer size barely reduces the cross-entropy amount).\n\n* MNIST paper: $d_{\\text{model}} = 16$\n* Attention paper: $d_{\\text{model}} = 1024$\n\n:::\n\n::::: {.panel-tabset}\n\n## 2\n\n![h = 2](NN_4_2_3.png)\n\n* cross-entropy: 0.0800\n\n## 4\n\n![h = 4](NN_4_4_3.png)\n\n* cross-entropy: 0.0509\n\n## 8\n\n![h = 8](NN_4_8_3.png)\n\n* cross-entropy: 0.0599\n\n## 16\n\n![h = 16](NN_4_16_3.png)\n\n* cross-entropy: 0.0280\n\n:::::\n\n\n# Deep Learning\n\n::: {.callout-note}\n## Deep Learning\n\nIn our studies of artificial learning, **deep learning** is using multiple hidden layers.\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Python Code\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nclass NN2H(pl.LightningModule):\n  # Neural Network object\n  # assumes two hidden layers (and default learning rate: 0.01)\n  # inheirited class from LightningModule (for faster computations)\n\n    def __init__(self, input_layer_size, h1, h2,\n                 output_layer_size, learning_rate = None):\n        super().__init__()\n        self.input_layer_size = input_layer_size #input layer size (number of explanatory variables)\n        self.h1 = h1\n        self.h2 = h2\n        self.learning_rate = learning_rate if learning_rate is not None else 0.01\n        self.output_layer_size = output_layer_size #output layer size: 3 (number of penguin species)\n        self.fc1 = nn.Linear(input_layer_size, h1)\n        self.fc2 = nn.Linear(h1, h2)\n        self.fc3 = nn.Linear(h2, output_layer_size)\n        self.test_step_outputs = []\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n    def training_step(self, batch, batch_idx): # what happen in each train step\n        x, y = batch\n        output = self(x)\n        loss = F.cross_entropy(output, y)\n        # self.log('train_loss', loss, on_epoch=True) # use this for logging (e.g. using TensorBoard)\n        return {'loss':loss}\n\n    def test_step(self, batch, batch_idx): # what happen in each test step\n        x, y = batch\n        output = self(x)\n        loss = F.cross_entropy(output, y)\n        self.test_step_outputs.append(loss)\n        return {'loss':loss}\n\n    def on_test_epoch_end(self):\n        epoch_average = torch.stack(self.test_step_outputs).mean()\n        self.log(\"test_epoch_average\", epoch_average)\n        self.test_step_outputs.clear()  # free memory\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.SGD(self.parameters(), lr = lr)\n        return optimizer\n```\n:::\n\n\n:::\n\n![deep learning](NN_4_2_4_3.png)\n\n\n# Recurrent Neural Networks\n\n::: {.callout-note}\n## Recurrent Neural Networks\n\nIn order to move toward retaining some *memory* in our inputs to artificial intelligence models, we will here briefly discuss **recurrent neural networks**.\n\n\n\n```{mermaid}\nflowchart LR\n\ninput1[input]\n\nneuron1[neuron]\n\noutput1[output]\n\ninput1 --> neuron1\nneuron1 --> output1\nneuron1 --> neuron1\n```\n\n\n\nAs we *unfold* the network, we use the same weights and bias values between the layers.\n\n:::\n\n## Example: Crosswalk\n\n![Washington Ave crosswalk](crosswalk_meme.png)\n\n* image source: [NYT](https://www.nytimes.com/2020/09/17/us/princeton-racism-federal-investigation.html)\n\n\nWe start with a sequence that has \n\n> Warning lights activated. Vehicles may not stop. Cross with caution. Warning lights activated. Vehicles may not stop. Cross with caution. Warning lights activated. Vehicles may not stop. Cross with caution. \n\nrepeated over and over, and we hope that\n\n* input \"Warning\" leads to a prediction of \"lights\"\n* input \"lights\" leads to a prediction of \"activated\"\n* input \"activated\" leads to a prediction of \"Vehicles\"\n* input \"Vehicles\" leads to a prediction of \"may\"\n* input \"may\" leads to a prediction of \"stop\"\n* input \"stop\" leads to a prediction of \"Cross\"\n* input \"Cross\" leads to a prediction of \"with\"\n* input \"with\" leads to a prediction of \"caution\"\n* input \"caution\" leads to a prediction of \"Warning\"\n\n## Unfolding\n\n\n\n### Warning\n\n* Does input \"Warning\" lead to a prediction of \"lights\"?\n\n\n\n```{mermaid}\nflowchart TD\n\ninput1[Warning]\n\nneuron1[ReLU]\n\noutput1[lights]\n\ninput1 -- 0.20x + 0.86 --> neuron1\n\nneuron1 -- 0.91x - 0.25 --> output1\n```\n\n\n\n\n\n### lights\n\n* Does input \"lights\" leads to a prediction of \"activated\"?\n\n\n\n```{mermaid}\nflowchart TD\n\ninput1[Warning]\ninput2[lights]\n\nneuron1[ReLU]\nneuron2[ReLU]\n\noutput1[NA]\noutput2[activated]\n\ninput1 -- 0.20x + 0.86 --> neuron1\ninput2 -- 0.20x + 0.86 --> neuron2\n\nneuron1 -- 0.91x - 0.25 --> output1\nneuron2 -- 0.91x - 0.25 --> output2\n\nneuron1 -- -0.85x + 0.81 --> neuron2\n```\n\n\n\n\n\n### activated\n\n* Does input \"activated\" leads to a prediction of \"Vehicles\"?\n\n\n\n```{mermaid}\nflowchart TD\n\ninput1[Warning]\ninput2[lights]\ninput3[activated]\n\nneuron1[ReLU]\nneuron2[ReLU]\nneuron3[ReLU]\n\noutput1[NA]\noutput2[NA]\noutput3[Vehicles]\n\ninput1 -- 0.20x + 0.86 --> neuron1\ninput2 -- 0.20x + 0.86 --> neuron2\ninput3 -- 0.20x + 0.86 --> neuron3\n\nneuron1 -- 0.91x - 0.25 --> output1\nneuron2 -- 0.91x - 0.25 --> output2\nneuron3 -- 0.91x - 0.25 --> output3\n\nneuron1 -- -0.85x + 0.81 --> neuron2\nneuron2 -- -0.85x + 0.81 --> neuron3\n```\n\n\n\n\n\n### Vehicles\n\n* Does input \"Vehicles\" leads to a prediction of \"may\"?\n\n\n\n```{mermaid}\nflowchart TD\n\ninput1[Warning]\ninput2[lights]\ninput3[activated]\ninput4[Vehicles]\n\nneuron1[ReLU]\nneuron2[ReLU]\nneuron3[ReLU]\nneuron4[ReLU]\n\noutput1[NA]\noutput2[NA]\noutput3[NA]\noutput4[may]\n\ninput1 -- 0.20x + 0.86 --> neuron1\ninput2 -- 0.20x + 0.86 --> neuron2\ninput3 -- 0.20x + 0.86 --> neuron3\ninput4 -- 0.20x + 0.86 --> neuron4\n\nneuron1 -- 0.91x - 0.25 --> output1\nneuron2 -- 0.91x - 0.25 --> output2\nneuron3 -- 0.91x - 0.25 --> output3\nneuron4 -- 0.91x - 0.25 --> output4\n\nneuron1 -- -0.85x + 0.81 --> neuron2\nneuron2 -- -0.85x + 0.81 --> neuron3\nneuron3 -- -0.85x + 0.81 --> neuron4\n```\n\n\n\n\n\n### may\n\n* Does input \"may\" leads to a prediction of \"not\"?\n\n\n\n```{mermaid}\nflowchart TD\n\ninput1[Warning]\ninput2[lights]\ninput3[activated]\ninput4[Vehicles]\ninput5[may]\n\nneuron1[ReLU]\nneuron2[ReLU]\nneuron3[ReLU]\nneuron4[ReLU]\nneuron5[ReLU]\n\noutput1[NA]\noutput2[NA]\noutput3[NA]\noutput4[NA]\noutput5[not]\n\ninput1 -- 0.20x + 0.86 --> neuron1\ninput2 -- 0.20x + 0.86 --> neuron2\ninput3 -- 0.20x + 0.86 --> neuron3\ninput4 -- 0.20x + 0.86 --> neuron4\ninput5 -- 0.20x + 0.86 --> neuron5\n\nneuron1 -- 0.91x - 0.25 --> output1\nneuron2 -- 0.91x - 0.25 --> output2\nneuron3 -- 0.91x - 0.25 --> output3\nneuron4 -- 0.91x - 0.25 --> output4\nneuron5 -- 0.91x - 0.25 --> output5\n\nneuron1 -- -0.85x + 0.81 --> neuron2\nneuron2 -- -0.85x + 0.81 --> neuron3\nneuron3 -- -0.85x + 0.81 --> neuron4\nneuron4 -- -0.85x + 0.81 --> neuron5\n```\n\n\n\n\n\n### not\n\n* Does input \"not\" leads to a prediction of \"stop\"?\n\n\n\n```{mermaid}\nflowchart TD\n\ninput1[Warning]\ninput2[lights]\ninput3[activate]\ninput4[Vehicles]\ninput5[may]\ninput6[not]\n\nneuron1[ReLU]\nneuron2[ReLU]\nneuron3[ReLU]\nneuron4[ReLU]\nneuron5[ReLU]\nneuron6[ReLU]\n\noutput1[NA]\noutput2[NA]\noutput3[NA]\noutput4[NA]\noutput5[NA]\noutput6[stop]\n\ninput1 -- 0.20x + 0.86 --> neuron1\ninput2 -- 0.20x + 0.86 --> neuron2\ninput3 -- 0.20x + 0.86 --> neuron3\ninput4 -- 0.20x + 0.86 --> neuron4\ninput5 -- 0.20x + 0.86 --> neuron5\ninput6 -- 0.20x + 0.86 --> neuron6\n\nneuron1 -- 0.91x - 0.25 --> output1\nneuron2 -- 0.91x - 0.25 --> output2\nneuron3 -- 0.91x - 0.25 --> output3\nneuron4 -- 0.91x - 0.25 --> output4\nneuron5 -- 0.91x - 0.25 --> output5\nneuron6 -- 0.91x - 0.25 --> output6\n\nneuron1 -- -0.85x + 0.81 --> neuron2\nneuron2 -- -0.85x + 0.81 --> neuron3\nneuron3 -- -0.85x + 0.81 --> neuron4\nneuron4 -- -0.85x + 0.81 --> neuron5\nneuron5 -- -0.85x + 0.81 --> neuron6\n```\n\n\n\n\n\n# Vanishing Gradients\n\nIn our recurrent neural network with\n\n* input layer size: 1\n* hidden layer size: 1\n* output layer size: 1\n\n\n\n```{mermaid}\nflowchart LR\n\ninput1[input]\n\nneuron1[neuron]\n\noutput1[output]\n\ninput1 -- w_i,h + b_i,h --> neuron1\nneuron1 -- w_h,0 + b_h,0 --> output1\nneuron1 -- w_h,h + b_h,h --> neuron1\n```\n\n\n\nwe have functions\n\n* $L_{i,h} = w_{i,h}x + b_{i,h}$\n* $L_{h,o} = w_{h,o}x + b_{h,o}$\n* $L_{h,h} = w_{h,h}x + b_{h,h}$\n\nand ReLU activation function\n\n$$\\text{ReLU}(x) = \\text{max}(x,0)$$\nwhose derivative is\n\n$$\\frac{d\\text{ReLU}}{dx} = \\begin{cases} 1, & x \\geq 0 \\\\ 0, & x < 0 \\end{cases}$$\n\n## Chain Rule\n\nToward back propagation, the derivative based on output $k$ given inputs $\\{1, 2, ..., k\\}$ is\n\n$$\\begin{array}{rcl}\n\\frac{\\partial\\text{SSR}}{\\partial x_{k}} & = & \\displaystyle\\sum_{a = 1}^{k}\n\\frac{\\partial\\text{SSR}}{\\partial L_{h,o}}\n\\left(\\displaystyle\\prod_{b=1}^{a-1}\n\\frac{\\partial L_{h,o}}{\\partial\\text{ReLU}}\\cdot\n\\frac{\\partial\\text{ReLU}}{\\partial L_{h,h}}\\cdot\n\\frac{\\partial L_{h,h}}{L_{i,h}}\n\\right)\n\\frac{\\partial L_{i,h}}{\\partial x_{k}} \\\\\n~ & = & \\displaystyle\\sum_{a = 1}^{k}\nw_{i,h}\n\\left(\\displaystyle\\prod_{b=1}^{a-1}\nw_{h,h}\\right)\nw_{h,0} \\\\\n\\end{array}$$\n\n## Gradient Propagation\n\n\nFor today's example, the derivative is\n\n$$\\frac{\\partial\\text{SSR}}{\\partial x_{k}} = \\sum_{a = 1}^{k}\n(0.20)\n\\left(\\prod_{b=1}^{a-1}\n(-0.85)\n\\right)\n(0.91)$$\n\n::::: {.panel-tabset}\n\n## Math Review\n\nLet us quickly recall some math\n\n## exp\n\n![exponential functions](exponential_functions.png)\n\n* image source: [University of Toronto](https://users.math.utoronto.ca/preparing-for-calculus/4_functions/we_5_exp_log.html)\n\n## ratio\n\n![ratio test](ratio_test.png)\n\n* image source: [Dan the Tutor](https://www.youtube.com/watch?v=qBFPSm0-41g)\n\n:::::\n\n::: {.callout-tip}\n## Gradient Propagation\n\n* we have **vanishing gradients** if $|w_{h,h}| < 1$\n* we have **exploding gradients** if $|w_{h,h}| > 1$\n\nIn the above hypothetical example, since $$|w_{h,h}| = |-0.85| < 1$$\nour model may be limited (forgetful) due to vanishing gradients.\n:::\n\n\n# Preview: Word Prediction\n\n\n\n```{mermaid}\nflowchart LR\n\ninput1[eggs]\ninput2[bread]\nstem1[pairs]\nstem2[well]\nstem3[with]\noutput1[bacon]\noutput2[butter]\n\ninput1 --> stem1\ninput2 --> stem1\nstem1 --> stem2\nstem2 --> stem3\nstem3 --> output1\nstem3 --> output2\n```\n\n\n\n* \"eggs pairs well with bacon\"\n* \"bread pairs well with butter\"\n* but how to represent words as numbers?\n\n    - later: sessions about NLP (natural language programming)\n\n\n# Main Example: First Names\n\nWe will be exploring the [Pytorch RNN tutorial](https://docs.pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html) by Sean Robertson.\n\n::::: {.panel-tabset}\n\n## data\n\n* over 20,000 first names\n* associated with 18 languages\n\n## languages\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n* Arabic\n* Chinese\n* Czech\n* Dutch\n* English\n* French\n:::\n\n::: {.column width=\"5%\"}\n\t\n:::\n\n::: {.column width=\"30%\"}\n* German\n* Greek\n* Irish\n* Italian\n* Japanese\n* Korean\n:::\n\n::: {.column width=\"5%\"}\n\t\n:::\n\n::: {.column width=\"30%\"}\n* Polish\n* Portuguese\n* Russian\n* Scottish\n* Spanish\n* Vietnamese\n:::\n\n::::\n\n## sample\n\nFor example, the Polish names are\n\nAdamczak\nAdamczyk\nAndrysiak\nAuttenberg\nBartosz\nBernard\nBobienski\nBosko\nBroż\nBrzezicki\nBudny\nBukoski\nBukowski\nChlebek\nChmiel\nCzajka\nCzajkowski\nDubanowski\nDubicki\nDunajski\nDziedzic\nFabian\nFilipek\nFilipowski\nGajos\nGniewek\nGomolka\nGomulka\nGorecki\nGórka\nGórski\nGrzeskiewicz\nGwozdek\nJagoda\nJanda\nJanowski\nJaskolski\nJaskulski\nJedynak\nJelen\nJez\nJordan\nKaczka\nKaluza\nKamiński\nKasprzak\nKava\nKedzierski\nKijek\nKlimek\nKosmatka\nKowalczyk\nKowalski\nKoziol\nKozlow\nKozlowski\nKrakowski\nKról\nKumiega\nLawniczak\nLis\nMajewski\nMalinowski\nMaly\nMarek\nMarszałek\nMaslanka\nMencher\nMiazga\nMichel\nMikolajczak\nMozdzierz\nNiemczyk\nNiemec\nNosek\nNowak\nPakulski\nPasternack\nPasternak\nPaszek\nPiatek\nPiontek\nPokorny\nPoplawski\nRóg\nRudaski\nRudawski\nRusnak\nRutkowski\nSadowski\nSalomon\nSerafin\nSienkiewicz\nSierzant\nSitko\nSkala\nSlaski\nŚlązak\nŚlusarczyk\nŚlusarski\nSmolák\nSniegowski\nSobol\nSokal\nSokolof\nSokoloff\nSokolofsky\nSokolowski\nSokolsky\nSówka\nStanek\nStarek\nStawski\nStolarz\nSzczepanski\nSzewc\nSzwarc\nSzweda\nSzwedko\nWalentowicz\nWarszawski\nWawrzaszek\nWiater\nWinograd\nWinogrodzki\nWojda\nWojewódka\nWojewódzki\nWronski\nWyrick\nWyrzyk\nZabek\nZawisza\nZdunowski\nZdunowski\nZielinski\nZiemniak\nZientek\nŻuraw\n\n:::::\n\n::: {.callout-warning}\n## DCP 1\n\n:::\n\n# AI Frameworks\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n### Keras\n\n* Francois Chollet\n* March 2015\n* Greek for \"horn\"\n* supports TensorFlow, JAX, PyTorch\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"30%\"}\n### PyTorch\n\n* Meta AI\n* Sept 2016\n* tensors intrinsic data type\n* automatic differentiation\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"20%\"}\n### Pytorch Lightning\n\n* William Falcon\n* May 2019\n:::\n\n::::\n\n\n\n\n\n\n\n\n\n\n# LSTMs\n\n::::: {.panel-tabset}\n\n## Intro\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n* Hochreiter and Schmidhuber, 1997\n* addressing vanishing/exploding gradients\n* novel architecture\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"50%\"}\n![Hochreiter, 1997](LSTM_paper.png)\n:::\n\n::::\n\n## Conclusion\n\n* CEC: constant error carrousel\n* cuts off memory leakage\n* protection against irrelevant inputs\n\n## Methods\n\nEach *module* of a **Long Short Term Memory** (LSTM) network has\n\n* 3 inputs: data input, long-term memory (aka **forget gate**), short-term memory (aka **output gate**)\n\nand outputs new values for the forget gate and the output gate\n\n![LSTM network](LSTM_chain.png)\n\n* image source: [Chris Olah](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n\n## Results\n\napplied LSTMs on\n\n* sequences of words\n* sequences of numbers\n* noisy sequences\n* math function values\n* temporal sequences\n\n:::::\n\n\n\n::: {.callout-warning}\n## DCP 2\n\n:::\n\n# GRUs\n\n::: {.callout-tip}\n## GRU\n\nEach *module* of a **Gated Recurrent Unit** (GRU) network has\n\n* *2* inputs: data input, long-term memory (aka **forget gate**)\n\nand outputs a new value for the forget gate along with the data prediction.  Hence, the GRU architecture is simpler than an LSTM, but can produce similar results.\n\n:::\n\n![GRU diagram](GRU_diagram.png)\n\n* image source: [O'Reilly](https://www.oreilly.com/library/view/advanced-deep-learning/9781789956177/8ad9dc41-3237-483e-8f6b-7e5f653dc693.xhtml)\n\n![GRU concepts](GRU_conceptual.png)\n\n* image source: [Harshed Abdulla](https://medium.com/@harshedabdulla/understanding-gated-recurrent-units-grus-in-deep-learning-4404599dcefb)\n\n::: {.callout-note}\n## Long-term memory\n\nBoth LSTM and GRU architectures allow for processing of sequences of data while avoiding the vanishing gradient problem.\n\n:::\n\n![recap](RNN_LSTM_GRU.png)\n\n::: {.callout-warning}\n## DCP 3\n\n:::\n\n# Data Ethics: Traveler Biomarkers\n\nEurope's Schengen Area is unrolling tech to collect biomarker information about incoming travelers.\n\n::::: {.panel-tabset}\n\n## Fingerprints\n\n![fingerprints](traveler_fingerprints.png)\n\n* image source: [Getty Images](https://www.cbsnews.com/news/americans-travel-europe-fingerprints-scan-entry-exit-system/)\n\n## Face Scan\n\n![face scan](traveler_face_scan.png)\n\n* image source: [Getty Images](https://www.cbsnews.com/news/americans-travel-europe-fingerprints-scan-entry-exit-system/)\n\n## roll out\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n* started Oct 12, 2025\t\n* six-month roll out\n\n    - start: Croatia, Spain\n    \n* data storage: 3 years\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![Entry/Exit System](travelers_Europe.png)\n\n* image source: [CBS News](https://www.cbsnews.com/news/americans-travel-europe-fingerprints-scan-entry-exit-system/)\n\n:::\n\n::::\n\n:::::\n\n::: {.callout-warning}\n## DCP 4\n\n:::\n\n# Semester Projects\n\n::::: {.panel-tabset}\n\n## Overview\n\nFor SML 301, students will complete a substantial machine learning project\n\n- exploration of large data set\n- neural network tuning\n- machine learning modeling\n- video presentation\n- research poster/slides\n\nYou (and your group) may select one of the following project ideas\n\n* or you may propose your own project\n\n## 1\n\n### TRAIL Microcard Collection\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n* \"Imaged from microcard, these technical reports describe research performed for U.S. government agencies from the 1930s to the 1960s. The reports were provided by the Technical Report Archive and Image Library ([TRAIL](https://digital.library.unt.edu/explore/collections/TRAMC/)).”\n* semi-supervised learning (clustering with some labels)\n* classification task: trying to organize scanned images to help a government organization\n* scope: convolutional neural networks, maybe OCR, maybe NLP\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"30%\"}\n![TRAIL Microcards](collection_TRAMC.png)\n:::\n\n::::\n\n## 2\n\n### Meso-American Migration Project\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n* \"The [MMP's](https://mmp.research.brown.edu/mmp) main focus is to gather social as well as economic information on Mexican and Central American migration to the United States. It is a unique source of data that enables researchers to track patterns and processes of migration, immigrant integration, and the consequences of migration for households and communities in places of origin.\"\n* creative wrangling of target and input variabes\n* scope: applying artificial intelligence methods to a social science data set\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"30%\"}\n![Meso-American Migration Project](MMP_Map.png)\n:::\n\n::::\n\n## 3\n\n### Yelp Open Dataset\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\"The [Yelp Open Dataset](https://business.yelp.com/data/resources/open-dataset/) is a subset of Yelp data that is intended for educational use. It provides real-world data related to businesses including reviews, photos, check-ins, and attributes like hours, parking availability, and ambience.\"\n\n* working with about 9 GB of data\n* predicting consumer ratings\n* scope: neural networks and several machine learning methods\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"30%\"}\n![Yelp Open Dataset](Yelp_Dataset_Map.png)\n:::\n\n::::\n\n## 4\n\n### NYC Taxi\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n* \"datasets were collected and provided to the NYC Taxi and Limousine Commission ([TLC](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)) by technology providers authorized under the Taxicab & Livery Passenger Enhancement Programs (TPEP/LPEP)\"\n\n* working with parquet files and temporal data\n* scope: neural networks and several machine learning methods\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"30%\"}\n![NYC TLC](NYC_taxi.png)\n:::\n\n::::\n\n:::::\n\n\n::: {.callout-note}\n## Standardized\n\nDerek will fine-tune and standardize the project instructions so that each path has roughly the same difficulty and amount of work\n:::\n\n\n\n\n\n\n\n\n# Quo Vadimus?\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n* due this Friday (October 24):\n\n    - Poster Feedback (2 hours)\n    - Precept 6 (1 hour)\n    - Project Proposal (survey)\n    - On Lit Reviews (survey)\n  \n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n* Prompt 3 (or Ed Discussion):\n\nHave there been confusing concepts in our SML 301 course?  Let Derek know!\n:::\n\n::::\n\n\n# Footnotes\n\n::: {.callout-note collapse=\"true\"}\n\n## (optional) Additional Resources and References\n\n* derivation of the [derivative of cross entropy](https://shivammehta25.github.io/posts/deriving-categorical-cross-entropy-and-softmax/) by Shivam Mehta\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.5.1 (2025-06-13 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n  LAPACK version 3.12.1\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4      forcats_1.0.0        stringr_1.5.1       \n [4] dplyr_1.1.4          purrr_1.1.0          readr_2.1.5         \n [7] tidyr_1.3.1          tibble_3.3.0         ggplot2_4.0.0       \n[10] tidyverse_2.0.0      patchwork_1.3.1      palmerpenguins_0.1.1\n\nloaded via a namespace (and not attached):\n [1] Matrix_1.7-3       gtable_0.3.6       jsonlite_2.0.0     compiler_4.5.1    \n [5] Rcpp_1.1.0         tidyselect_1.2.1   png_0.1-8          scales_1.4.0      \n [9] yaml_2.3.10        fastmap_1.2.0      lattice_0.22-7     reticulate_1.43.0 \n[13] R6_2.6.1           generics_0.1.4     knitr_1.50         htmlwidgets_1.6.4 \n[17] pillar_1.11.0      RColorBrewer_1.1-3 tzdb_0.5.0         rlang_1.1.6       \n[21] stringi_1.8.7      xfun_0.52          S7_0.2.0           timechange_0.3.0  \n[25] cli_3.6.5          withr_3.0.2        magrittr_2.0.3     digest_0.6.37     \n[29] grid_4.5.1         rstudioapi_0.17.1  hms_1.1.3          lifecycle_1.0.4   \n[33] vctrs_0.6.5        evaluate_1.0.4     glue_1.8.0         farver_2.1.2      \n[37] rmarkdown_2.29     tools_4.5.1        pkgconfig_2.0.3    htmltools_0.5.8.1 \n```\n\n\n:::\n:::\n\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}