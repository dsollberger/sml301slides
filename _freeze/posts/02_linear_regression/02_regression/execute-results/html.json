{
  "hash": "79ae6e7fd880bb999a49d42012edbc87",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"3: Regression\"\nauthor: \"Derek Sollberger\"\ndate: \"2025-09-08\"\nformat:\n  html:\n    toc: true\n---\n\n::: {.cell}\n\n:::\n\n\n\n# Session 3: Regression\n\n## Start\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n* **Goal**: Foray into regression\n\n* **Objective**: Implement various linear regression models\n:::\n\n::: {.column width=\"40%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_regression_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n:::\n\n::::\n\n# Palmer Penguins\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\nThis semester, Derek is trying out this idea where he will present the main idea at the of each session in the same data set: `palmerpenguins`.\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![Palmer Penguins](lter_penguins.png)\n:::\n\n::::\n\n## Scatterplot\n\nExample: Can we predict the `bill_length` of a penguin whose `body_mass` is 5 kg?\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_regression_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n## Best-Fit Line\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_regression_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlin_fit <- lm(bill_length_mm ~ body_mass_g, data = penguins)\ny_pred <- predict(lin_fit, newdata = data.frame(body_mass_g = 5000))\n```\n:::\n\n\n\n## Prediction\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_regression_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n## Commentary\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n* How reliable is the prediction?\n* How reliable is the [linear] model?\n* Can we deploy a different model?\n* Do we have all of the data, or just a sample?\n\n   * Does this prediction apply to all similar penguins?\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"30%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_regression_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n:::\n\n::::\n\n::: {.callout-warning collapse=\"true\"}\n## DCP1\n\nDCP1\n:::\n\n\n# Model Diagnostics\n\nFollowing discussion in the *ISLP* textbook, we proceed to perform some diagnostics on the linear regression process.\n\n::::: {.panel-tabset}\n\n## splash\n\n![model diagnostics](model_0.png)\n\n## residuals\n\n![residuals](model_1.png)\n\nThe nearly horizontal LOESS (summary) line may indicate that this data is appropriate for this model.  There may be an outlier (in this point of view).\n\n## QQ\n\n![quantile-quantile plot](model_2.png)\n\nThe QQ plot shows that the residuals are somewhat normally distributed (perhaps with less reliable data in the tails).\n\n## SL\n\n![scale-location](model_3.png)\n\nSince none of the standardized residuals are greater than 3 in magnitude, our data set does not have outliers (in this point of view)\n\n## Leverage\n\n![leverage](model_4.png)\n\nWe seem to have no outliers (in this point of view) as data points have similar **leverage** on the linear regression model (i.e. removing one data point would drastically change the regression coefficients)\n\n:::::\n\n\n\n\n# Linear Model\n\n$$\\hat{y} = a + bx$$\n\n* $\\hat{y}$: predicted value\n* $a$: intercept\n* $b$: slope\n\n## Residuals\n\nA *residual* is the difference between a predicted value and its true value.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_regression_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n## Method of Least Squares\n\n**Idea**: The *best-fit* line is where the sum-of-squared residuals is minimized.\n\n$$E(a,b) = \\sum_{i=1}^{n} (y_{i} - a - bx_{i})^{2}$$\n\n**Claim**: $$a = \\frac{ (\\sum y_{i})(\\sum x_{i}^{2}) - (\\sum x_{i})(\\sum x_{i}y_{i}) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} }, \\quad b = \\frac{ n\\sum x_{i}y_{i} - (\\sum x_{i})(\\sum y_{i}) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} }$$\n\n::: {.callout-note collapse=\"true\"}\n## (optional) Proof\n\nSearch for a critical point by setting the partial derivatives (along with the Chain Rule) equal to zero.\n\n$$0 = \\frac{\\partial E}{\\partial a} = -2\\sum_{i = 1}^{n} (y_{i} - a - bx_{i}) = 2an + 2b\\sum_{i = 1}^{n}x_{i} - 2\\sum_{i = 1}^{n} y_{i}$$\n$$0 = \\frac{\\partial E}{\\partial b} = -2\\sum_{i = 1}^{n} (y_{i} - a - bx_{i})x_{i} = 2a\\sum_{i = 1}^{n}x_{i} + 2b\\sum_{i = 1}^{n}x_{i}^{2} - 2\\sum_{i = 1}^{n} x_{i}y_{i}$$\n\nCreate a matrix system of equations.\n\n$$\\left[  \\begin{array}{cc}\n  n & \\sum_{i = 1}^{n}x_{i} \\\\\n  \\sum_{i = 1}^{n}x_{i} & \\sum_{i = 1}^{n}x_{i}^{2} \\\\\n  \\end{array}\\right]\n  \\left[  \\begin{array}{c}  a \\\\ b \\end{array}\\right]\n  =\n  \\left[  \\begin{array}{c}  \\sum_{i = 1}^{n} y_{i} \\\\ \\sum_{i = 1}^{n} x_{i}y_{i} \\end{array}\\right]\n  $$\n\n\nEmploy a matrix inverse.\n\n$$\\begin{array}{rcl}\n  \\left[  \\begin{array}{c}  a \\\\ b \\end{array}\\right] & = & \n  \\left[  \\begin{array}{cc}\n  n & \\sum_{i = 1}^{n}x_{i} \\\\\n  \\sum_{i = 1}^{n}x_{i} & \\sum_{i = 1}^{n}x_{i}^{2} \\\\\n  \\end{array}\\right]^{-1}\\left[  \\begin{array}{c}  \\sum_{i = 1}^{n} y_{i} \\\\ \\sum_{i = 1}^{n} x_{i}y_{i} \\end{array}\\right] \\\\\n  \n  ~ & ~ & ~ \\\\\n  \n  \\left[  \\begin{array}{c}  a \\\\ b \\end{array}\\right] & = & \\frac{1}{n\\sum x_{i}^{2} - (\\sum x_{i})^{2}} \\left[  \\begin{array}{cc}\n  \\sum_{i = 1}^{n}x_{i}^{2} & -\\sum_{i = 1}^{n}x_{i} \\\\\n  -\\sum_{i = 1}^{n}x_{i} & n \\\\\n  \\end{array}\\right]  \\left[  \\begin{array}{c}  \\sum_{i = 1}^{n} y_{i} \\\\ \\sum_{i = 1}^{n} x_{i}y_{i} \\end{array}\\right] \\\\\n  \n  ~ & ~ & ~ \\\\\n  \n  \\left[  \\begin{array}{c}  a \\\\ b \\end{array}\\right] & = & \\frac{1}{n\\sum x_{i}^{2} - (\\sum x_{i})^{2}} \n  \\left[  \\begin{array}{c}  (\\sum y_{i})(\\sum x_{i}^{2}) - (\\sum x_{i})(\\sum x_{i}y_{i}) \\\\  n\\sum x_{i}y_{i} - (\\sum x_{i})(\\sum y_{i}) \\end{array}\\right] \\\\\n\\end{array}$$\n\n:::\n\n\n# Train-Test Split\n\nToday's big idea is the training-testing split.\n\n![train-test split](train-test-split.png)\n\n* image source: [Michael Galarnyk](https://builtin.com/data-science/train-test-split)\n\n::: {.callout-tip}\n## Training and Testing Sets\n\nIn a machine learning workflow, it is customary to partition the data into a **training set** and a **test set**.\n\n* we *build a model* on the training set\n* we *evaluate the model* on the test set\n\nOne recommendation is to use a 70:30 ratio for the number of observations that go into the training set and the test set respectively.\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Train-Val-Test\n\nIn the near future, when comparing results *between different types of models*, we want to prevent *leaking data* into the test set.  Thus, each model has a stage that uses a *validation set*.\n\n* we *build a model* on the training set\n* we *evaluate the model* on the validation set\n* we *evaluate the model type* on the test set\n\nOne recommendation is to use a 70:15:15 partition for the number of observations that go into the training set, validation set, and the test set respectively.\n\n:::\n\n::: {.callout-warning collapse=\"true\"}\n## DCP2\n\nDCP2\n:::\n\n# Metric\n\nTo *evaluate* proposed models, we use the same **metric** on the test data for all of the models.\n\n::: {.callout-note}\n## MSE\n\nFor regression tasks, a common *metric* is **MSE** (mean-squared error)\n\n$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(\\hat{y}_{i} - y_{i})^{2}$\n\nSince we are discussing errors (or residuals), the lowest value for MSE points toward the best model.\n\n:::\n\n# Ethics Enclave: Music\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\"... she followed a link the fan had posted and was taken to what appeared to be her latest release. \"But I didn't recognise it because I hadn't released a new album,\" Portman says.\n\n\"I clicked through and discovered an album online everywhere - on Spotify and iTunes and all the online platforms.\n\n\"It was called Orca, and it was music that was evidently AI-generated, but it had been cleverly trained, I think, on me.\"\"\"\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![Emily Portman](Emily_Portman.png)\n:::\n\n::::\n\n# Hyperparameters\n\nModels are *fit* to the training data by finding the best parameter values (e.g. coefficients) through optimization.\n\nThe shape of the models are affected by **hyperparameters**\n\n## Polynomial Regression\n\n$d = 2: \\hat{y} = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{1}^{2}$\n\n$d = 3: \\hat{y} = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{1}^{2} + \\beta_{3}X_{1}^{3}$\n\n::: {.callout-note}\n## Degree is a hyperparameter\n\nFor polynomial regression, the degree $d$ is a hyperparameter.\n\n* parameters: $\\beta_{0}, \\beta_{1}, \\beta_{2}, ...$\n* hyperparametr: $d$\n\n:::\n\n# Multiple Linear Regression\n\n$$\\hat{y} = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{3} + ...$$\n\nis likewise solved by [ordinary least squares](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf)\n\n::: {.callout-warning collapse=\"true\"}\n## DCP3\n\nDCP3\n:::\n\n# Penalization\n\nLater, we will discuss the notion of *variance* (in the test set error totals) for machine learning.  For now, we try out **penalization** methods to suppress some of the coefficients to try to reduce variance.\n\n## Ridge Regression\n\n## Ridge Regression\n\n$\\hat{y} = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\alpha\\sum_{i = 1}^{2} \\beta_{i}^{2}$\n\nwhere $\\alpha > 0$ is the penalization coefficient.\n\n* ridge regression is also called **L2 penalization**\n\n## LASSO Regression\n\n$\\hat{y} = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\alpha\\sum_{i = 1}^{2} |\\beta_{i}|$\n\nwhere $\\alpha > 0$ is the penalization coefficient.\n\n* LASSO regression (least absolute shrinkage and selection operator) is also called **L1 penalization**\n\n## Elastic Net\n\nFor these penalized linear models, we proceed to **elastic net**, which is a linear combination of the previous ridge and LASSO ideas.\n\n$\\hat{y} = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\alpha[L\\sum_{i = 1}^{2} \\beta_{i}^{2}+ (1-L)\\sum_{i = 1}^{2} |\\beta_{i}|]$\n\n* $\\alpha$: penalization coefficient\n* $L$: $L1$ ratio\n\n\n\n\n# Quo Vadimus?\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n* due this Friday:\n\n    - Precept 2\n\n*  talk to some of your classmates and draft plans to form groups (toward the midterm and semester projects)\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![Palmer Penguins](lter_penguins.png)\n:::\n\n::::\n\n\n# Footnotes\n\n::: {.callout-note collapse=\"true\"}\n\n## (optional) Additional Resources\n\n* [OLS](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf) description and proof by Professor Michael J Rosenfeld\n* [Train Test Split](https://builtin.com/data-science/train-test-split) by Michael Galanyk\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.5.1 (2025-06-13 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n  LAPACK version 3.12.1\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4      forcats_1.0.0        stringr_1.5.1       \n [4] dplyr_1.1.4          purrr_1.1.0          readr_2.1.5         \n [7] tidyr_1.3.1          tibble_3.3.0         ggplot2_3.5.2       \n[10] tidyverse_2.0.0      palmerpenguins_0.1.1\n\nloaded via a namespace (and not attached):\n [1] Matrix_1.7-3       gtable_0.3.6       jsonlite_2.0.0     compiler_4.5.1    \n [5] tidyselect_1.2.1   splines_4.5.1      scales_1.4.0       yaml_2.3.10       \n [9] fastmap_1.2.0      lattice_0.22-7     R6_2.6.1           labeling_0.4.3    \n[13] generics_0.1.4     knitr_1.50         htmlwidgets_1.6.4  pillar_1.11.0     \n[17] RColorBrewer_1.1-3 tzdb_0.5.0         rlang_1.1.6        stringi_1.8.7     \n[21] xfun_0.52          timechange_0.3.0   cli_3.6.5          mgcv_1.9-3        \n[25] withr_3.0.2        magrittr_2.0.3     digest_0.6.37      grid_4.5.1        \n[29] rstudioapi_0.17.1  hms_1.1.3          nlme_3.1-168       lifecycle_1.0.4   \n[33] vctrs_0.6.5        evaluate_1.0.4     glue_1.8.0         farver_2.1.2      \n[37] rmarkdown_2.29     tools_4.5.1        pkgconfig_2.0.3    htmltools_0.5.8.1 \n```\n\n\n:::\n:::\n\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::",
    "supporting": [
      "02_regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}