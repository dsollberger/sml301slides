{
  "hash": "7d888a4a2dd4d1e85e4be4113f787111",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"24: Multimodal Models\"\nauthor: \"Derek Sollberger\"\ndate: \"2025-04-23\"\nformat:\n  html:\n    toc: true\n---\n\n::: {.cell}\n\n:::\n\n\n\n# 24: Multimodal Models\n\n## Learning objectives:\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- Introduce generative adversarial networks\n- Introduce multimodal models\n- Finish!\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n![Cycle GANs](cycle_gan_paintings.png)\n\n* image credit: [Jun-Yan Zhu, et al.](https://junyanz.github.io/CycleGAN/)\n:::\n\n::::\n\n## Generative AI\n\n::::: {.panel-tabset}\n\n### Definition\n\n:::: {.columns}\n\n::: {.column width=\"75%\"}\n* \"Generative AI is a form of artificial intelligence that is designed to generate content, including text, images, video and music. It uses large language models and algorithms to analyze patterns in datasets to mimic the style or structure of specific types of content.\"\n* [quote and image source](https://www.eweek.com/artificial-intelligence/generative-ai-vs-machine-learning/)\n:::\n\n::: {.column width=\"25%\"}\n![generative AI](generative_ai.png)\n:::\n\n::::\n\n### Dall-E\n\n![June 2022](Dall_e_cheeseburgers.png)\n\n[image source](https://www.onefootdown.com/2022/6/21/23175949/notre-dame-football-immersive-and-horrifying-dall-e-mini-art-experience-gumbo-pitbull-cheeseburgers)\n\n### Stable Diffusion\n\n![August 2023](stable_diffusion.png)\n\n[image source](https://www.geeky-gadgets.com/stable-diffusion-sdxl-beginners-guide/)\n\n:::::\n\n# GANS\n\n## Overview\n\n![GANs overview](DLI_fig_12_1.png)\n\n* image source: [Deep Learning Illustrated](https://www.deeplearningillustrated.com/)\n\n## Discriminator\n\n![Training the discriminator](DLI_fig_12_2.png)\n\n* image source: [Deep Learning Illustrated](https://www.deeplearningillustrated.com/)\n\n## Generator\n\n![Training the generator](DLI_fig_12_3.png)\n\n* image source: [Deep Learning Illustrated](https://www.deeplearningillustrated.com/)\n\n::: {.callout-tip}\n## GAN Objectives\n\n* the goal of a generator is to create better fake images\n* the goal of a discriminator is to better classify fake images\n\n:::\n\n## Adversarial Network\n\n![Adversarial network](DLI_fig_12_4.png)\n\n* image source: [Deep Learning Illustrated](https://www.deeplearningillustrated.com/)\n\n## Initial Results\n\n![Goodfellow, et al., 2014](Goodfellow_2014.png)\n\n\n# Conditional GANs\n\n::: {.callout-note}\n## cGANs\n\n\"We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also **learn a loss function** to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations.\"\n\n* [Isola, et al.](https://phillipi.github.io/pix2pix/)\n:::\n\n![cGAN in ecology](cGAN_ecology.png)\n\n* image source: [Hayatbini, et al.](https://www.mdpi.com/2072-4292/11/19/2193#)\n\n## Activity: Pix2Pix Image-to-Image\n\n::::: {.panel-tabset}\n\n## App\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![image-to-image](pix2pix_shoes.png)\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n* [app (link)](https://affinelayer.com/pixsrv/) by Christopher Hesse\n* [GitHub repo](https://github.com/affinelayer/pix2pix-tensorflow)\n:::\n\n::::\n\n## ex1\n\n![edges2cats](edges2cats.png)\n\n## ex2\n\n![facades](facades.png)\n\n## ex1\n\n![edges2handbags](edges2handbags.png)\n\n:::::\n\n## Activity: Pix2Pix Instruct\n\n::::: {.panel-tabset}\n\n## App\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![pix2pix](pix2pix.png)\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n* [app (link)](https://huggingface.co/spaces/timbrooks/instruct-pix2pix) by Tim Brooks\n* [GitHub repo](https://github.com/timothybrooks/instruct-pix2pix)\n:::\n\n::::\n\n## ex1\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![New Jersey flag](NJ_flag.png)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![convert to grayscale](NJ_flag_grayscale.png)\n:::\n\n::::\n\n## ex2\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![New Jersey flag](NJ_flag.png)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![add more horses](NJ_flag_more_horses.png)\n:::\n\n::::\n\n## ex3\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![New Jersey flag](NJ_flag.png)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![replace humans with ramen](NJ_flag_ramen.png)\n:::\n\n::::\n\n:::::\n\n\n# Stacked GANs\n\n::: {.callout-note}\n## Stacked GANs\n\n*  \"Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images\"\n* \" Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details\"\n\n:::\n\n![Zhang, et al., 2016](stackGAN_stages.png)\n\n## Architecture\n\n![Zhang, et al., 2016](stackGAN_architecture.png)\n\n## Scaling\n\n![Zhang, et al., 2016](stackGAN_scaling.png)\n\n\n# Cycle GANs\n\n![cycle GAN possibilities](cycle_GAN_splash.png)\n\n::: {.callout-note}\n## cGANs\n\n* \"... learning to translate an image from a source domain $X$ to a target domain $Y$ in the absence of paired examples ...\" --- [Jun-Yan Zhu, et al., 2017](https://junyanz.github.io/CycleGAN/)\n\n:::\n\n## Architecture\n\n![](cycle_GAN_architecture_A2B.png)\n\n![cycle GAN architecture](cycle_GAN_architecture_B2A.png)\n\n* images source: [Bansal and Rathore](https://hardikbansal.github.io/CycleGANBlog/)\n\n::: {.callout-tip}\n## cGAN Objective\n\nIn a cyclic GAN, the generator and discriminator converge toward a Nash equilibrium.\n\n:::\n\n# Unsupervised Representation Learning\n\n![modifications to pictures](radford_rooms.png)\n\n* image source: [Radford, et al, 2016](https://arxiv.org/abs/1511.06434)\n\n![trained filters](radford_filters.png)\n\n* image source: [Radford, et al, 2016](https://arxiv.org/abs/1511.06434)\n\n::::: {.panel-tabset}\n\n## Image Space\n\n![vector space of images](image space.png)\n\n* image source: [Deep Learning Illustrated](https://www.deeplearningillustrated.com/)\n\n## ex1\n\n![image space](image_space_ex1.png)\n\n* image source: [Radford, et al, 2016](https://arxiv.org/abs/1511.06434)\n\n## ex2\n\n![image space](image_space_ex2.png)\n\n* image source: [Radford, et al, 2016](https://arxiv.org/abs/1511.06434)\n\n:::::\n\n\n# Variational Auto-Encoders\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\nIn AI architecture, **variational autoencoders** simulate the latent space (between the encoder and decoder)---usually with a mixture of Gaussian functions---to maximize the ELBO (**evidence lower bound**)\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![VAE Gaussians](VAE_gaussians.png)\n\n* [image source](https://www.researchgate.net/publication/379478988_HGMVAE_hierarchical_disentanglement_in_Gaussian_mixture_variational_autoencoder)\n:::\n\n::::\n\n\n::::: {.panel-tabset}\n\n## without\n\n![without VAE](VAE_before.png)\n\n* image source: [Jeff Orchard](https://www.youtube.com/watch?v=FSBLj74Qy4I)\n\n## with\n\n![with VAE](VAE_after.png)\n\n* image source: [Jeff Orchard](https://www.youtube.com/watch?v=FSBLj74Qy4I)\n\n:::::\n\n## VQ-VAE\n\n**Vector quantized variational autoencoders** (VQ-VAE) utilize an *discrete embedding space*\n\n* for example: 32x32 embedding space of vectors\n\n![VQ-VAE](VQ_VAE.png)\n\n* [image source](https://www.researchgate.net/publication/370101880_DL-based_Generation_of_facial_portraits_from_diverse_data_sources)\n\n\n# Diffusion Models\n\n::: {.callout-tip}\n## Diffusion Models\n\n> A (denoising) **diffusion model** isn't that complex if you compare it to other generative models such as Normalizing Flows, GANs or VAEs: they all convert noise from some simple distribution to a data sample. This is also the case here where *a neural network learns to gradually denoise data* starting from pure noise. \n\n---[HuggingFace](https://huggingface.co/blog/annotated-diffusion)\n:::\n\n![U-net architecture](unet_architecture.png)\n\n* image source: [HuggingFace](https://huggingface.co/blog/annotated-diffusion)\n\n\n# Tokenization Beyond Text\n\n## Text\n\n![tokenization of text](tokenization_text.png)\n\n* image source: [Murilo Gustineli](https://medium.com/data-science/the-art-of-tokenization-breaking-down-text-for-ai-43c7bccaed25)\n\n## Images\n\n::::: {.panel-tabset}\n\n## Vectorization\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![tokenization of images](tokenization_images_vectorization.png)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n* images are partitioned into **patches**\n* each patch is flattened into a vector\n\n* image source: [Shusen Wang](https://github.com/wangshusen/DeepLearning/blob/master/Slides/10_ViT.pdf)\n:::\n\n::::\n\n## Positional Encoding\n\n![positional encoding of patches](tokenization_images_pe.png)\n\n* image source: [Shusen Wang](https://github.com/wangshusen/DeepLearning/blob/master/Slides/10_ViT.pdf)\n\n:::::\n\n## Audio\n\n::::: {.panel-tabset}\n\n## Abstraction\n\n![audio abstraction](tokenization_audio_abstraction.png)\n\n* image source: [Valerio Velardo](https://www.youtube.com/playlist?list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0)\n\n## Music\n\n* beat\n* timbre\n* pitch\n* harmony\n* ...\n\n* source: [Valerio Velardo](https://www.youtube.com/playlist?list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0)\n\n## Fourier\n\n![signal domain](tokenization_audio_Fourier.png)\n\n* image source: [Valerio Velardo](https://www.youtube.com/playlist?list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0)\n\n## Trends\n\n* digital signal processing $\\rightarrow$ rule-based systems\n* traditional ML $\\rightarrow$ feature engineering\n* deep learning $\\rightarrow$ automatic feature engineering\n\n* source: [Valerio Velardo](https://www.youtube.com/playlist?list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0)\n\n:::::\n\n\n# Multimodal Models\n\n## CLIP\n\n**Contrastive Language Image Pre-Training**\n\n![CLIP architecture](7-clip.png)\n\n* image source: [Chip Huyen](https://huyenchip.com/2023/10/10/multimodal.html)\n\n## Vision Neurons\n\n![vision neurons](vision_neurons.png)\n\n![vision neurons](vision_neuron_grid.png)\n\n* image source: [dstill.pub](https://distill.pub/2021/multimodal-neurons/)\n\n## Audio Example\n\nVideo game streamer Jesse Cox tried out a generative AI program that was specifically built to create music and lyrics.  Here is a [theme song for constipation medicine](https://www.youtube.com/clip/Ugkxp9W922jbYWPvfggHRQ-84vsS2aIhOKRq)\n\n\n# Quo Vadimus?\n\nPresently, here are some more applications of multimodal models.\n\n::::: {.panel-tabset}\n\n## home\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![receipt bookkeeping](receipts.png)\n\n* image source: [Recycle This Pittsburgh](https://recyclethispgh.com/item/paper-receipts/)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n* scan grocery receipts\n* OCR\n* AI text decoding\n* code expense report\n\n:::\n\n::::\n\n## pedagogy\n\n![COPUS](copus.png)\n\n* image source: [COPUS](https://www.lifescied.org/doi/10.1187/cbe.13-08-0154)\n\n## software dev\n\n**Vision Question Answering**\n\n![VQA](vqa_examples.png)\n\n* image source: [paper](https://arxiv.org/pdf/1612.00837)\n\n## comp bio\n\n*transfer learning* between RNA and ATAC sequencing\n\n![scButterfly](scButterfly.png)\n\n* image source: [scButterfly](https://www.nature.com/articles/s41467-024-47418-x?fromPaywallRec=false)\n\n## medicine\n\n![data types](multimodel_models_for_medicine.png)\n\n* image source: [Science Direct](https://www.sciencedirect.com/science/article/pii/S1566253524004688)\n\n:::::\n\n\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.2    fastmap_1.2.0     cli_3.6.3        \n [5] tools_4.4.2       htmltools_0.5.8.1 rstudioapi_0.17.1 yaml_2.3.10      \n [9] rmarkdown_2.29    knitr_1.50        jsonlite_1.8.9    xfun_0.52        \n[13] digest_0.6.37     rlang_1.1.5       evaluate_1.0.3   \n```\n\n\n:::\n:::\n\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::",
    "supporting": [
      "24_MLLMs_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}