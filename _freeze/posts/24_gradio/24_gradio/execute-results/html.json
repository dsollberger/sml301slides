{
  "hash": "7205fb18b75004e3480fe15e6d97f3a9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"24: Gradio\"\nauthor: \"Derek Sollberger\"\ndate: \"2025-12-03\"\nformat:\n  html:\n    toc: true\n---\n\n\n\n\n# 24: Gradio\n\n## Learning Objectives\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- explore Gradio\n- outline semester project\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n![gradio](gradio_icon.png)\n\n:::\n\n::::\n\n# Setup\n\n## Installation\n\nWe can now install `gradio` (say, within a virtual environment) with\n\n```\npip install gradio\n```\n\n# Tutorials\n\n## HuggingFace\n\n[HuggingFace Gradio Tutorials](https://huggingface.co/learn/llm-course/en/chapter9/1)\n\n## Fast.ai\n\n[Image Segmentation app](https://forums.fast.ai/t/lesson-2-gradio-app-for-image-segmentation/113696)\n\n\n# Example: AudioBot\n\nHere we look at an example Gradio app from [AssemblyAI](https://www.assemblyai.com/blog/getting-started-with-huggingfaces-gradio)\n\n![AudioBot](audio_bot_1.png)\n\n* input: users can upload or record an audio file\n* output: AI analyses of the audio file\n\n## Outputs\n\n::::: {.panel-tabset}\n\n### transcript\n\n![AudioBot](audio_bot_2.png)\n\n### speakers\n\n![AudioBot](audio_bot_3.png)\n\n### attention\n\n![AudioBot](audio_bot_4.png)\n\n### summary\n\n![AudioBot](audio_bot_5.png)\n\n![AudioBot](audio_bot_6.png)\n\n:::::\n\n::::: {.panel-tabset}\n\n### topics\n\n![AudioBot](audio_bot_7.png)\n\n### sentiment\n\n![AudioBot](audio_bot_8.png)\n\n### entities\n\n![AudioBot](audio_bot_9.png)\n\n### content\n\n![AudioBot](audio_bot_10.png)\n\n:::::\n\n::: {.callout-warning}\n## DCP1\n:::\n\n# Data Ethics: Avoiding Fakes\n\nThe following are the section headings from Chapter 10 of *Calling Bull___* by Carl T Bergstrom and Jevin D West\n\n1. Question the source of information\n2. Beware of unfair comparisons\n3. If it seems to be too good or too bad to be true ...\n4. Think in orders of magnitude\n5. Avoid confirmation bias\n6. Consider multiple hypotheses\n\n::: {.callout-warning}\n## DCP2\n:::\n\n# Case Study: Color Sensitivity\n\n* Derek Sollberger, Princeton University\n* Hayley Orndorf, BioMADE\n\n## Introduction\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- We hope to build a CNN to classify pictures in terms of susceptibility to color blindness\n- Trained model on Ishihara data set\n- Ran model on novel images from the Princeton Art Museum\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n![301](ishihara_example_301.png)\n:::\n\n::::\n\n### Data Description\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- [Kaggle data set](https://www.kaggle.com/datasets/dupeljan/ishihara-blind-test-cards) by Dupeljan\n- synethetic set of 1400 Ishihara blind test cards\n- digits 0 to 9\n- Google fonts\n- for exploring dichromacy:\n\n    1. protanopia\n    2. deuteranopia\n    3. tritanopia\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n![color vision deficiency](color_vision_deficiency.png)\n\n* image source: [Male, et al., 2022](https://pmc.ncbi.nlm.nih.gov/articles/PMC9498227/)\n:::\n\n::::\n\n### Literature Search\n\n::::: {.panel-tabset}\n\n#### cGANs\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- Conditional Adversarial Networks\t\n- Phillip Isola, et al.\n- \"... learn the mapping from\ninput image to output image, but also learn a loss function to train this mapping\"\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![Isola et al., 2018](Isola_cGANs.png)\n:::\n\n::::\n\n#### NIR\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- The Potential of Diffusion-Based Near-Infrared Image Colorization \t\n- Borsetelmann, et al., 2024\n- \"... utilizing diffusion models for the colorization of NIR images\"\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![Borsetelmann, et al., 2024](Borsetelmann_NIR.png)\n:::\n\n::::\n\n#### LineGAN\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- LineGAN: An image colourisation method combined with a line art network\t\n- Dahua Lv, Yuanyuan Pu, Rencan Nie\n- \"LineGAN learns the corresponding colour mapping from datasets, improving the accuracy of image colourisation\"\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![Lv, et al., 2022](Lv_LineGAN.png)\n:::\n\n::::\n\n:::::\n\n\n## Methods\n\n### Exploratory Data Analysis\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n* 1400 images\n* 531x531 pixels\n\n    * 3 channels (RGB)\n\n* 45 Google fonts\n* partition:\n\n    * training: 70%\n    * testing: 30%\n    \n* strata:\n\n    * type 1: 25%\n    * type 2: 25%\n    * type 3: 50%\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n* `script.ipynb`\n* `ishihara_train_test/`\n\n    * `training_data/`\n    \n        * `type_1/`\n        * `type_2/`\n        * `type_3/`\n    \n    * `testing_data/`\n    \n        * `type_1/`\n        * `type_2/`\n        * `type_3/`\n:::\n\n::::\n\n::: {.callout-note collapse=\"true\"}\n### Tabular Data?\n\nIf you are working with tabular data (such as a CSV file), your EDA is more conventional, and you should provide explorations such as bar graphs, histograms, and/or scatterplots along with counts and/or correlations.\n:::\n\n* baseline models\n\n    * random guessing (33%)\n    * majority classifier (50%)\n\n### CNN\n\n![convolutional neural network](CNN_architecture.png)\n\n* 30,000 parameters!\n* image source: [NN-SVG](https://alexlenail.me/NN-SVG/index.html)\n\n::: {.callout-warning}\n## DCP3\n:::\n\n## Discussion\n\n### Hyperparameters\n\n::::: {.panel-tabset}\n\n#### Deep Learning\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n* input: 3x32x32\n\n* 2 convolution layers\n\n    * each with max-pool\n    \n* dense layers of 400, 64, 32\n\n* output: 3 class labels\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"30%\"}\nWe increased the hidden layer sizes and the number of hidden layers in hopes of reducing the classification error.\n:::\n\n::::\n\n#### Learning Rate\n\n* We chose a learning rate of 0.05 based on work done on the CIFAR-10 data set.\n\n* TODO: elaborate on larger or smaller learning rate\n\n#### Momentum\n\n* We chose a momentum of 0.9 based on the PyTorch Lightning documentation for the SGD optimizer\n\n* TODO: elaborate on larger or smaller momentum\n\n:::::\n\n\n### Results\n\n![sample of predictions](results_collage.png)\n\n* classification test error: 50%\n\n    * beat random guessing (33%)\n    * did not outperform majority classifier (50%)\n\n### Future Directions\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n* better data\n\n    * increase data set by factor of 10\n    * recenter, resize, rotations, shears\n\n* model architecture\n\n    * more hidden layers\n    * vary filter sizes\n    * 3D convolution filters\n    \n* hyperparameter search\n\n* validate results\n\n* regression model\n\n    * goal: image's susceptibility to color vision deficiency\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"30%\"}\n![3D convolutions](3d_convolution.png)\n\n* image source: [DeepLearning AI](https://www.youtube.com/watch?v=KTB_OFoAQcc)\n:::\n\n::::\n\n\n\n\n\n\n\n## Conclusion\n\nTODO: summarize discussion and results\n\n### Citations\n\n* Borstelmann, A., Haucke, T., & Steinhage, V. (2024). \"The Potential of Diffusion-Based Near-Infrared Image Colorization. Sensors\", 24(5), 1565. https://doi.org/10.3390/s24051565 \n\n* Dupeljian (2021). *Ishihara blind test cards*. Retrieved February 2025 from [https://www.kaggle.com/datasets/dupeljan/ishihara-blind-test-cards/data](https://www.kaggle.com/datasets/dupeljan/ishihara-blind-test-cards/data).\n\n* Isola, Phillip, et al. ‘Image-to-Image Translation with Conditional Adversarial Networks’. CoRR, vol. abs/1611.07004, 2016.\n\n* LeNail, (2019). NN-SVG: Publication-Ready Neural Network Architecture Schematics. Journal of Open Source Software, 4(33), 747, https://doi.org/10.21105/joss.00747\n\n* Lv, Dahua et al. “LineGAN: An image colourisation method combined with a line art network.” IET Computer Vision vol. 16,5, pages 403-417. 08 Mar. 2022, doi:10.1049/cvi2.12096\n\n* Male, Shiva Ram et al. “Color vision devices for color vision deficiency patients: A systematic review and meta-analysis.” Health science reports vol. 5,5 e842. 22 Sep. 2022, doi:10.1002/hsr2.842\n\n\n\n\n\n\n\n\n# Quo Vadimus?\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n* due this Friday (Dec 5):\n\n    - surveys:\n        - **CLO Assessment (after)** (10 minutes)\n        - **CuriosityPost** (5 minutes)\n        - **Literature Review Survey (post)** (10 minutes)\n\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.callout-tip}\n## Semester Project due December 16\n\n* Project Code\n* Project Presentation\n* Project Report\n:::\n\n:::\n\n::::\n\n\n# Footnotes\n\n::: {.callout-note collapse=\"true\"}\n\n## (optional) Additional Resources and References\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.5.1 (2025-06-13 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n  LAPACK version 3.12.1\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.5.1    fastmap_1.2.0     cli_3.6.5        \n [5] tools_4.5.1       htmltools_0.5.8.1 rstudioapi_0.17.1 yaml_2.3.10      \n [9] rmarkdown_2.29    knitr_1.50        jsonlite_2.0.0    xfun_0.52        \n[13] digest_0.6.37     rlang_1.1.6       evaluate_1.0.4   \n```\n\n\n:::\n:::\n\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::",
    "supporting": [
      "24_gradio_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}