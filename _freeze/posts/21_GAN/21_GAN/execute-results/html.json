{
  "hash": "6a5db8a3b8fee42dbf02250334cfb8a6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"21: Generative Adversarial Networks\"\nauthor: \"Derek Sollberger\"\ndate: \"2025-04-14\"\nformat:\n  html:\n    toc: true\n---\n\n::: {.cell}\n\n:::\n\n\n\n# Session 21: Generative Adversarial Networks\n\n## Learning objectives:\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- Introduce generative adversarial networks\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n![Cycle GANs](cycle_gan_paintings.png)\n\n* image credit: [Jun-Yan Zhu, et al.](https://junyanz.github.io/CycleGAN/)\n:::\n\n::::\n\n## Generative AI\n\n::::: {.panel-tabset}\n\n### Definition\n\n:::: {.columns}\n\n::: {.column width=\"75%\"}\n* \"Generative AI is a form of artificial intelligence that is designed to generate content, including text, images, video and music. It uses large language models and algorithms to analyze patterns in datasets to mimic the style or structure of specific types of content.\"\n* [quote and image source](https://www.eweek.com/artificial-intelligence/generative-ai-vs-machine-learning/)\n:::\n\n::: {.column width=\"25%\"}\n![generative AI](generative_ai.png)\n:::\n\n::::\n\n### Dall-E\n\n![June 2022](Dall_e_cheeseburgers.png)\n\n[image source](https://www.onefootdown.com/2022/6/21/23175949/notre-dame-football-immersive-and-horrifying-dall-e-mini-art-experience-gumbo-pitbull-cheeseburgers)\n\n### Stable Diffusion\n\n![August 2023](stable_diffusion.png)\n\n[image source](https://www.geeky-gadgets.com/stable-diffusion-sdxl-beginners-guide/)\n\n:::::\n\n## Activity: Quick Draw!\n\n::::: {.panel-tabset}\n\n## App\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n![Quick Draw, by Google](QuickDraw.png)\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"60%\"}\nTry out Quick Draw!\n\n* [https://quickdraw.withgoogle.com/](https://quickdraw.withgoogle.com/)\n* app will ask you to sketch 6 images\n\n    * and it will try to recognize your artwork\n\n* caution: there is sound\n\n:::\n\n::::\n\n## Lions\n\n![Quick, Draw! Lions](quickdraw_lions.png)\n\n## Tigers\n\n![Quick, Draw! Tigers](quickdraw_tigers.png)\n\n## Penguins\n\n![Quick, Draw! Penguins](quickdraw_penguins.png)\n\n:::::\n\n\n# GANS\n\n## Overview\n\n![GANs overview](DLI_fig_12_1.png)\n\n* image source: [Deep Learning Illustrated](https://www.deeplearningillustrated.com/)\n\n## Discriminator\n\n![Training the discriminator](DLI_fig_12_2.png)\n\n* image source: [Deep Learning Illustrated](https://www.deeplearningillustrated.com/)\n\n## Generator\n\n![Training the generator](DLI_fig_12_3.png)\n\n* image source: [Deep Learning Illustrated](https://www.deeplearningillustrated.com/)\n\n::: {.callout-tip}\n## GAN Objectives\n\n* the goal of a generator is to create better fake images\n* the goal of a discriminator is to better classify fake images\n\n:::\n\n## Adversarial Network\n\n![Adversarial network](DLI_fig_12_4.png)\n\n* image source: [Deep Learning Illustrated](https://www.deeplearningillustrated.com/)\n\n## Initial Results\n\n![Goodfellow, et al., 2014](Goodfellow_2014.png)\n\n\n# Conditional GANs\n\n::: {.callout-note}\n## cGANs\n\n\"We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also **learn a loss function** to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations.\"\n\n* [Isola, et al.](https://phillipi.github.io/pix2pix/)\n:::\n\n![cGAN in ecology](cGAN_ecology.png)\n\n* image source: [Hayatbini, et al.](https://www.mdpi.com/2072-4292/11/19/2193#)\n\n## Activity: Pix2Pix Image-to-Image\n\n::::: {.panel-tabset}\n\n## App\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![image-to-image](pix2pix_shoes.png)\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n* [app (link)](https://affinelayer.com/pixsrv/) by Christopher Hesse\n* [GitHub repo](https://github.com/affinelayer/pix2pix-tensorflow)\n:::\n\n::::\n\n## ex1\n\n![edges2cats](edges2cats.png)\n\n## ex2\n\n![facades](facades.png)\n\n## ex1\n\n![edges2handbags](edges2handbags.png)\n\n:::::\n\n## Activity: Pix2Pix Instruct\n\n::::: {.panel-tabset}\n\n## App\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![pix2pix](pix2pix.png)\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n* [app (link)](https://huggingface.co/spaces/timbrooks/instruct-pix2pix) by Tim Brooks\n* [GitHub repo](https://github.com/timothybrooks/instruct-pix2pix)\n:::\n\n::::\n\n## ex1\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![New Jersey flag](NJ_flag.png)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![convert to grayscale](NJ_flag_grayscale.png)\n:::\n\n::::\n\n## ex2\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![New Jersey flag](NJ_flag.png)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![add more horses](NJ_flag_more_horses.png)\n:::\n\n::::\n\n## ex3\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![New Jersey flag](NJ_flag.png)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![replace humans with ramen](NJ_flag_ramen.png)\n:::\n\n::::\n\n:::::\n\n\n# Stacked GANs\n\n::: {.callout-note}\n## Stacked GANs\n\n*  \"Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images\"\n* \" Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details\"\n\n:::\n\n![Zhang, et al., 2016](stackGAN_stages.png)\n\n## Architecture\n\n![Zhang, et al., 2016](stackGAN_architecture.png)\n\n## Scaling\n\n![Zhang, et al., 2016](stackGAN_scaling.png)\n\n\n# Cycle GANs\n\n![cycle GAN possibilities](cycle_GAN_splash.png)\n\n::: {.callout-note}\n## cGANs\n\n* \"... learning to translate an image from a source domain $X$ to a target domain $Y$ in the absence of paired examples ...\" --- [Jun-Yan Zhu, et al., 2017](https://junyanz.github.io/CycleGAN/)\n\n:::\n\n# Ethics Corner: Research Consent\n\n![IRB principles](IRB_principles.png)\n\n* image source: [Ross Avilla](https://methods.sagepub.com/video/ethical-guidelines-and-irb)\n\n## Architecture\n\n![](cycle_GAN_architecture_A2B.png)\n\n![cycle GAN architecture](cycle_GAN_architecture_B2A.png)\n\n* images source: [Bansal and Rathore](https://hardikbansal.github.io/CycleGANBlog/)\n\n::: {.callout-tip}\n## cGAN Objective\n\nIn a cyclic GAN, the generator and discriminator converge toward a Nash equilibrium.\n\n:::\n\n# Unsupervised Representation Learning\n\n![modifications to pictures](radford_rooms.png)\n\n* image source: [Radford, et al, 2016](https://arxiv.org/abs/1511.06434)\n\n![trained filters](radford_filters.png)\n\n* image source: [Radford, et al, 2016](https://arxiv.org/abs/1511.06434)\n\n::::: {.panel-tabset}\n\n## Image Space\n\n![vector space of images](image space.png)\n\n* image source: [Deep Learning Illustrated](https://www.deeplearningillustrated.com/)\n\n## ex1\n\n![image space](image_space_ex1.png)\n\n* image source: [Radford, et al, 2016](https://arxiv.org/abs/1511.06434)\n\n## ex2\n\n![image space](image_space_ex2.png)\n\n* image source: [Radford, et al, 2016](https://arxiv.org/abs/1511.06434)\n\n:::::\n\n\n\n\n# Quo Vadimus?\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n* due this Friday (April 18):\n\n    - Precept 10\n    - Research Consent\n    \n* semester projects will be due May 10\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\nStudent evaluation custom questions:\n\n1. In the space below, please describe the learning environment of your precept section – what about it worked for you, what suggestions do you have to make it work better for you, what challenges did you face – and mention your preceptor by name.\n\n2. For future offerings of this course, how could we better align with the research goals of a Princeton student (such as their senior thesis)?\n\n3. Which course topics do you wish we discussed more of the mathematical background and rigor?  On the contrary, which topics could have had less of an emphasis  on mathematical background and rigor? Please be as specific as possible.\n:::\n\n::::\n\n\n# Footnotes\n\n::: {.callout-note collapse=\"true\"}\n\n## (optional) Additional Resources and References\n\n* [flag of New Jersey](https://www.britannica.com/topic/flag-of-New-Jersey) article at Britannica\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.2    fastmap_1.2.0     cli_3.6.3        \n [5] tools_4.4.2       htmltools_0.5.8.1 rstudioapi_0.17.1 yaml_2.3.10      \n [9] rmarkdown_2.29    knitr_1.49        jsonlite_1.8.9    xfun_0.50        \n[13] digest_0.6.37     rlang_1.1.5       evaluate_1.0.3   \n```\n\n\n:::\n:::\n\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}