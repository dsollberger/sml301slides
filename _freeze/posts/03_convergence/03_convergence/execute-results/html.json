{
  "hash": "9546e56ae1d7ccfe0405f76ea41bca16",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"3: Convergence\"\nauthor: \"Derek Sollberger\"\ndate: \"2025-09-10\"\nformat:\n  html:\n    toc: true\n---\n\n::: {.cell}\n\n:::\n\n\n\n# Session 3: Convergence\n\n## Start\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n* **Goal**: Preview major concepts\n\n* **Objective**: Explore some Python codes about root finding and stochastic processes\n:::\n\n::: {.column width=\"40%\"}\n![bias-variance tradeoff](biasvariance.png)\n\nimage source: [Scott Fortmann-Roe ](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n:::\n\n::::\n\n# SGD\n\nEventually we will discuss upon how neural network weights are found via stochastic gradient descent\n\n* stochastic?\n* gradient?\n* descent?\n\n# Little Perturbations\n\n::::: {.panel-tabset}\n\n## rounding\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n### 2 + 2 = 5\n\n... for extremely large values of 2\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![tshirt](2_2_5.png)\n:::\n\n::::\n\n## truncation\n\n![truncation error](truncation-errors.png)\n\n* image source: [BYJUs](https://byjus.com/maths/truncation-errors/)\n\n## consideration\n\nNumerical analysis: many computational algorithms have been designed to mitigate the possible propogation of rounding and truncation errors.\n\n* example: [the quadratic formula](https://zingale.github.io/comp_astro_tutorial/basics/floating-point/numerical_error.html)\n\n:::::\n\n::: {.callout-caution collapse=\"true\"}\n## Dev Corner: Binary\n\nResearch teams are still affected by these rounding and truncation errors\n\n* different operating systems (Mac, Windows, Linux)\n* different processor chips (Intel, AMD)\n\nTip: data engineers can store computation results in their pure binary representations\n\n:::\n\n# Literature\n\nWe will spend some time discussing what makes for a great research paper.  There are many suggestions available, and for brevity, we will focus on these common sections:\n\n* abstract and introduction\n* conclusion\n* methods\n* results\n\n## Activity: Literature Introductions\n\nWe will look at the abstracts and introduction paragraphs for some of the [most influential papers](https://github.com/daturkel/learning-papers?tab=readme-ov-file) in the history of machine learning:\n\n::: {.callout-tip}\n## Suggested Papers\n\n### AlexNet\n\n* ImageNet Classification with Deep Convolutional Neural Networks (2012)\n\n### GANs\n\n* General Adversarial Nets (2014), Goodfellow et al.\n\n### TensorFlow\n\n* TensorFlow: A system for large-scale machine learning (2016), Abadi et al.\n\n### Word2Vec\n\n* Efficient Estimation of Word Representations in Vector Space (2013), Mikolov, Chen, Corrado, and Dean\n:::\n\n::: {.callout-warning}\n## DCP1\n:::\n\n\n# Stochastic\n\n* **Stochastic**: word of Greek origin, meaning to guess at\n\n## Sequences\n\n* index: $n \\in \\mathbb{N} = \\{1, 2, 3, 4, 5, ...\\}$\n* formulaic: f(n) = 2n - 1\n\n$$1, 3, 5, 7, 9, ...$$\n\n* constructive:\n\n$$3, 3.1, 3.14, 3.141, 3.1415, 3.14159, ...$$\n\n* shapes:\n\n![triangular numbers](triangular-numbers.png)\n\n* image source: [BYJUs](https://byjus.com/maths/triangular-numbers/)\n\n\n## Random Variables\n\nA **random variable** has no set value, but rather represents an element of chance.  We can better understand a random variable through statistics like\n\n* mean\n* variance\n* distribution\n\n::: {.callout-tip collapse=\"true\"}\n## Stochastic Process\n\nA **stochastic process** is a *sequence* of *random variables*\n:::\n\n\n# Gradient\n\nFor a multivariate function $f(\\vec{x})$, the **gradient** is a vector of partial derivatives\n\n$$\\nabla f = \\left[ \\frac{\\partial}{\\partial x_{1}}, \\frac{\\partial}{\\partial x_{2}}, \\frac{\\partial}{\\partial x_{3}}, ..., \\frac{\\partial}{\\partial x_{n}}  \\right]$$\n\n\n# Descent\n\n::::: {.panel-tabset}\n\n## Scenario\n\nFind the roots of the function\n\n$$f(x) = x^{3} - 12x^{2} + 44x - 48$$\n\nIn other words, find the solutions of the equation\n\n$$f(x) = x^{3} - 12x^{2} + 44x - 48 = 0$$\n\n## Bisection\n\n![Bisection Method](bisection_method.png)\n\n* image source: [Python Numerical Methods](https://pythonnumericalmethods.studentorg.berkeley.edu/notebooks/chapter19.03-Bisection-Method.html)\n\n## code\n\n```\ndef bisection_method(f, a, b, tol):\n  # tries to find a root of f between a and b (i.e. f(root) = 0)\n  # Inputs: real numbers a, b; tolerance\n  # Output: root\n\n  # check if root is in the interval (i.e. Intermediate Value Theorem)\n    if np.sign(f(a)) == np.sign(f(b)):\n        raise Exception(\"The endpoints do not contain a root\")\n\n  # initialization\n    iter_num = 1\n\n    while (b - a) > tol:\n      c = (a + b) / 2\n\n      print(\"iter_num: \" + str(iter_num) + \", midpoint: \" + str(c))\n\n      if f(c) * f(a) < 0:\n          b = c\n      else:\n          a = c\n\n      iter_num += 1\n\n    print(\"Root:\", c)\n\n```\n\n```\nf = lambda x: x**3 - 12*x**2  + 44*x - 48\n```\n\n\nTry calling the `bisection_method` with different initial values for a and b, such as\n\n*    [0, 3]\n*    [3, 7]\n*    [5, 10]\n\n## Newton's\n\n![Newton's Method](Newtons_Method.png)\n\n* image source: [Paul's Online Notes](https://tutorial.math.lamar.edu/classes/calci/newtonsmethod.aspx)\n\n## code\n\n```\ndef Newton_Method(f, f_prime, x_0, tol, max_iter):\n  # tries to find a root of f between a and b (i.e. f(root) = 0)\n  # Inputs: function f, derivative function f_prime,\n  # initial guess x_0, tolerance tol, and maximum number of iterations\n  # Output: root\n\n  # initialization\n    iter_num = 1\n    x_n = x_0\n\n    while (abs(f(x_n)) > tol) & (iter_num <= max_iter):\n      print(\"iter_num: \" + str(iter_num) + \", guess: \" + str(x_n))\n      f_x = f(x_n)\n      f_prime_x = f_prime(x_n)\n\n      x_n = x_n - f_x / f_prime_x\n      iter_num += 1\n\n    print(\"Root:\", x_n)\n```\n\n```\nf_prime = lambda x: 3*x**2 - 24*x + 44\n```\n\nTry calling Newton_Method with a few different guesses for the initial value\n\n## Cauchy\n\nAn infinite sequence $\\{x_{i}\\}_{i=1}^{\\infty}$ is **Cauchy convergent** if\n\n$$\\forall \\epsilon \\exists N \\text{ such that } \\forall m,n > N$$\n$$|x_{m} - x_{n}| < \\epsilon$$\n\n:::::\n\n::: {.callout-warning}\n## DCP2\n:::\n\n# Trade-Off: Speed versus Complexity\n\nNewton's Method\n\n$$x_{n} = x_{n-1} - \\frac{f(x_{n-1})}{f'(x_{n-1})}$$\n\n* converges faster\n* needs computation of the derivative\n* what if $f' \\approx 0$?\n\n\n# Preview: Backpropagation\n\nIn this experiment, we will see if we can get some intuition about backpropagation in a relatively simple setting of linear regression\n\n$$\\hat{y} = \\beta_{0} + \\beta_{1}x$$\n\nwith loss function\n\n$$L = \\sum(y - \\hat{y})^{2} = \\sum(y - \\beta_{0} - \\beta_{1}x)^{2}$$\n\nThe **gradient** is a vector of partial derivatives\n\n$$\\nabla L(\\vec{\\beta}) = \n\\left[\\begin{array}{c} \\frac{\\partial L}{\\partial \\beta_{0}} \\\\ \\frac{\\partial L}{\\partial \\beta_{1}}\\end{array}\\right]\n= \n\\left[\\begin{array}{c} 2\\sum(y - \\beta_{0} - \\beta_{1}x)(-1) \\\\ 2\\sum(y - \\beta_{0} - \\beta_{1}x)(-x)\\end{array}\\right]$$\n\n::::: {.panel-tabset}\n\n## Setup\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![best fit line](backprop_setup.png)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\nLet us explore this gradient in a setup where\n\n$$x \\in (-5, 5)$$\n$$Y ~ 25 + 20x + N(0,5^{2})$$\n:::\n\n::::\n\n## Ex1\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![wrong slope](backprop_ex1.png)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\nWith initial conditions $\\vec{\\beta_{0}} = \\left[\\begin{array}{c} 25 \\\\ 40 \\end{array}\\right]$\n\nthe gradient is\n\n$$\\nabla L(\\vec{\\beta}) = \n\\left[\\begin{array}{c} \\frac{\\partial L}{\\partial \\beta_{0}} \\\\ \\frac{\\partial L}{\\partial \\beta_{1}}\\end{array}\\right]\n\\approx \n\\left[\\begin{array}{c} 819 \\\\ 36867 \\end{array}\\right]$$\n\nwhich indicates that more effort is needed to adjust the slope than the intercept!\n\n:::\n\n::::\n\n## Ex2\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![wrong slope](backprop_ex2.png)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\nWith initial conditions $\\vec{\\beta_{0}} = \\left[\\begin{array}{c} 40 \\\\ 20 \\end{array}\\right]$\n\nthe gradient is\n\n$$\\nabla L(\\vec{\\beta}) = \n\\left[\\begin{array}{c} \\frac{\\partial L}{\\partial \\beta_{0}} \\\\ \\frac{\\partial L}{\\partial \\beta_{1}}\\end{array}\\right]\n\\approx \n\\left[\\begin{array}{c} 3819 \\\\ 2860 \\end{array}\\right]$$\n\nwhich indicates that more effort is needed to adjust the intercept than the slope!\n\n:::\n\n::::\n\n:::::\n\n\n\n\n\n\n::: {.callout-warning}\n# DCP3\n:::\n\n# Markov Chains\n\nA **Markov chain** is a stochastic process whose state depends only on the immediate previous iteration.\n\n$$P_{ij} = P(X_{n} = j | X_{n-1} = i)$$\n\n## Application: Dinner Choices\n\nSuppose that we have a Princeton student whose behavior includes eating only three types of dinner: \n\n$$S = \\{\\text{ramen}, \\text{pizza}, \\text{sushi}\\}$$\n\nwith transition matrix\n\n$$P = \\left(\\begin{array}{ccc}\n0.2 & 0.4 & 0.4 \\\\\n0.3 & 0.4 & 0.3 \\\\\n0.2 & 0.2 & 0.6\n\\end{array}\\right)$$\n\n![dinner choices network](dinner_network_graph.png)\n\n::: {.callout-note collapse=\"true\"}\n## Network terminology\n\n* directed versus undirected graphs\n* cyclic versus acyclic graphs\n\nLater studies focus on **DAGs**: directed, acyclic network graphs\n:::\n\nSuppose that, on a Monday, the student's preferences are\n\n$$x_0 = \\left(\\begin{array}{ccc} 0.5 & 0.25 & 0.25 \\end{array}\\right)$$\n\na. What is the probability that the student will eat ramen on Tuesday (i.e. the next day)?\nb. What is the probability that the student will eat pizza on Wednesday (i.e. two days later)?\nc. What is the long-term dinner-choice behavior of this student?\n\n::: {.callout-note collapse=\"true\"}\n## calculations\n\na. after one day\n\n$$x_{0}P = \\left(\\begin{array}{ccc} 0.225 & 0.35 & 0.425 \\end{array}\\right)$$\n\nThe probability of eating ramen the next day is about 22.5 percent.\n\nb. after two days\n\n$$x_{0}P^{2} = \\left(\\begin{array}{ccc} 0.235 & 0.315 & 0.45 \\end{array}\\right)$$\nThe probability of eating pizza two days later is about 31.5 percent.\n\nc. after many days\n\n$$x_{0}P^{100} = \\left(\\begin{array}{ccc} 0.2307 & 0.3077 & 0.4615 \\end{array}\\right)$$\n\nThe probability of the student being at the sushi place is about 46.15 percent\n:::\n\n# Preview: Softmax\n\nIn the previous example, we ended with an inference that is similar to what we will encounter with a *softmax*.\n\n$$x_{0}P^{100} = \\left(\\begin{array}{ccc} 0.2307 & 0.3077 & 0.4615 \\end{array}\\right)$$\n\n1. the student's first choice is sushi\n2. the student's second choice is pizza\n3. the student's third choice is ramen\n\n## Hardmax\n\nIn contrast, a **hardmax** is one-hot encoded.  \n\n$$\\left(\\begin{array}{ccc} 0 & 0 & 1 \\end{array}\\right)$$\n\nIn this, we are certain that the student will be at the sushi place.\n\n\n# Bias-Variance Trade-off\n\nWithin a *hypothesis class* of similar modeling functions, we are concerned with the **bias-variance tradeoff** in model selection.\n\n![bias-variance tradeoff](biasvariance.png)\n\nimage source: [Scott Fortmann-Roe ](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n\n\n# Quo Vadimus?\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n* due this Friday:\n\n    - Precept 2\n\n*  talk to some of your classmates and draft plans to form groups (toward the midterm and semester projects)\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n* tip: talk about your career and research goals with your instructors\n* (optional) If you have seen the Bisection and Newton's Methods before, let me know which class(es) cover that material.\n\n:::\n\n::::\n\n\n# Footnotes\n\n::: {.callout-note collapse=\"true\"}\n\n## (optional) Additional Resources\n\n* [Bisection Method](https://flexiple.com/python/bisection-method-python) by Harsh Pandey\n* [Newton's Method](https://flexiple.com/python/newton-raphson-method-python) by Harsh Pandey\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.5.1 (2025-06-13 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n  LAPACK version 3.12.1\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4 forcats_1.0.0   stringr_1.5.1   dplyr_1.1.4    \n [5] purrr_1.1.0     readr_2.1.5     tidyr_1.3.1     tibble_3.3.0   \n [9] ggplot2_3.5.2   tidyverse_2.0.0 gt_1.0.0       \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6       jsonlite_2.0.0     compiler_4.5.1     tidyselect_1.2.1  \n [5] xml2_1.3.8         scales_1.4.0       yaml_2.3.10        fastmap_1.2.0     \n [9] R6_2.6.1           generics_0.1.4     knitr_1.50         htmlwidgets_1.6.4 \n[13] pillar_1.11.0      RColorBrewer_1.1-3 tzdb_0.5.0         rlang_1.1.6       \n[17] stringi_1.8.7      xfun_0.52          timechange_0.3.0   cli_3.6.5         \n[21] withr_3.0.2        magrittr_2.0.3     digest_0.6.37      grid_4.5.1        \n[25] rstudioapi_0.17.1  hms_1.1.3          lifecycle_1.0.4    vctrs_0.6.5       \n[29] evaluate_1.0.4     glue_1.8.0         farver_2.1.2       rmarkdown_2.29    \n[33] tools_4.5.1        pkgconfig_2.0.3    htmltools_0.5.8.1 \n```\n\n\n:::\n:::\n\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::",
    "supporting": [
      "03_convergence_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}