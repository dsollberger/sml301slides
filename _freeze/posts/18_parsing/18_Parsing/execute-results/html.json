{
  "hash": "0fe35a437c5f77e0df29b1c264eb5517",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"18: Parsing\"\nauthor: \"Derek Sollberger\"\ndate: \"2025-04-02\"\nformat:\n  html:\n    toc: true\n---\n\n::: {.cell}\n\n:::\n\n\n\n# Session 18: Parsing\n\n## Learning objectives:\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- Continue word embedding\n- Explore tokenization\n- Introduce computer vision\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n![301](ishihara_example_301.png)\n:::\n\n::::\n\n# Word Embedding\n\n## Linear Relationships\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n$$w_{\\text{king}}\t- w_{\\text{man}} + w_{\\text{woman}} \\approx w_{\\text{queen}}$$\n* explore in this [app](https://dash.gallery/dash-word-arithmetic/)!\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"40%\"}\n\n![word vector space](word_vector_network.png)\n\n* image credit: [plotly](https://medium.com/plotly/understanding-word-embedding-arithmetic-why-theres-no-single-answer-to-king-man-woman-cd2760e2cb7f)\n\n:::\n\n::::\n\n\n![word vector spaces](linear-relationships_orig.png)\n\n* image credit: [Fabian Pfurtscheller](https://www.askyourdata.co/blog/an-introduction-to-natural-language-processing-nlp)\n\n\n# Tokenization\n\nAndrej Karpathy inspired many machine learning practioners to think about tokenization with the following open problems:\n\nTokenization is at the heart of much weirdness of LLMs. Do not brush it off.\n\n- Why can't LLM spell words? **Tokenization**.\n- Why can't LLM do super simple string processing tasks like reversing a string? **Tokenization**.\n- Why is LLM worse at non-English languages (e.g. Japanese)? **Tokenization**.\n- Why is LLM bad at simple arithmetic? **Tokenization**.\n- Why did GPT-2 have more than necessary trouble coding in Python? **Tokenization**.\n- Why did my LLM abruptly halt when it sees the string \"<|endoftext|>\"? **Tokenization**.\n- What is this weird warning I get about a \"trailing whitespace\"? **Tokenization**.\n- Why the LLM break if I ask it about \"SolidGoldMagikarp\"? **Tokenization**.\n- Why should I prefer to use YAML over JSON with LLMs? **Tokenization**.\n- Why is LLM not actually end-to-end language modeling? **Tokenization**.\n\nTo explore **tokenization**, let us try out the following passages in the [Tiktokenizer app](https://tiktokenizer.vercel.app/) (as seen in Andrej Karpathy's videos):\n\n```\nTokenization is at the heart of much weirdness of LLMs. Do not brush it off.\n\n127 + 677 = 804\n1275 + 6773 = 8041\n\nEgg.\nI have an Egg.\negg.\nEGG.\n\n만나서 반가워요. 저는 OpenAI에서 개발한 대규모 언어 모델인 ChatGPT입니다. 궁금한 것이 있으시면 무엇이든 물어보세요.\n\nfor i in range(1, 101):\n    if i % 3 == 0 and i % 5 == 0:\n        print(\"FizzBuzz\")\n    elif i % 3 == 0:\n        print(\"Fizz\")\n    elif i % 5 == 0:\n        print(\"Buzz\")\n    else:\n        print(i)\n```\n\n## Byte Pair Encoding\n\nTo compress long text passages, we can employ [byte pair encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding)\n\n\n# Computer Vision\n\nIn this course, we are exploring images in the Ishihara cards (to test for colorblindness).\n\n![301](ishihara_example_301.png)\n\n## Bootstrapping\n\nIf we don't have enough images to easily train a convolutional neural network, we can synthetically increase the number of image files by **bootstrapping** the original images with minor alterations such as\n\n* *rotations*\n* *reflections*\n\nand we might also want to *resize* the images while drafting a neural net workflow.\n\n# Quo Vadimus?\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n* due this Friday (April 4):\n\n    - Precept 8\n    - Literature Review\n  \n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n\n# Footnotes\n\n::: {.callout-note collapse=\"true\"}\n\n## (optional) Additional Resources and References\n\n* [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE) by Andrej Karpathy\n\n    * [notebook](https://colab.research.google.com/drive/1y0KnCFZvGVf_odSfcNAws6kcDD7HsI0L?usp=sharing)\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.2    fastmap_1.2.0     cli_3.6.3        \n [5] tools_4.4.2       htmltools_0.5.8.1 rstudioapi_0.17.1 yaml_2.3.10      \n [9] rmarkdown_2.29    knitr_1.49        jsonlite_1.8.9    xfun_0.50        \n[13] digest_0.6.37     rlang_1.1.5       evaluate_1.0.3   \n```\n\n\n:::\n:::\n\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}