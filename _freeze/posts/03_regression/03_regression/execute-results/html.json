{
  "hash": "36f253f597fc9d08587cd019b54d223c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"3: Regression\"\nauthor: \"Derek Sollberger\"\ndate: \"2025-02-03\"\nformat:\n  html:\n    toc: true\n---\n\n::: {.cell}\n\n:::\n\n\n\n# Session 3: Regression\n\n## Start\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n* **Goal**: Discuss the bias-variance trade-ff\n\n* **Objective**: Explore linear regression\n:::\n\n::: {.column width=\"40%\"}\nAs we get started, try to install the `ISLP` package in your Python software\n:::\n\n::::\n\n# Linear Model\n\n$$\\hat{y} = a + bx$$\n\n* $\\hat{y}$: predicted value\n* $a$: intercept\n* $b$: slope\n\n## Residuals\n\nA *residual* is the difference between a predicted value and its true value.\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](03_regression_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n## Method of Least Squares\n\n**Idea**: The *best-fit* line is where the sum-of-squared residuals is minimized.\n\n$$E(a,b) = \\sum_{i=1}^{n} (y_{i} - a - bx_{i})^{2}$$\n\n**Claim**: $$a = \\frac{ (\\sum y_{i})(\\sum x_{i}^{2}) - (\\sum x_{i})(\\sum x_{i}y_{i}) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} }, \\quad b = \\frac{ n\\sum x_{i}y_{i} - (\\sum x_{i})(\\sum y_{i}) }{ n\\sum x_{i}^{2} - (\\sum x_{i})^{2} }$$\n\n::: {.callout-note collapse=\"true\"}\n## (optional) Proof\n\nSearch for a critical point by setting the partial derivatives (along with the Chain Rule) equal to zero.\n\n$$0 = \\frac{\\partial E}{\\partial a} = -2\\sum_{i = 1}^{n} (y_{i} - a - bx_{i}) = 2an + 2b\\sum_{i = 1}^{n}x_{i} - 2\\sum_{i = 1}^{n} y_{i}$$\n$$0 = \\frac{\\partial E}{\\partial b} = -2\\sum_{i = 1}^{n} (y_{i} - a - bx_{i})x_{i} = 2a\\sum_{i = 1}^{n}x_{i} + 2b\\sum_{i = 1}^{n}x_{i}^{2} - 2\\sum_{i = 1}^{n} x_{i}y_{i}$$\n\nCreate a matrix system of equations.\n\n$$\\left[  \\begin{array}{cc}\n  n & \\sum_{i = 1}^{n}x_{i} \\\\\n  \\sum_{i = 1}^{n}x_{i} & \\sum_{i = 1}^{n}x_{i}^{2} \\\\\n  \\end{array}\\right]\n  \\left[  \\begin{array}{c}  a \\\\ b \\end{array}\\right]\n  =\n  \\left[  \\begin{array}{c}  \\sum_{i = 1}^{n} y_{i} \\\\ \\sum_{i = 1}^{n} x_{i}y_{i} \\end{array}\\right]\n  $$\n\n\nEmploy a matrix inverse.\n\n$$\\begin{array}{rcl}\n  \\left[  \\begin{array}{c}  a \\\\ b \\end{array}\\right] & = & \n  \\left[  \\begin{array}{cc}\n  n & \\sum_{i = 1}^{n}x_{i} \\\\\n  \\sum_{i = 1}^{n}x_{i} & \\sum_{i = 1}^{n}x_{i}^{2} \\\\\n  \\end{array}\\right]^{-1}\\left[  \\begin{array}{c}  \\sum_{i = 1}^{n} y_{i} \\\\ \\sum_{i = 1}^{n} x_{i}y_{i} \\end{array}\\right] \\\\\n  \n  ~ & ~ & ~ \\\\\n  \n  \\left[  \\begin{array}{c}  a \\\\ b \\end{array}\\right] & = & \\frac{1}{n\\sum x_{i}^{2} - (\\sum x_{i})^{2}} \\left[  \\begin{array}{cc}\n  \\sum_{i = 1}^{n}x_{i}^{2} & -\\sum_{i = 1}^{n}x_{i} \\\\\n  -\\sum_{i = 1}^{n}x_{i} & n \\\\\n  \\end{array}\\right]  \\left[  \\begin{array}{c}  \\sum_{i = 1}^{n} y_{i} \\\\ \\sum_{i = 1}^{n} x_{i}y_{i} \\end{array}\\right] \\\\\n  \n  ~ & ~ & ~ \\\\\n  \n  \\left[  \\begin{array}{c}  a \\\\ b \\end{array}\\right] & = & \\frac{1}{n\\sum x_{i}^{2} - (\\sum x_{i})^{2}} \n  \\left[  \\begin{array}{c}  (\\sum y_{i})(\\sum x_{i}^{2}) - (\\sum x_{i})(\\sum x_{i}y_{i}) \\\\  n\\sum x_{i}y_{i} - (\\sum x_{i})(\\sum y_{i}) \\end{array}\\right] \\\\\n\\end{array}$$\n\n:::\n\n## Multiple Linear Regression\n\n$$\\hat{y} = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{3} + ...$$\n\nis likewise solved by [ordinary least squares](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf)\n\n\n# Activity: Literature Introductions\n\nWe will look at the abstracts and introduction paragraphs for some of the most influential papers in the history of machine learning:\n\n## Adaboost\n\n* A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting (1997â€”published as abstract in 1995), Freund and Schapire\n\n## AlexNet\n\n* ImageNet Classification with Deep Convolutional Neural Networks (2012)\n\n## DropOut\n\n* Dropout: A Simple Way to Prevent Neural Networks from Overfitting (2014), Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov\n\n## GANs\n\n* General Adversarial Nets (2014), Goodfellow et al.\n\n## TensorFlow\n\n* TensorFlow: A system for large-scale machine learning (2016), Abadi et al.\n\n## Word2Vec\n\n* Efficient Estimation of Word Representations in Vector Space (2013), Mikolov, Chen, Corrado, and Dean\n\n\n## Bias-Variance Trade-off\n\nWithin a *hypothesis class* of similar modeling functions, we are concerned with the **bias-variance tradeoff** in model selection.\n\n![bias-variance tradeoff](biasvariance.png)\n\nimage source: [Scott Fortmann-Roe ](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n\n\n# Quo Vadimus?\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n* due this Friday (5 PM):\n\n    - Precept 2\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n* tip: talk about your career and research goals with your instructors\n* (optional) If you have seen the Bisection and Newton's Methods before, let me know which class(es) cover that material.\n\n:::\n\n::::\n\n\n# Footnotes\n\n::: {.callout-note collapse=\"true\"}\n\n## (optional) Additional Resources\n\n* [bias-variance demo](https://spotintelligence.com/2023/04/11/bias-variance-trade-off/) by Neet Van Otten\n* [bias-variance demo](https://machinelearningmastery.com/calculate-the-bias-variance-trade-off/) by Jason Brownlee\n* [OLS](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf) description and proof by Professor Michael J Rosenfeld\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.3 forcats_1.0.0   stringr_1.5.1   dplyr_1.1.4    \n [5] purrr_1.0.2     readr_2.1.5     tidyr_1.3.1     tibble_3.2.1   \n [9] ggplot2_3.5.1   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] Matrix_1.7-1      gtable_0.3.6      jsonlite_1.8.9    compiler_4.4.2   \n [5] tidyselect_1.2.1  splines_4.4.2     scales_1.3.0      yaml_2.3.10      \n [9] fastmap_1.2.0     lattice_0.22-6    R6_2.5.1          labeling_0.4.3   \n[13] generics_0.1.3    knitr_1.49        htmlwidgets_1.6.4 munsell_0.5.1    \n[17] pillar_1.10.1     tzdb_0.4.0        rlang_1.1.5       stringi_1.8.4    \n[21] xfun_0.50         timechange_0.3.0  cli_3.6.3         mgcv_1.9-1       \n[25] withr_3.0.2       magrittr_2.0.3    digest_0.6.37     grid_4.4.2       \n[29] rstudioapi_0.17.1 hms_1.1.3         nlme_3.1-166      lifecycle_1.0.4  \n[33] vctrs_0.6.5       evaluate_1.0.3    glue_1.8.0        farver_2.1.2     \n[37] colorspace_2.1-1  rmarkdown_2.29    tools_4.4.2       pkgconfig_2.0.3  \n[41] htmltools_0.5.8.1\n```\n\n\n:::\n:::\n\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::",
    "supporting": [
      "03_regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}