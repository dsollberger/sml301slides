{
  "hash": "336f938965fa0cc7c352019489a5aad9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"23: LLMs\"\nauthor: \"Derek Sollberger\"\ndate: \"2025-04-21\"\nformat:\n  html:\n    toc: true\n---\n\n::: {.cell}\n\n:::\n\n\n\n# Session 23: Large Language Models\n\n## Learning objectives:\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- Discuss encoder-only models\n- Discuss decoder-only models\n- Give example of semester project\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n![301](ishihara_example_301.png)\n:::\n\n::::\n\n# BERT\n\n## Transformers\n\n* Encoder-only: BERT\n\n> bidirectional encoder representations from transformers\n\n* Decoder-only: GPT\n\n> generative pre-trained transformer\n\n## Encoders vs Decoders\n\n![BERT vs GPT](BERT_vs_GPT.png)\n\n* image source: [Ronak Verma](https://www.linkedin.com/pulse/bert-vs-gpt-which-better-llm-model-ronak-verma-xeixc/)\n\n## Applications\n\n* BERT\n\n    * text classification\n    * data labeling\n    * recommender\n    * sentiment analysis\n    \n* GPT\n\n    * content generation\n    * conversational chatbots\n\n# Case Study: Color Sensitivity\n\n* Derek Sollberger, Princeton University\n* Hayley Orndorf, BioMADE\n\n## Introduction\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- We hope to build a CNN to classify pictures in terms of susceptibility to color blindness\n- Trained model on Ishihara data set\n- Ran model on novel images from the Princeton Art Museum\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n![301](ishihara_example_301.png)\n:::\n\n::::\n\n### Data Description\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- [Kaggle data set](https://www.kaggle.com/datasets/dupeljan/ishihara-blind-test-cards) by Dupeljan\n- synethetic set of 1400 Ishihara blind test cards\n- digits 0 to 9\n- Google fonts\n- for exploring dichromacy:\n\n    1. protanopia\n    2. deuteranopia\n    3. tritanopia\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n![color vision deficiency](color_vision_deficiency.png)\n\n* image source: [Male, et al., 2022](https://pmc.ncbi.nlm.nih.gov/articles/PMC9498227/)\n:::\n\n::::\n\n### Literature Search\n\n::::: {.panel-tabset}\n\n#### cGANs\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- Conditional Adversarial Networks\t\n- Phillip Isola, et al.\n- \"... learn the mapping from\ninput image to output image, but also learn a loss function to train this mapping\"\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![Isola et al., 2018](Isola_cGANs.png)\n:::\n\n::::\n\n#### NIR\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- The Potential of Diffusion-Based Near-Infrared Image Colorization \t\n- Borsetelmann, et al., 2024\n- \"... utilizing diffusion models for the colorization of NIR images\"\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![Borsetelmann, et al., 2024](Borsetelmann_NIR.png)\n:::\n\n::::\n\n#### LineGAN\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- LineGAN: An image colourisation method combined with a line art network\t\n- Dahua Lv, Yuanyuan Pu, Rencan Nie\n- \"LineGAN learns the corresponding colour mapping from datasets, improving the accuracy of image colourisation\"\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![Lv, et al., 2022](Lv_LineGAN.png)\n:::\n\n::::\n\n:::::\n\n\n## Methods\n\n### Exploratory Data Analysis\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n* 1400 images\n* 531x531 pixels\n\n    * 3 channels (RGB)\n\n* 45 Google fonts\n* partition:\n\n    * training: 70%\n    * testing: 30%\n    \n* strata:\n\n    * type 1: 25%\n    * type 2: 25%\n    * type 3: 50%\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n* `script.ipynb`\n* `ishihara_train_test/`\n\n    * `training_data/`\n    \n        * `type_1/`\n        * `type_2/`\n        * `type_3/`\n    \n    * `testing_data/`\n    \n        * `type_1/`\n        * `type_2/`\n        * `type_3/`\n:::\n\n::::\n\n::: {.callout-note collapse=\"true\"}\n### Tabular Data?\n\nIf you are working with tabular data (such as a CSV file), your EDA is more conventional, and you should provide explorations such as bar graphs, histograms, and/or scatterplots along with counts and/or correlations.\n:::\n\n### CNN\n\n![convolutional neural network](CNN_architecture.png)\n\n* 30,000 parameters!\n* image source: [NN-SVG](https://alexlenail.me/NN-SVG/index.html)\n\n\n## Discussion\n\n### Hyperparameters\n\n::::: {.panel-tabset}\n\n#### Deep Learning\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n* input: 3x32x32\n\n* 2 convolution layers\n\n    * each with max-pool\n    \n* dense layers of 400, 64, 32\n\n* output: 3 class labels\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"30%\"}\nWe increased the hidden layer sizes and the number of hidden layers in hopes of reducing the classification error.\n:::\n\n::::\n\n#### Learning Rate\n\n* We chose a learning rate of 0.05 based on work done on the CIFAR-10 data set.\n\n* TODO: elaborate on larger or smaller learning rate\n\n#### Momentum\n\n* We chose a momentum of 0.9 based on the PyTorch Lightning documentation for the SGD optimizer\n\n* TODO: elaborate on larger or smaller momentum\n\n:::::\n\n\n### Results\n\n![sample of predictions](results_collage.png)\n\n* classification test error: 50%\n\n    * beat random guessing (33%)\n    * did not outperform majority classifier (50%)\n\n\n### Future Directions\n\n* better data\n\n    * increase data set by factor of 10\n    * recenter, resize, rotations, shears\n\n* model architecture\n\n    * more hidden layers\n    * vary filter sizes\n    \n* hyperparameter search\n\n* validate results\n\n* regression model\n\n    * goal: image's susceptibility to color vision deficiency\n\n\n\n## Conclusion\n\nTODO: summarize discussion and results\n\n### Citations\n\n* Borstelmann, A., Haucke, T., & Steinhage, V. (2024). \"The Potential of Diffusion-Based Near-Infrared Image Colorization. Sensors\", 24(5), 1565. https://doi.org/10.3390/s24051565 \n\n* Dupeljian (2021). *Ishihara blind test cards*. Retrieved February 2025 from [https://www.kaggle.com/datasets/dupeljan/ishihara-blind-test-cards/data](https://www.kaggle.com/datasets/dupeljan/ishihara-blind-test-cards/data).\n\n* Isola, Phillip, et al. ‘Image-to-Image Translation with Conditional Adversarial Networks’. CoRR, vol. abs/1611.07004, 2016.\n\n* LeNail, (2019). NN-SVG: Publication-Ready Neural Network Architecture Schematics. Journal of Open Source Software, 4(33), 747, https://doi.org/10.21105/joss.00747\n\n* Lv, Dahua et al. “LineGAN: An image colourisation method combined with a line art network.” IET Computer Vision vol. 16,5, pages 403-417. 08 Mar. 2022, doi:10.1049/cvi2.12096\n\n* Male, Shiva Ram et al. “Color vision devices for color vision deficiency patients: A systematic review and meta-analysis.” Health science reports vol. 5,5 e842. 22 Sep. 2022, doi:10.1002/hsr2.842\n\n\n\n\n\n\n\n\n\n\n\n# Quo Vadimus?\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n* due this Friday (April 18):\n\n    - CLO Assessment (After)\n    \n        * about 20 minutes\n    \n* semester projects will be due May 10\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\nStudent evaluation custom questions:\n\n1. In the space below, please describe the learning environment of your precept section – what about it worked for you, what suggestions do you have to make it work better for you, what challenges did you face – and mention your preceptor by name.\n\n2. For future offerings of this course, how could we better align with the research goals of a Princeton student (such as their senior thesis)?\n\n3. Which course topics do you wish we discussed more of the mathematical background and rigor?  On the contrary, which topics could have had less of an emphasis  on mathematical background and rigor? Please be as specific as possible.\n:::\n\n::::\n\n\n# Footnotes\n\n::: {.callout-note collapse=\"true\"}\n\n## (optional) Additional Resources and References\n\n\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.2    fastmap_1.2.0     cli_3.6.3        \n [5] tools_4.4.2       htmltools_0.5.8.1 rstudioapi_0.17.1 yaml_2.3.10      \n [9] rmarkdown_2.29    knitr_1.50        jsonlite_1.8.9    xfun_0.52        \n[13] digest_0.6.37     rlang_1.1.5       evaluate_1.0.3   \n```\n\n\n:::\n:::\n\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}