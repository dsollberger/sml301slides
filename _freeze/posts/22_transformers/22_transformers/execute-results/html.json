{
  "hash": "0f081eae4b4b2e999a3aa2baf9e97b06",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"22: Transformers\"\nauthor: \"Derek Sollberger\"\ndate: \"2025-04-16\"\nformat:\n  html:\n    toc: true\n---\n\n::: {.cell}\n\n:::\n\n\n\n# Session 22: Transformers\n\n## Learning objectives:\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- Revise understanding of long-term memory\n- Explore text encoder-decoder modeling\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n![encoder-decoder](statquest_transformer.png)\n\n* image credit: [StatQuest](https://github.com/StatQuest/signa/blob/main/chapter_12/chapter_12_encoder_decoder_transformer.ipynb)\n:::\n\n::::\n\n## Main Example\n\n![Datenwissenschaft](Datenwissenschaft.png)\n\n::: {.callout-warning collapse=\"true\"}\n## Deutsch\n\nI did ask a German person to check my translation.  They kindly informed me that German data scientists tend to say \"data science\" instead of \"Datenwissenschaft\".\n\nI opted for the large compound word to fermet discussion about the grammatical differences.\n\n:::\n\n## Recall: RNNs\n\n::::: {.panel-tabset}\n\n## Example\n\n\n\n```{mermaid}\nflowchart LR\n\ninput1[input]\n\nneuron1[neuron]\n\noutput1[output]\n\ninput1 --> neuron1\nneuron1 --> output1\nneuron1 --> neuron1\n```\n\n\n\n## Unfolded\n\n![unfolded RNN](RNN_example_unfolded.png)\n\n## Vanishing Gradients\n\n::: {.callout-tip}\n## Gradient Propagation\n\n* we have **vanishing gradients** if $|w_{h,h}| < 1$\n* we have **exploding gradients** if $|w_{h,h}| > 1$\n\n:::\n\n:::::\n\n# LSTMs\n\n::: {.callout-tip}\n## LSTM\n\nEach *module* of a **Long Short Term Memory** (LSTM) network has\n\n* 3 inputs: data input, long-term memory (aka **forget gate**), short-term memory (aka **output gate**)\n\nand outputs new values for the forget gate and the output gate\n\n:::\n\n![LSTM network](LSTM_chain.png)\n\n* image source: [Chris Olah](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n\n::: {.callout-caution}\n## 867-5309\n\nTO DO: apply LSTM to the \"8675309\" sequence of integers\n\n:::\n\n# GRUs\n\n::: {.callout-tip}\n## GRU\n\nEach *module* of a **Gated Recurrent Unit** (GRU) network has\n\n* *2* inputs: data input, long-term memory (aka **forget gate**)\n\nand outputs a new value for the forget gate along with the data prediction.  Hence, the GRU architecture is simpler than an LSTM, but can produce similar results.\n\n:::\n\n![GRU diagram](GRU_diagram.png)\n\n* image source: [O'Reilly](https://www.oreilly.com/library/view/advanced-deep-learning/9781789956177/8ad9dc41-3237-483e-8f6b-7e5f653dc693.xhtml)\n\n![GRU concepts](GRU_conceptual.png)\n\n* image source: [Harshed Abdulla](https://medium.com/@harshedabdulla/understanding-gated-recurrent-units-grus-in-deep-learning-4404599dcefb)\n\n::: {.callout-note}\n## Long-term memory\n\nBoth LSTM and GRU architectures allow for processing of sequences of data while avoiding the vanishing gradient problem.\n\n:::\n\n![recap](RNN_LSTM_GRU.png)\n\n\n# Position Encoding\n\n::: {.callout-note}\n## Indexing\n\nAt this moment, we wish to keep track of word order (i.e. where they are in a sentence).  A first draft is to create an *index column*:\n\n1. I\n2. want\n3. to\n4. learn\n5. data\n6. science\n\n:::\n\n::: {.callout-warning}\n## Motivation\n\nHowever, as developers explored **positional embedding**, they wanted a system that satisfied useful qualities such as\n\n* distinguish between small and large distances\n* bounded values (between zero and one)\n* invariant to sequence length\n* deterministic (does not depend on RNG seed or initial values)\n\nsource: [Harrison Pim](https://harrisonpim.com/blog/understanding-positional-embeddings-in-transformer-models)\n\n:::\n\n::: {.callout-note}\n## Fourier Series\n\n![positional encoding calculations](positional_encoding_sine_cosine.png)\n\n* image source: [Kemal Erdem](https://erdem.pl/2021/05/understanding-positional-encoding-in-transformers)\n\n:::\n\n# Attention\n\n::::: {.panel-tabset}\n\n## Attention\n\n![attention](self_attention_1.png)\n\n* image source: [StatQuest](https://github.com/StatQuest/signa/blob/main/chapter_12/chapter_12_encoder_decoder_transformer.ipynb)\n\n## Masked Attention\n\n![attention](masked_attention_1.png)\n\n* image source: [StatQuest](https://github.com/StatQuest/signa/blob/main/chapter_12/chapter_12_encoder_decoder_transformer.ipynb)\n\n## Transformer Attention\n\n![attention](enc_dec_attention_1.png)\n\n* image source: [StatQuest](https://github.com/StatQuest/signa/blob/main/chapter_12/chapter_12_encoder_decoder_transformer.ipynb)\n\n:::::\n\n# Encoder\n\n## Main Example\n\nLet us try running through our main example through a trained transformer\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![Datenwissenschaft](Datenwissenschaft.png)\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![encoder-decoder](statquest_transformer.png)\n\n* image credit: [StatQuest](https://github.com/StatQuest/signa/blob/main/chapter_12/chapter_12_encoder_decoder_transformer.ipynb)\n:::\n\n::::\n\n## Word Encoding\n\nWe start out with one-hot encoding of the input multiplied by the encoder weights:\n\n$$\\begin{pmatrix} 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 1 \\end{pmatrix}\\begin{pmatrix} 0.34 & 0.13 \\\\ 0.23 & 0.23 \\\\ -1.12 & -0.19 \\\\ 2.21 & -0.64 \\\\ 0.46 & 0.27 \\\\ 0.53 & 0.81 \\\\ 1.11 & -1.69 \\end{pmatrix} = \\begin{pmatrix} 0.34 & 0.13 \\\\ 0.23 & 0.23 \\\\ -1.12 & -0.19 \\\\ 2.21 & -0.64 \\\\ 0.46 & 0.27 \\\\ 0.53 & 0.81 \\\\ 1.11 & -1.69 \\end{pmatrix}$$\n\n## Position Encoding\n\nNext, we get our position encoding values\n\n* $n_{\\text{input}} = 7$\n* $d_{\\text{model}} = 2$\n\n$$\\begin{pmatrix} 0.84 & 0.54 \\\\ 0.91 & -0.42 \\\\ 0.14 & -0.99 \\\\ -0.76 & -0.65 \\\\ -0.96 & 0.28 \\\\ -0.28 & 0.96 \\\\ 0.66 & 0.75 \\end{pmatrix}$$\n\nand then the word embedding plus position encoding is\n\n$$\\begin{pmatrix} 0.34 & 0.13 \\\\ 0.23 & 0.23 \\\\ -1.12 & -0.19 \\\\ 2.21 & -0.64 \\\\ 0.46 & 0.27 \\\\ 0.53 & 0.81 \\\\ 1.11 & -1.69 \\end{pmatrix} + \\begin{pmatrix} 0.84 & 0.54 \\\\ 0.91 & -0.42 \\\\ 0.14 & -0.99 \\\\ -0.76 & -0.65 \\\\ -0.96 & 0.28 \\\\ -0.28 & 0.96 \\\\ 0.66 & 0.75 \\end{pmatrix} = \\begin{pmatrix} 1.18 & 0.67 \\\\ 1.14 & -0.19 \\\\ -0.98 & -1.18 \\\\ 1.45 & -1.29 \\\\ \\frac{-1}{2} & 0.55 \\\\ \\frac{1}{4} & 1.77 \\\\ 1.77 & -0.94 \\end{pmatrix}$$\n\n## Self-Attention\n\n### Queries\n\nWe find the **queries** by multiplying the encoded values by the transpose of the query weights:\n\n$$\\begin{pmatrix} 1.18 & 0.67 \\\\ 1.14 & -0.19 \\\\ -0.98 & -1.18 \\\\ 1.45 & -1.29 \\\\ \\frac{-1}{2} & 0.55 \\\\ \\frac{1}{4} & 1.77 \\\\ 1.77 & -0.94 \\end{pmatrix}\\begin{pmatrix} 0.07 & 0.64 \\\\ -0.70 & -0.60 \\end{pmatrix} = \\begin{pmatrix} -0.39 & 0.35 \\\\ 0.21 & 0.84 \\\\ 0.76 & 0.08 \\\\ 1 & 1.70 \\\\ -0.42 & -0.65 \\\\ -1.22 & -0.90 \\\\ 0.78 & 1.70 \\end{pmatrix}$$\n\n### Keys\n\nWe find the **keys** by multiplying the encoded values by the transpose of the key weights:\n\n$$\\begin{pmatrix} 1.18 & 0.67 \\\\ 1.14 & -0.19 \\\\ -0.98 & -1.18 \\\\ 1.45 & -1.29 \\\\ \\frac{-1}{2} & 0.55 \\\\ \\frac{1}{4} & 1.77 \\\\ 1.77 & -0.94 \\end{pmatrix}\\begin{pmatrix} 0.55 & -0.23 \\\\ 0.12 & 0.44 \\end{pmatrix} = \\begin{pmatrix} 0.73 & 0.02 \\\\ 0.60 & -0.35 \\\\ -0.68 & -0.29 \\\\ 0.64 & -0.90 \\\\ -0.21 & 0.36 \\\\ 0.35 & 0.72 \\\\ 0.86 & -0.82 \\end{pmatrix}$$\n\n### Values\n\nWe find the **values** by multiplying the encoded values by the transpose of the value weights:\n\n$$\\begin{pmatrix} 1.18 & 0.67 \\\\ 1.14 & -0.19 \\\\ -0.98 & -1.18 \\\\ 1.45 & -1.29 \\\\ \\frac{-1}{2} & 0.55 \\\\ \\frac{1}{4} & 1.77 \\\\ 1.77 & -0.94 \\end{pmatrix}\\begin{pmatrix} 0.11 & 0.08 \\\\ 0.57 & -0.22 \\end{pmatrix} = \\begin{pmatrix} 0.51 & -0.05 \\\\ 0.02 & 0.13 \\\\ -0.78 & 0.18 \\\\ -0.58 & 0.40 \\\\ 0.26 & -0.16 \\\\ 1.04 & -0.37 \\\\ -0.34 & 0.35 \\end{pmatrix}$$\n\n### Self-Attention\n\nNow, **self-attention** is defined (in AI literature) as\n\n$$\\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^{T}}{\\sqrt{d_{\\text{model}}}}\\right)V$$\n\n$$\\begin{pmatrix} 0.13 & 0.12 & 0.18 & 0.11 & 0.18 & 0.17 & 0.10 \\\\ 0.16 & 0.13 & 0.11 & 0.09 & 0.17 & 0.23 & 0.10 \\\\ 0.17 & 0.16 & 0.08 & 0.16 & 0.11 & 0.15 & 0.18 \\\\ 0.20 & 0.12 & 0.05 & 0.06 & 0.15 & 0.35 & 0.08 \\\\ 0.11 & 0.14 & 0.20 & 0.18 & 0.13 & 0.09 & 0.16 \\\\ 0.08 & 0.11 & 0.33 & 0.15 & 0.14 & 0.07 & 0.12 \\\\ 0.19 & 0.11 & 0.06 & 0.06 & 0.17 & 0.35 & 0.07 \\end{pmatrix}\\begin{pmatrix} 0.51 & -0.05 \\\\ 0.02 & 0.13 \\\\ -0.78 & 0.18 \\\\ -0.58 & 0.40 \\\\ 0.26 & -0.16 \\\\ 1.04 & -0.37 \\\\ -0.34 & 0.35 \\end{pmatrix}$$\n\n::: {.callout-note}\n### Softmax\n\nBefore multiplying by $V$, the result of the softmax gives us a glimpse of how the machine views the relationship between the words.  For example, in the last row for \"science\", we see that \"science\" is most related to \"data\" in our input phrase.\n\n:::\n\n$$\\text{Attention}(Q, K, V) = \\begin{pmatrix} 0.06 & 0.03 \\\\ 0.20 & -0.01 \\\\ 0.06 & 0.08 \\\\ 0.40 & -0.09 \\\\ -0.12 & 0.12 \\\\ -0.23 & 0.12 \\\\ 0.40 & -0.09 \\end{pmatrix}$$\n\n## Encoder Output\n\nFinally, the encoder output is the sum of the encoded values and the self-attention:\n\n$$\\begin{pmatrix} 1.18 & 0.67 \\\\ 1.14 & -0.19 \\\\ -0.98 & -1.18 \\\\ 1.45 & -1.29 \\\\ \\frac{-1}{2} & 0.55 \\\\ \\frac{1}{4} & 1.77 \\\\ 1.77 & -0.94 \\end{pmatrix} + \\begin{pmatrix} 0.06 & 0.03 \\\\ 0.20 & -0.01 \\\\ 0.06 & 0.08 \\\\ 0.40 & -0.09 \\\\ -0.12 & 0.12 \\\\ -0.23 & 0.12 \\\\ 0.40 & -0.09 \\end{pmatrix} = \\begin{pmatrix} 1.24 & 0.70 \\\\ 1.34 & -0.20 \\\\ -0.92 & -1.10 \\\\ 1.85 & -1.38 \\\\ -0.62 & 0.67 \\\\ 0.02 & 1.89 \\\\ 2.17 & -1.03 \\end{pmatrix}$$\n\n\n# Decoder\n\nNow, let us try to extract the German translation through the decoder.\n\n## Word Encoding\n\nWe start out with one-hot encoding of the input multiplied by the encoder weights:\n\n$$\\begin{pmatrix} 1 & 0 & 0 & 0 & 0 & 0  \\\\ 0 & 1 & 0 & 0 & 0 & 0  \\\\ 0 & 0 & 1 & 0 & 0 & 0  \\\\ 0 & 0 & 0 & 1 & 0 & 0  \\\\ 0 & 0 & 0 & 0 & 1 & 0 & \\\\ 0 & 0 & 0 & 0 & 0 & 1  \\end{pmatrix}\\begin{pmatrix} 0.34 & 0.13 \\\\ 0.23 & 0.23 \\\\ -1.12 & -0.19 \\\\ 2.21 & -0.64 \\\\ 0.46 & 0.27 \\\\ 0.53 & 0.81 \\end{pmatrix} = \\begin{pmatrix} 0.34 & 0.13 \\\\ 0.23 & 0.23 \\\\ -1.12 & -0.19 \\\\ 2.21 & -0.64 \\\\ 0.46 & 0.27 \\\\ 0.53 & 0.81 \\end{pmatrix}$$\n\n## Position Encoding\n\nNext, we get our position encoding values\n\n* $n_{\\text{output}} = 6$\n* $d_{\\text{model}} = 2$\n\n$$\\begin{pmatrix} 0.84 & 0.54 \\\\ 0.91 & -0.42 \\\\ 0.14 & -0.99 \\\\ -0.76 & -0.65 \\\\ -0.96 & 0.28 \\\\ -0.28 & 0.96 \\end{pmatrix}$$\n\nand then the word embedding plus position encoding is\n\n$$\\begin{pmatrix} 0.34 & 0.13 \\\\ 0.23 & 0.23 \\\\ -1.12 & -0.19 \\\\ 2.21 & -0.64 \\\\ 0.46 & 0.27 \\\\ 0.53 & 0.81 \\end{pmatrix} + \\begin{pmatrix} 0.84 & 0.54 \\\\ 0.91 & -0.42 \\\\ 0.14 & -0.99 \\\\ -0.76 & -0.65 \\\\ -0.96 & 0.28 \\\\ -0.28 & 0.96 \\end{pmatrix} = \\begin{pmatrix} 1.18 & 0.67 \\\\ 1.14 & -0.19 \\\\ -0.98 & -1.18 \\\\ 1.45 & -1.29 \\\\ \\frac{-1}{2} & 0.55 \\\\ \\frac{1}{4} & 1.77 \\end{pmatrix}$$\n\n## Self-Attention\n\n### Queries\n\nWe find the **queries** by multiplying the encoded values by the transpose of the query weights:\n\n$$\\begin{pmatrix} 1.18 & 0.67 \\\\ 1.14 & -0.19 \\\\ -0.98 & -1.18 \\\\ 1.45 & -1.29 \\\\ \\frac{-1}{2} & 0.55 \\\\ \\frac{1}{4} & 1.77 \\end{pmatrix}\\begin{pmatrix} -0.56 & -0.20 \\\\ -0.33 & -0.43 \\end{pmatrix} = \\begin{pmatrix} -0.88 & -0.52 \\\\ -0.58 & -0.15 \\\\ 0.94 & 0.70 \\\\ -0.39 & 0.26 \\\\ 0.10 & -0.14 \\\\ -0.72 & -0.81 \\end{pmatrix}$$\n\n### Keys\n\nWe find the **keys** by multiplying the encoded values by the transpose of the key weights:\n\n$$\\begin{pmatrix} 1.18 & 0.67 \\\\ 1.14 & -0.19 \\\\ -0.98 & -1.18 \\\\ 1.45 & -1.29 \\\\ \\frac{-1}{2} & 0.55 \\\\ \\frac{1}{4} & 1.77 \\end{pmatrix}\\begin{pmatrix} 0.07 & 0.64 \\\\ -0.70 & -0.60 \\end{pmatrix} = \\begin{pmatrix} -0.39 & 0.35 \\\\ 0.21 & 0.84 \\\\ 0.76 & 0.08 \\\\ 1 & 1.70 \\\\ -0.42 & -0.65 \\\\ -1.22 & -0.90 \\end{pmatrix}$$\n\n### Values\n\nWe find the **values** by multiplying the encoded values by the transpose of the value weights:\n\n$$\\begin{pmatrix} 1.18 & 0.67 \\\\ 1.14 & -0.19 \\\\ -0.98 & -1.18 \\\\ 1.45 & -1.29 \\\\ \\frac{-1}{2} & 0.55 \\\\ \\frac{1}{4} & 1.77 \\end{pmatrix}\\begin{pmatrix} 0.55 & -0.23 \\\\ 0.12 & 0.44 \\end{pmatrix} = \\begin{pmatrix} 0.73 & 0.02 \\\\ 0.60 & -0.35 \\\\ -0.68 & -0.29 \\\\ 0.64 & -0.90 \\\\ -0.21 & 0.36 \\\\ 0.35 & 0.72 \\end{pmatrix}$$\n\n### Masked Self-Attention\n\nNow, **masked self-attention** is defined (in AI literature) as\n\n$$\\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^{T}}{\\sqrt{d_{\\text{model}}}} + M\\right)V$$\n\n$$\\begin{pmatrix} 1 & 0 & 0 & 0 & 0 & 0 \\\\ 0.57 & 0.43 & 0 & 0 & 0 & 0 \\\\ 0.21 & 0.40 & 0.39 & 0 & 0 & 0 \\\\ 0.29 & 0.27 & 0.20 & \\frac{1}{4} & 0 & 0 \\\\ 0.19 & 0.19 & 0.21 & 0.19 & 0.21 & 0 \\\\ 0.14 & 0.08 & 0.09 & 0.03 & 0.24 & 0.43 \\end{pmatrix}\\begin{pmatrix} 0.73 & 0.02 \\\\ 0.60 & -0.35 \\\\ -0.68 & -0.29 \\\\ 0.64 & -0.90 \\\\ -0.21 & 0.36 \\\\ 0.35 & 0.72 \\end{pmatrix}$$\n\n::: {.callout-note}\n### Softmax\n\nBefore multiplying by $V$, the result of the softmax gives us a glimpse of how the machine views the relationship between the words.  For example, in the last row for $EOS$ (\"end of sequence\"), we see that $EOS$ is most related to $EOS$ in our output phrase.\n\n:::\n\n$$\\text{Attention}(Q, K, V) = \\begin{pmatrix} 0.73 & 0.02 \\\\ 0.60 & -0.35 \\\\ -0.68 & -0.29 \\\\ 0.64 & -0.90 \\\\ -0.21 & 0.36 \\\\ 0.35 & 0.72 \\end{pmatrix}$$\n\n## Decoder Output\n\nFinally, the decoder output is the sum of the encoded values and the self-attention:\n\n$$\\begin{pmatrix} 0.34 & 0.13 \\\\ 0.23 & 0.23 \\\\ -1.12 & -0.19 \\\\ 2.21 & -0.64 \\\\ 0.46 & 0.27 \\\\ 0.53 & 0.81 \\end{pmatrix} + \\begin{pmatrix} 0.73 & 0.02 \\\\ 0.60 & -0.35 \\\\ -0.68 & -0.29 \\\\ 0.64 & -0.90 \\\\ -0.21 & 0.36 \\\\ 0.35 & 0.72 \\end{pmatrix} = \\begin{pmatrix} 1.91 & 0.69 \\\\ 1.82 & -0.32 \\\\ -0.85 & -1.43 \\\\ 1.84 & -1.66 \\\\ -0.31 & 0.33 \\\\ 0.45 & 2.09 \\end{pmatrix}$$\n\n\n# Encoder-Decoder\n\n## Self-Attention\n\n### Queries\n\nWe find the **queries** by multiplying the decoder values by the transpose of the query weights:\n\n$$\\begin{pmatrix} 1.91 & 0.69 \\\\ 1.82 & -0.32 \\\\ -0.85 & -1.43 \\\\ 1.84 & -1.66 \\\\ -0.31 & 0.33 \\\\ 0.45 & 2.09 \\end{pmatrix}\\begin{pmatrix} -0.56 & -0.20 \\\\ -0.33 & -0.43 \\end{pmatrix} = \\begin{pmatrix} -1.30 & -0.68 \\\\ -0.91 & -0.22 \\\\ 0.95 & 0.78 \\\\ -0.49 & 0.34 \\\\ 0.07 & -0.08 \\\\ -0.94 & -0.99 \\end{pmatrix}$$\n\n### Keys\n\nWe find the **keys** by multiplying the encoder values by the transpose of the key weights:\n\n$$\\begin{pmatrix} 1.24 & 0.70 \\\\ 1.34 & -0.20 \\\\ -0.92 & -1.10 \\\\ 1.85 & -1.38 \\\\ -0.62 & 0.67 \\\\ 0.02 & 1.89 \\\\ 2.17 & -1.03 \\end{pmatrix}\\begin{pmatrix} 0.19 & 0.30 \\\\ -0.19 & 0.63 \\end{pmatrix} = \\begin{pmatrix} 0.23 & 1.01 \\\\ 0.41 & 0.34 \\\\ 0.11 & -1.16 \\\\ 0.67 & -0.49 \\\\ -0.12 & 0.12 \\\\ -0.31 & 1.45 \\end{pmatrix}$$\n\n### Values\n\nWe find the **values** by multiplying the encoder values by the transpose of the value weights:\n\n$$\\begin{pmatrix} 1.24 & 0.70 \\\\ 1.34 & -0.20 \\\\ -0.92 & -1.10 \\\\ 1.85 & -1.38 \\\\ -0.62 & 0.67 \\\\ 0.02 & 1.89 \\\\ 2.17 & -1.03 \\end{pmatrix}\\begin{pmatrix} 0.41 & 0.41 \\\\ -0.31 & 0.13 \\end{pmatrix} = \\begin{pmatrix} 0.57 & 0.87 \\\\ 0.85 & 0.70 \\\\ 0.09 & -0.54 \\\\ 1.27 & 0.54 \\\\ -0.23 & -0.09 \\\\ -0.46 & 0.46 \\end{pmatrix}$$\n\n### Self-Attention\n\nNow, **self-attention** is defined (in AI literature) as\n\n$$\\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^{T}}{\\sqrt{d_{\\text{model}}}}\\right)V$$\n\n$$\\begin{pmatrix} 0.10 & 0.12 & 0.31 & 0.14 & 0.21 & 0.13 \\\\ 0.14 & 0.14 & 0.21 & 0.13 & 0.20 & 0.18 \\\\ \\frac{1}{4} & 0.19 & 0.07 & 0.15 & 0.12 & 0.22 \\\\ 0.19 & 0.15 & 0.12 & 0.11 & 0.17 & \\frac{1}{4} \\\\ 0.16 & 0.17 & 0.18 & 0.18 & 0.17 & 0.15 \\\\ 0.08 & 0.11 & 0.38 & 0.17 & 0.18 & 0.08 \\end{pmatrix}\\begin{pmatrix} 0.57 & 0.87 \\\\ 0.85 & 0.70 \\\\ 0.09 & -0.54 \\\\ 1.27 & 0.54 \\\\ -0.23 & -0.09 \\\\ -0.46 & 0.46 \\end{pmatrix}$$\n$$\\begin{pmatrix} 0.57 & 0.87 \\\\ 0.69 & 0.80 \\\\ 0.49 & \\frac{1}{4} \\\\ 0.72 & 0.47 \\\\ 0.48 & 0.27 \\\\ -0.07 & 0.31 \\end{pmatrix}$$\n\n## Encoder-Decoder Output\n\nFinally, the encoder-decoder output is the sum of the decoder values and the self-attention:\n\n$$\\begin{pmatrix} 1.91 & 0.69 \\\\ 1.82 & -0.32 \\\\ -0.85 & -1.43 \\\\ 1.84 & -1.66 \\\\ -0.31 & 0.33 \\\\ 0.45 & 2.09 \\end{pmatrix} + \\begin{pmatrix} 0.57 & 0.87 \\\\ 0.69 & 0.80 \\\\ 0.49 & \\frac{1}{4} \\\\ 0.72 & 0.47 \\\\ 0.48 & 0.27 \\\\ -0.07 & 0.31 \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{4} & 1.54 \\\\ 1.83 & 0.61 \\\\ -0.49 & -0.93 \\\\ 2.17 & -0.82 \\\\ -0.02 & 0.82 \\\\ 0.18 & 2.08 \\end{pmatrix}$$\n\n# Fully-Connected Layer\n\nWe are almost done!\n\n## Weights\n\nFor the fully-connected layer at the end of the transformer, we multiply the output of the encoder-decoder by the weights of the dense layer:\n\n$$\\begin{pmatrix} \\frac{7}{4} & 1.54 \\\\ 1.83 & 0.61 \\\\ -0.49 & -0.93 \\\\ 2.17 & -0.82 \\\\ -0.02 & 0.82 \\\\ 0.18 & 2.08 \\end{pmatrix}\\begin{pmatrix} 0.36 & -0.70 & -0.54 & 0.20 & 0.22 & 0.55 \\\\ -0.43 & -0.27 & 0.58 & 0.29 & -0.01 & \\frac{-1}{2} \\end{pmatrix}$$\n$$\\begin{pmatrix} -0.03 & -1.64 & -0.05 & 0.80 & 0.37 & 0.19 \\\\ 0.40 & -1.44 & -0.63 & 0.54 & 0.40 & 0.70 \\\\ 0.22 & 0.59 & -0.27 & -0.37 & -0.10 & 0.20 \\\\ 1.14 & -1.30 & -1.65 & 0.20 & 0.49 & 1.61 \\\\ -0.36 & -0.21 & 0.49 & 0.23 & -0.01 & -0.42 \\\\ -0.83 & -0.69 & 1.11 & 0.64 & 0.02 & -0.94 \\end{pmatrix}$$\n\n## Bias\n\nWe add the bias values to each column of the previous result:\n\n\\begin{pmatrix} 0.01 & -2.12 & 0.17 & 0.56 & 0.59 & 0.04 \\\\ 0.44 & -1.92 & -0.41 & 0.30 & 0.62 & 0.55 \\\\ 0.26 & 0.11 & -0.05 & -0.61 & 0.12 & 0.05 \\\\ 1.18 & -1.78 & -1.43 & -0.04 & 0.71 & 1.46 \\\\ -0.32 & -0.69 & 0.71 & -0.01 & 0.21 & -0.57 \\\\ -0.79 & -1.17 & 1.33 & 0.40 & 0.24 & -1.09 \\end{pmatrix}\n\n## Softmax\n\nFinally, we apply a softmax rowwise.\n\n$$\\begin{pmatrix} 0.15 & 0.02 & 0.17 & \\frac{1}{4} & 0.26 & 0.15 \\\\ 0.21 & 0.02 & 0.09 & 0.19 & \\frac{1}{4} & 0.24 \\\\ 0.21 & 0.18 & 0.16 & 0.09 & 0.19 & 0.17 \\\\ 0.30 & 0.02 & 0.02 & 0.09 & 0.19 & 0.39 \\\\ 0.12 & 0.08 & 0.34 & 0.16 & 0.20 & 0.09 \\\\ 0.06 & 0.04 & 0.49 & 0.20 & 0.17 & 0.04 \\end{pmatrix}$$\n\n# Translation?\n\nFollowing the softmax, starting with $SOS$ (\"start of sequence\"), we have\n\n> SOS, lernen, mÖchte, SOS, ...\n\n\n# Quo Vadimus?\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n* due this Friday (April 18):\n\n    - Precept 10\n    - Research Consent\n    \n* semester projects will be due May 10\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\nStudent evaluation custom questions:\n\n1. In the space below, please describe the learning environment of your precept section – what about it worked for you, what suggestions do you have to make it work better for you, what challenges did you face – and mention your preceptor by name.\n\n2. For future offerings of this course, how could we better align with the research goals of a Princeton student (such as their senior thesis)?\n\n3. Which course topics do you wish we discussed more of the mathematical background and rigor?  On the contrary, which topics could have had less of an emphasis  on mathematical background and rigor? Please be as specific as possible.\n:::\n\n::::\n\n\n# Footnotes\n\n::: {.callout-note collapse=\"true\"}\n\n## (optional) Additional Resources and References\n\n\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.2    fastmap_1.2.0     cli_3.6.3        \n [5] tools_4.4.2       htmltools_0.5.8.1 rstudioapi_0.17.1 yaml_2.3.10      \n [9] rmarkdown_2.29    knitr_1.49        jsonlite_1.8.9    xfun_0.50        \n[13] digest_0.6.37     rlang_1.1.5       evaluate_1.0.3   \n```\n\n\n:::\n:::\n\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}