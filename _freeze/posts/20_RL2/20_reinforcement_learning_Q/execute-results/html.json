{
  "hash": "23c428bb0414bffc3a9d2790e898c008",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"20: Reinforcement Learning\"\nauthor: \"Derek Sollberger\"\ndate: \"2025-04-09\"\nformat:\n  html:\n    toc: true\n---\n\n::: {.cell}\n\n:::\n\n\n\n# Session 20: Reinforcement Learning\n\n## Learning objectives:\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- Explore the breadth of reinforcement learning\n- Introduce Q Learning\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n![Reinforcement Learning](RL_robot.png)\n\n* image credit: [Data Base Camp](https://databasecamp.de/en/ml/q-learnings)\n:::\n\n::::\n\n# Monte Carlo Methods\n\n::: {.callout-note}\n## Relaxing the Distribution Assumption\n\nWe start out by no longer assuming that we know the distribution\n\n$$p(s',r|s,a)$$\n\nSimilar to bootstrapping methods in statistics, we *simulate* a distribution by collecting samples of trajectories\n\n$$S_{0}, A_{0}, R_{1}, S_{1}, A_{1}, R_{2}, ..., R_{T}, S_{T}$$\n\nby proceeding through a Markov Decision Process (MDP)\n:::\n\n::: {.callout-warning}\n## MC distribution\n\nWithout the theoretical distribution $p(s',r|s,a)$\n\n* we don't have the expected values, so instead we use averages of Monte Carlo methods as unbiased estimators\n* we deploy Generalized Policy Iteration (GPI) to approximate the optimal policy $\\pi_{*}$\n:::\n\n## Model-Free\n\n::: {.callout-tip}\n## Model\n\nA **model** is a mechanism that an agent uses to predict the results of its actions\n\n* e.g. estimating $p(s',r|s,a)$\n:::\n\n::: {.callout-tip}\n## Model-Free\n\n* **Model-Based**: methods that use a model to *plan* actions beforehand\n* **Model-Free**: methods that *learn* action-to return associations\n:::\n\n::: {.callout-note}\n## MC is Model-Free\n\n*Monte Carlo methods are model-free*. As such, it may be more useful to estimate state-action values $q$ rather than state values $v$. Recall that\n\n$$Q(s,a) = \\displaystyle\\sum_{\\begin{array}{c} s'\\in\\mathcal{S} \\\\ r\\in\\mathcal{R}\\end{array}} p(s',r|s,a)[r + \\gamma V(s')]$$\n\nBut since we are no longer assuming knowledge of $p(s',r|s,a)$, let us estimate the optimal state-action values $q_{*}(s,a)$ directly.\n\n:::\n\n\n# Monte Carlo Evaluation\n\n::: {.callout-tip}\n## MRP\n\nA **Markov Reward Process** (MRP) is a Markov Decision Process (MDP) without actions\n\n$$S_{0}, R_{1}, S_{1}, R_{2}, S_{3}, R_{3}, ..., R_{T}, S_{T}$$\n:::\n\n::: {.callout-note}\n## Update Rule\n\nWith a data set of $M$ trajectories ($m = 1, 2, ..., M$),\n\n$$V(s_{t}^{m}) \\leftarrow V(s_{t}^{m}) + \\frac{1}{C(s_{t}^{m})}\\left( g_{t}^{m} - V(s_{t}^{m}) \\right)$$\n\nwhere\n\n$$V(s_{t}^{m}) = \\frac{1}{C(s)}\\displaystyle\\sum_{m=1}^{M}\\sum_{\\tau=0}^{T_{m}-1} \\delta(s_{\\tau}^{m}, s)g_{\\tau}^{m}$$\n\nis estimating the expected return $\\text{E}_{\\pi}]G_{t}|S_{t}=s]$\n\n* $C(s)$ is a count of how many times state $s$ was visited\n* to keep track if state $s$ was visited: $\\delta(s_{\\tau}^{m}, s) = \\begin{cases} 1, & s_{\\tau}^{m} = s \\\\ 0, & s_{\\tau}^{m} \\neq s \\end{cases}$\n\n:::\n\n::: {.callout-tip}\n## Constant $\\alpha$ MC\n\nBut if we are assured that our Monte Carlo approach will converge, then we can simplify the update rule to\n\n$$V(s_{t}^{m}) \\leftarrow V(s_{t}^{m}) + \\alpha\\left( g_{t}^{m} - V(s_{t}^{m}) \\right), \\quad \\alpha \\in (0,1)$$\n:::\n\n::: {.callout-warning}\n## How do we choose $\\alpha$?\n\nAlas, **learning rate** $\\alpha$ is yet another hyperparameter that has trade-offs\n\n* \"small\" $\\alpha$: slow learning, but more reliable estimate\n* \"large\" $\\alpha$: fast learning, but noisy estimate\n:::\n\n# Exploration-Exploitation Trade-Off\n\n::: {.callout-tip}\n## Exploration-Exploitation Trade-Off\n\n* To *discover optimal policies*, we must **explore** all state-action pairs.\n* To *get high returns*, we must **exploit** known high value pairs.\n\n:::\n\n::: {.callout-note}\n## $\\epsilon$ Greedy Policy\n\nIn theory, with infinite data, optimal policy $\\pi_{*}$ is always a limit of a **soft policy**:\n\n$$\\pi(a|s) > 0, \\quad \\forall s \\in \\mathcal{S} \\quad \\forall a \\in \\mathcal{A}$$\n\nInstead, we allow for some random selection of actions.  For $u \\in U(0,1)$ and probability $\\epsilon \\in (0,1)$\n\n* if $u < \\epsilon$, choose an action randomly from $\\mathcal{A}$ (uniformly distributed)\n* if $u \\geq \\epsilon$, choose action $\\text{argmax}_{a}Q(s,a)$\n\n:::\n\n# Case Study: Blackjack\n\n::::: {.panel-tabset}\n\n## Game\n\nIn the card game Blackjack ([rules](https://bicyclecards.com/how-to-play/blackjack)), a player is dealt two cards and the dealer also starts with two cards for themselves.  Only one of the dealer's cards is visible.  If the dealer's visible card is a \"usable\" ace, the odds can change drastically.  We hope to find a *policy* for choosing between \"hit\" (gain another card) or \"stay\"---while also considering the possibility of a usuable ace by the dealer---toward the goal of having cards that have a high total that does not exceed 21.\n\n## Settings\n\nThe following images come from Mutual information ([video](https://www.youtube.com/watch?v=bpUszPiWM7o&t=300s), [code](https://github.com/Duane321/mutual_information/tree/main/videos/monte_carlo_for_RL_and_off_policy_methods)), where the hyperparameter settings included\n\n* $\\epsilon = 0.1$\n* $\\alpha = \\frac{1}{5000}$\n* $M = 10^{7}$ games played!\n\n## Q Table\n\n![Q Table](blackjack_Q_table.png)\n\n* image credit: [Mutual Information](https://www.youtube.com/watch?v=bpUszPiWM7o&t=300s)\n\n## Constant alpha\n\n![Constant alpha](blackjack_constant_alpha.png)\n\n* image credit: [Mutual Information](https://www.youtube.com/watch?v=bpUszPiWM7o&t=300s)\n\n:::::\n\n# Off-Policy Methods\n\n::: {.callout-warning}\n## Behavior Policy\n\n* **Behavior policy** $b(a|s)$ generates the trajectory data\n* **Target policy** $\\pi(a|s)$ is what we have been generating so far\n\nand they might not be the same (especially if we do not have the data trajectories known in advance).\n\n:::\n\n::: {.callout-note}\n## Off-Policy Methods\n\n* **On-Policy Methods** $b = \\pi$ \n* **Off-Policy Methods** $b \\neq \\pi$ \n\nThat is, the data trajectories should be generated after estimating a policy.\n\n:::\n\n::: {.callout-note}\n## State-Action Calculation\n\nWe still want to estimate the state-action values from a policy as\n\n$$q_{\\pi}(s,a) = \\text{E}_{\\pi}[G_{t}|S_{t} = s, A_{t}= a]$$\n\nbut the data trajectories might have a slightly different distribution with \n\n$$\\text{E}_{b}[G_{t}|S_{t} = s, A_{t} = a]$$\n\nFrom [importance sampling](https://builtin.com/articles/importance-sampling)\n\n$$q_{\\pi}(s,a) = \\text{E}_{\\pi}[\\rho \\cdot G_{t}|S_{t} = s, A_{t}= a]$$\nwhere\n\n$$\\rho = \\frac{p_{\\pi}(G_{t})}{p_{b}(G_{t})} = \\displaystyle\\prod_{\\tau = t+1}^{T-1} \\frac{\\pi(A_{t}|S_{t})}{b(A_{t}|S_{t})}$$\n\nand we need **coverage** (i.e. non-zero probabilities $\\pi(a|s) > 0$ and $b(a|s) > 0$ over the same states).\n\n\n:::\n\n\n# Case Study: Blackjack Revisited\n\n::::: {.panel-tabset}\n\n## Settings\n\nInstead of an $\\epsilon$-soft algorithm, the trajectory samples are now created under a provided behavior policy $b$, and the returns $g_{t}^{m}$ are calculated in part with a scaling factor of $\\rho_{t}^{m}$.\n\n## Q Table\n\n![Q Table](blackjack_Q_table.png)\n\n* image credit: [Mutual Information](https://www.youtube.com/watch?v=bpUszPiWM7o&t=1500s)\n\n## Constant alpha\n\n![Constant alpha](blackjack_constant_alpha.png)\n\n* image credit: [Mutual Information](https://www.youtube.com/watch?v=bpUszPiWM7o&t=1500s)\n\n\n\n:::::\n\n\n\n\n# Quo Vadimus?\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n* due this Friday (April 11):\n\n    - Precept 9\n    - Data Glimpse\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n* semester projects will be due May 10\n:::\n\n::::\n\n\n# Footnotes\n\n::: {.callout-note collapse=\"true\"}\n\n## (optional) Additional Resources and References\n\n* [Reinforcement Learning by the Book](https://www.youtube.com/watch?v=bpUszPiWM7o) by Duane \"Mutual Information\" Rich\n\n* [Q-Learning Easily Explained](https://databasecamp.de/en/ml/q-learnings) at Data Base Camp\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.2    fastmap_1.2.0     cli_3.6.3        \n [5] tools_4.4.2       htmltools_0.5.8.1 rstudioapi_0.17.1 yaml_2.3.10      \n [9] rmarkdown_2.29    knitr_1.49        jsonlite_1.8.9    xfun_0.50        \n[13] digest_0.6.37     rlang_1.1.5       evaluate_1.0.3   \n```\n\n\n:::\n:::\n\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}