{
  "hash": "381371bd7b98ae6a84dada0f6e7ff535",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"19: Reinforcement Learning\"\nauthor: \"Derek Sollberger\"\ndate: \"2025-04-07\"\nformat:\n  html:\n    toc: true\n---\n\n::: {.cell}\n\n:::\n\n\n\n# Session 19: Reinforcement Learning\n\n## Learning objectives:\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- Introduce reinforcement learning\n- Discuss Markov Decision Processes\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n![robots!](boston_dynamics_robots.png)\n\n* image credit: [Boston Dynamics](https://johnkoetsier.com/boston-dynamics-the-golden-age-of-robotics/)\n\n:::\n\n::::\n\n# RL in the News\n\n::::: {.panel-tabset}\n\n## Checkers\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![Chinook, 1994](checkers_1994.png)\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n[Solving Checkers](https://www.ijcai.org/Proceedings/05/Papers/0515.pdf) by Schaeffer, et al., 2005\n\n:::\n\n::::\n\n## Chess\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![Deep Blue, 1997](chess_1997.png)\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n[Deep Blue](https://www.chess.com/terms/deep-blue-chess-computer) defeated Garry Kasparov in 1997\n\n:::\n\n::::\n\n## Go\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![AlphaGo, 2016](go_2016.png)\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n[AlphaGo](https://gomagic.org/alphago-and-lee-sedol/) defeated Lee Sedol in 2016\n\n:::\n\n::::\n\n## Starcraft\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![AlphaStar, 2019](alphastar_2019.png)\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n[AlphaStar](https://deepmind.google/discover/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii/) won competitive matches against LiquidTLO in 2019.\n\n:::\n\n::::\n\n:::::\n\n# Gym: Mountain Car\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![Mountain Car](mountain_car.gif)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n* states: $x$ coordinate\n* actions: push left or push right\n* reward: given upon reaching top of hill\n:::\n\n::::\n\n\n# Case Study: Where to Eat?\n\n:::: {.columns}\n\n::: {.column width=\"35%\"}\n![food choices](food_choices.png)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"55%\"}\n* $F$: choosing to eat at Frist Campus Center\n* $R$: choosing to eat at your residential college\n* Input: hunger\n\n$$H \\in [0,1]$$\n\n* Reward: net satisfaction\n\n    * meal size\n    * distance to food\n    \nExample adapted from examples by Professor Josh Starmer\n\n:::\n\n::::\n\n::: {.callout-note}\n## What if we don't already have training data?\n\nIn previous sessions, we trained a model with training data.  In particular, we were able to compute a loss function (such as the difference between predictions and observed outputs).\n\nHere, instead, we will use **reinforcement learning** to employ *rewards* to help provide signals for the backpropagation.\n:::\n\n\n## Neural Network\n\n\n\n```{mermaid}\nflowchart LR\n\ninput[hunger]\nactivation[sigmoid]\noutput[residential college]\n\ninput -- wH + b --> activation\nactivation --> output\n```\n\n\n\n* linear transformation: $L = wh + b$ in the fully connected layer\n* output: probability of choosing to eat at your residential college (instead of Frist)\n* learning rate (hyperparameter): 0.301\n\n::: {.callout-note collapse=\"true\"}\n## Sigmoid instead of ReLU?\n\nHere, I am using the sigmoid activation function instead of the ReLU simply because it is easier in the slides presentation.\n\n* sigmoid\n\n$$A(x) = \\frac{1}{1 + e^{-x}} \\text{ with }\\frac{dA}{dx} = \\sigma(x)(1 - \\sigma(x))$$\n* ReLU\n\n$$f(x) = \\text{max}(x,0) \\text{ with } f'(x) = \\begin{cases} 1, & x > 0 \\\\ 0, & x \\leq 0 \\end{cases}$$\n\n:::\n\n## Forward Propagation\n\nSuppose that you are not hungry ($H = 0$) but you still want to eat some dinner at this time.\n\n* initialized as $w = 20$ and $b = 0$\n* we will train for the bias value\n* we don't know the meal satisfaction in advance\n\n\n\n```{mermaid}\nflowchart LR\n\ninput[H = 0]\nactivation[sigmoid]\noutput[residential college]\n\ninput -- 20H + 0 --> activation\nactivation --> output\n```\n\n\n\nSo far, our *distribution* of choices is\n\n* $P(\\text{residential college}) = A(0) = \\frac{1}{1 + e^{-0}} = 0.5$\n* $P(\\text{Frist}) = 1 - P(\\text{residential college}) = 0.5$\n\nwhose $[0,1]$ probability space can be mapped as\n\n* $[0, 0.5)$: go to Frist\n* $(0.5, 1.0]$: go to your residential college\n\nNow, suppose that we used a random number generator and obtained 0.2025.  Thus, we are visiting Frist.\n\n## Cross Entropy\n\nWe will use *cross entropy* to measure the loss.\n\n$$C_{\\text{res}} = -\\ln(P(\\text{res}))$$\n$$C_{\\text{Frist}} = -\\ln(1 - P(\\text{res}))$$\n\n## Chain Rule\n\nToward using the output to train the bias, we examine the chain rule to apply the change in the cross entropy with respect to the bias.\n\n$$\\begin{array}{rcl}\n\\frac{dC_{\\text{Frist}}}{db} & = & \\frac{dC_{\\text{Frist}}}{dP(\\text{res})} \\cdot \\frac{dP(\\text{res})}{dL} \\cdot \\frac{dL}{db} \\\\\n~ & = & \\frac{1}{1 - P(\\text{res})} \\cdot P(\\text{res}) \\cdot (1 - P(\\text{res})) \\cdot (1) \\\\\n~ & = & 0.5 \\\\\n\\end{array}$$\n\n::: {.callout-note}\n## Signs\n\nIf we were training a simple neural network\n\n* negative derivative --> decrease parameter\n* positive derivative --> increase parameter\n\n:::\n\n::: {.callout-tip}\n## Rewards\n\nHowever, we need to incorporate the possible **rewards**\n\n$$\\text{updated derivative} = \\text{derivative}*\\text{reward}$$\n\n:::\n\n## Rewards\n\nIn this scenario, let us assign the **rewards** as follows\n\n* If hunger = 0 and we choose Frist: reward = 1.0\n\n    * perhaps valued location over quantity of food\n\n* If hunger = 0 and we choose the residential college: reward = -1.0\n\n::: {.callout-caution collapse=\"true\"}\n## How are reward amounts chosen?\n\nFor now, we note that reward amounts do not have to be $\\pm 1.0$.  Amounts can be *weighted* to give more priority to certain outcomes.\n:::\n\nThus, in our scenario here, our updated derivative is\n\n$$\\begin{array}{rcl}\n\\text{updated derivative} & = & \\text{derivative}*\\text{reward} \\\\\n~ & = & (0.5)(1.0) \\\\\n~ & = & 0.5\n\\end{array}$$\n\n## Update\n\n$$\\begin{array}{rcl}\n\\text{step size} & = &\\text{learning rate}*\\text{updated derivative} \\\\\n~ & = & (0.301)(0.5) \\\\\n~ & = & 0.1505 \\\\\n\\end{array}$$\n\n$$\\begin{array}{rcl}\n\\text{updated bias} & = & \\text{old bias} - \\text{step size} \\\\\n~ & = & 0.0 - 0.1505 \\\\\n~ & = & -0.1505 \\\\\n\\end{array}$$\n\n\n\n```{mermaid}\nflowchart LR\n\ninput[H = 0]\nactivation[sigmoid]\noutput[residential college]\n\ninput -- 20H - 0.1505 --> activation\nactivation --> output\n```\n\n\n\nNow, our distribution of choices is\n\n* $P(\\text{residential college}) = A(-0.1505) = \\frac{1}{1 + e^{0.1505}} \\approx 0.4624$\n* $P(\\text{Frist}) = 1 - P(\\text{residential college}) \\approx 0.5376$\n\nwhose $[0,1]$ probability space can be mapped as\n\n* $[0, 0.5376)$: go to Frist\n* $(0.5376, 1.0]$: go to your residential college\n\n## Second Epoch\n\n::::: {.panel-tabset}\n\n## Start\n\nNow, suppose that we used a random number generator and obtained 0.678.  Thus, we are visiting your residential college.\n\n* $P(\\text{residential college}) = A(-0.1505) = \\frac{1}{1 + e^{0.1505}} \\approx 0.4624$\n* $P(\\text{Frist}) = 1 - P(\\text{residential college}) \\approx 0.5376$\n\n\n## CE\n\n$$\\begin{array}{rcl}\n\\frac{dC_{\\text{res}}}{db} & = & \\frac{dC_{\\text{res}}}{dP(\\text{res})} \\cdot \\frac{dP(\\text{res})}{dL} \\cdot \\frac{dL}{db} \\\\\n~ & = & \\frac{-1}{P(\\text{res})} \\cdot P(\\text{res}) \\cdot (1 - P(\\text{res})) \\cdot (1) \\\\\n~ & = & -0.5376 \\\\\n\\end{array}$$\n\n## Rewards\n\nBut eating at the residential college when you are not hungry was assigned a reward value of -1.0.\n\n$$\\begin{array}{rcl}\n\\text{updated derivative} & = & \\text{derivative}*\\text{reward} \\\\\n~ & = & (-0.5376)(-1.0) \\\\\n~ & = & 0.5376 \n\\end{array}$$\n\n## Update\n\n$$\\begin{array}{rcl}\n\\text{step size} & = &\\text{learning rate}*\\text{updated derivative} \\\\\n~ & = & (0.301)(0.5376) \\\\\n~ & \\approx & 0.1618 \\\\\n\\end{array}$$\n\n$$\\begin{array}{rcl}\n\\text{updated bias} & = & \\text{old bias} - \\text{step size} \\\\\n~ & = & -0.1505 - 0.1618 \\\\\n~ & = & -0.3123 \\\\\n\\end{array}$$\n\n:::::\n\n\n\n```{mermaid}\nflowchart LR\n\ninput[H = 0]\nactivation[sigmoid]\noutput[residential college]\n\ninput -- 20H - 0.3123 --> activation\nactivation --> output\n```\n\n\n\nAfter the second epoch, our distribution of choices is\n\n* $P(\\text{residential college}) = A(-0.3123) = \\frac{1}{1 + e^{0.3123}} \\approx 0.4226$\n* $P(\\text{Frist}) = 1 - P(\\text{residential college}) \\approx 0.5774$\n\nwhose $[0,1]$ probability space can be mapped as\n\n* $[0, 0.5774)$: go to Frist\n* $(0.5774, 1.0]$: go to your residential college\n\nThat is, in situations without hunger, your chance of choosing dining at your residential college is *decreasing*.\n\n## Many Epochs\n\nAssuming that we can get data from many dinners (and/or many students), the neural network would be trained over\n\n* many epochs\n* many values for hunger ($H \\in [0,1]$)\n\nand would perhaps converage toward\n\n\n\n```{mermaid}\nflowchart LR\n\ninput[H]\nactivation[sigmoid]\noutput[residential college]\n\ninput -- 20H - 10 --> activation\nactivation --> output\n```\n\n\n\n$$\\begin{array}{rcl}\n  H \\in [0, 0.5) & \\rightarrow & \\text{prefer Frist} \\\\\n  H \\in (0.5, 1.0] & \\rightarrow & \\text{prefer residential college} \\\\\n\\end{array}$$\n\n\n# SAR\n\nTo rigorously formulate the concepts of reinforcement learning, we think of states, actions, and rewards.\n\n![reinforcement learing structure](RL_main.png)\n\n* image credit: [Dive into Deep Learning](https://d2l.ai/chapter_reinforcement-learning/index.html)\n\nFor some terminology,\n\n* $\\mathcal{S}$: set of states\n* $\\mathcal{A}$: set of actions\n* *distribution*: $P(s'|s,a)$\n* *reward*: $r(s,a)$\n\n# MDP\n\nTogether we have a **Markov decision process** (MDP).  \n\n$$\\text{MDP}: (\\mathcal{S}, \\mathcal{A}, T, r)$$\n\nStarting at an *initial state* $s_{0}$, a **trajectory** happens over time $t$\n\n$$t \\in \\{0, 1, 2, ... , T\\}$$\n\n(where $T$ is the **terminal time**) and looks like\n\n$$\\tau = (s_{0}, a_{0}, r_{0}, s_{1}, a_{1}, r_{1}, s_{2}, a_{2}, r_{2}, ...)$$\n\n::: {.callout-note collapse=\"true\"}\n## Markov assumption\n\nRecall at for a Markov process, we assume that the present iteration (i.e. its probabilities) depend only on the previous iteration\n\n$$P(s_{t}|s_{t-1}, a_{t-1})$$\n:::\n\nIn terms of random variables, the trajectories are\n\n$$\\tau \\in (S_{0}, A_{0}, R_{0}, S_{1}, A_{1}, R_{1}, S_{2}, A_{2}, R_{2}, ...)$$\n\n## Return\n\nThe **return** of a trajectory is the total of the rewards\n\n$$r_{0} + \\gamma r_{1} + \\gamma^{2}r_{2} + \\gamma_{3}r_{3} + ... = \\displaystyle\\sum_{t = 0}^{T} \\gamma^{t}r_{t}$$\n\nwhere $\\gamma \\in [0,1]$ is the **discount rate**.\n\n::: {.callout-tip}\n## Goal\n\nThe **goal** in reinforcement learning is to seek out a **policy** $\\pi$ that maximizes the return.\n:::\n\nFrom a later moment in time (i.e. not the initial state), we can think of the return as\n\n$$G_{t} = \\displaystyle\\sum_{k = t+1}^{T} \\gamma^{k - t - 1}R_{k}$$\n\nand then the goal can be expressed as\n\n$$\\text{max}_{\\pi} \\text{E}_{\\pi}[G_{t}]$$\n\n\n# Ethics Corner: Training Data with Human Subjects\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![AI x-ray bias](ai_xray_bias.png)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n*AI models miss disease in Black and female patients*\n\n* [Science Magazine](https://www.science.org/content/article/ai-models-miss-disease-black-female-patients), March 26, 2025\n:::\n\n::::\n\n\n\n# Bellman Equations\n\n## State and Action Functions\n\n::: {.callout-note}\n## State Value Function\n\n$$v_{\\pi}(s) = \\text{E}[G_{t}|S_{t} = s]$$\n\nis the value of being at state $s$, and is defined as the expected value of the return given that we are at state $s$.\n\n:::\n\n::: {.callout-note}\n## Action Value Function\n\n$$q_{\\pi}(s,a) = \\text{E}[G_{t}|S_{t} = s, A_{t} = a]$$\n\nis the expected return given that we are at state $s$ and take action $a$.\n\n:::\n\n## Optimal Policy\n\nThere exists an optimal policy $\\pi_{*}$ where\n\n$$\\begin{array}{rclc}\n  v_{\\pi_{*}}(s) & \\geq & v_{\\pi}(s) & \\forall s \\forall \\pi \\\\\n  q_{\\pi_{*}}(s,a) & \\geq & q_{\\pi}(s,a) & \\forall s \\forall a \\forall \\pi \\\\\n\\end{array}$$\n\nTo iterate toward an optimal policy, we try dynamic programming.\n\n::: {.callout-warning collapse=\"true\"}\n## Dynamic Programming\n\nHere, dynamic programming will assume *complete* knowledge of the Markov decision process, where\n\n* state space $S$ is finite\n* state space is discrete\n* temporal space is finite (i.e. $T < \\infty$)\n\nDynamic programming can rely on knowing distribution\n\n$$P(s', r| s,a)$$\n:::\n\n*Bellman optimality* says that the agent must choose an action that has the maximum value. Computing total probabilities yields\n\n## Forming the Bellman Equations\n\n$$\\begin{array}{c|rcl}\n  (1) & v_{\\pi}(s) & = & \\displaystyle\\sum_{a\\in\\mathcal{A}(s)} \\pi(a|s)q_{\\pi}(s,a) \\\\\n  (2) & q_{\\pi}(s,a) & = & \\displaystyle\\sum_{\\begin{array}{c} s'\\in\\mathcal{S} \\\\ r\\in\\mathcal{R}\\end{array}} p(s',r|s,a)[r + \\gamma v_{\\pi}(s')] \n\\end{array}$$\n\n* If we substitute (2) into (1), we can form the **Bellman Equation for state values** that relates any state value from any state value one step away:\n\n$$v_{\\pi}(s) = \\displaystyle\\sum_{a\\in\\mathcal{A}(s)} \\pi(a|s)\\displaystyle\\sum_{\\begin{array}{c} s'\\in\\mathcal{S} \\\\ r\\in\\mathcal{R}\\end{array}} p(s',r|s,a)[r + \\gamma v_{\\pi}(s')]$$\n\n* If we substitute (1) into (2), we can form the **Bellman Equation for action values** that relates any action value to any action value one step away:\n\n$$q_{\\pi}(s,a) = \\displaystyle\\sum_{\\begin{array}{c} s'\\in\\mathcal{S} \\\\ r\\in\\mathcal{R}\\end{array}} p(s',r|s,a)\\left[r + \\gamma \\displaystyle\\sum_{a'\\in\\mathcal{A}(s')} \\pi(a'|s')q_{\\pi}(s',a')\\right] $$\n\n::: {.callout-tip}\n## Bellman's contribution\n\nFor a Markov decision process (MDP) where we know the distribution of possible states, the Bellman equations above all us to compute any state value or any action value from all of the other information.\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Recursion\n\nThe derivation of equation (2) above involves how the returns\n\n$$G_{t} = R_{t+1} + \\gamma G_{t+1}$$\nThen, the expected values are\n\n$$\\begin{array}{rcl}\n\\text{E}[G_{t}|s,a] & = & \\text{E}[R_{t+1} + \\gamma G_{t+1}|s,a] \\\\\n~ & = & \\text{E}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1})|s,a] \\\\\n\\end{array}$$\n\nOne cannot say that distributions $G_{t+1} = v_{\\pi}(S_{t+1})$, but we are allowed to equate the expected values!\n:::\n\n\n# Policy Improvement\n\n## Definition\n\nIf we have the state value function $v_{\\pi}(s)$, and assuming a deterministic policy (i.e. $a = \\pi(s)$), an optimal policy is defined as\n\n$$\\pi_{*}(s) = \\text{argmax}_{a} q_{*}(s,a)$$\nWe define a new policy $\\pi'$ as\n\n$$\\pi'(s) = \\text{argmax}_{a} q_{\\pi}(s,a)$$\n\nand due to the Policy Improvement Theorem, we are assured that\n\n$$v_{\\pi'}(s) \\geq v_{\\pi}(s) \\quad \\forall s \\in \\mathcal{S}$$\n\n## GPI\n\nFor **Generalized Policy Iteration**, we hope to converge to the optimal policy $\\pi_{*}$ and optimal state value function $v_{*}$.\n\n\"Almost all reinforcement learning methods are well described as GPI\" --- Simon and Barto\n\n\n# Gym: Frozen Lake\n\n::::: {.panel-tabset}\n\n## Scene\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![Frozen Lake](frozen_lake_start.png)\n\n* image credit: [Alexandar Haber](https://aleksandarhaber.com/policy-iteration-algorithm-in-python-and-tests-with-frozen-lake-openai-gym-environment-reinforcement-learning-tutorial/)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n* start at square \"S\"\n* can move up, down, left, or right from *frozen* squares \"F\"\n* trajectory ends at holes (\"H\") with negative reward\n* trajectory ends at finish with positive reward\n\nIterate until policy converges.\n:::\n\n::::\n\n## Search\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![Frozen Lake](frozen_lake.gif)\n\n* image credit: [Gym Library](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![policy improvement](Q_learning_FrozenLake_33_0.png)\n\n* image credit: [Gertjan Verhoeven](https://gsverhoeven.github.io/post/frozenlake-qlearning-convergence/)\n:::\n\n::::\n\n## Findings\n\n![optimal policy](output_value-iter_b30603_3_0.png)\n\n* image credit: [Dive into Deep Learning](https://d2l.ai/chapter_reinforcement-learning/value-iter.html)\n\n:::::\n\n# Quo Vadimus?\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n* due this Friday (April 11):\n\n    - Precept 9\n    - Data Glimpse\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n* semester projects will be due May 10\n:::\n\n::::\n\n\n# Footnotes\n\n::: {.callout-note collapse=\"true\"}\n\n## (optional) Additional Resources and References\n\n* [Reinforcement Learning](https://d2l.ai/chapter_reinforcement-learning/index.html) from the *Dive into Deep Learning* book\n* Reinforcement Learning with Neural Networks by Josh Starmer\n\n    * [essentials](https://www.youtube.com/watch?v=xyfSvzZuk5Y)\n    * [math details](https://www.youtube.com/watch?v=DVGmsnxB2UQ)\n\n* [Simple Reinforcement Learning](https://medium.com/@MachineLearningYearning/simple-reinforcement-learning-in-python-2340c4df8c04) by Machine Yearner\n* [Train an AI Agent to play Mountain Car with RL](https://isaac-the-man.dev/posts/reinforcement-learning-mountain-car/) by Yu-Kai \"Steven\" Wang\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.2    fastmap_1.2.0     cli_3.6.3        \n [5] tools_4.4.2       htmltools_0.5.8.1 rstudioapi_0.17.1 yaml_2.3.10      \n [9] rmarkdown_2.29    knitr_1.49        jsonlite_1.8.9    xfun_0.50        \n[13] digest_0.6.37     rlang_1.1.5       evaluate_1.0.3   \n```\n\n\n:::\n:::\n\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}