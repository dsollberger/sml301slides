{
  "hash": "e0af24c3c7094ffa52fcfb25e304f634",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"23: Debate\"\nauthor: \"Derek Sollberger\"\ndate: \"2025-12-01\"\nformat:\n  html:\n    toc: true\n---\n\n\n\n\n# 23: Teaching Controversial Topics\n\n## Learning Objectives\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- view results of prompt engineering\n- discuss governmental regulation in AI\n- participate in agile ethics\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n![AI in Ethics](ai_ethics_flyer.png)\n\n* image source: [Princeton Dialogue on AI and Ethics](https://aiethics.princeton.edu/events/)\n:::\n\n::::\n\n# Scene\n\nI am in a teaching group called `Teaching Controversial Topics`, and with them I want to draft a case study from the realm of artificial intelligence. For this teaching group, I am hoping to produce\n\n* 20-minute classroom activity for the SML 301 students\n* a blog post for me\n\nDr Sarah-Jane Leslie suggested that I look into either \"privacy/copyright issues around training data for LLMs\" or \"the role of government regulation in AI development\".  Especially drawn to the latter, idea I have then been picturing a classroom debate about government regulation in AI.  I think that this would appeal to my audience of mostly engineering majors. Over the summer in the AI4ALL summer camp, Dr Steven Kelts led a great ethics debate with the students, and I am hoping to emulate that by partitioning the SML 301 students into 4 roles such as\n\n1. congressional staffers\n2. business lobbyists\n3. data privacy NGO\n4. legal advisors\n\n::: {.callout-warning}\n## Disclosure on AI Use\n\nDespite thinking about this project for the past 4 months (inspired by the BIOME25 conference), I admit that I am assembling these materials merely a couple of weeks before my proposed launch date.  I hardly have experience in debate, much less in setting up one.  Thus, one can imagine that I too would want AI assistance to complete this task.\n\nMy students and I have recently covered materials about AI---particularly prompt engineering and retrieval augmented generation---so I am indeed inclined to try to justify AI usage here as careful examination of LLM results.\n:::\n\n\n# Prompt Engineering\n\nMy students, perhaps as a reflection of the learning environment that I create, are a quiet group of people.  In fact, I set up an anonymous Slido poll each week to receive questions rather than having the students raise their hands.  With that in mind, asking them to vocalize a debate will be quite a shift in expectations.  \n\nTo mitigate that aberrant request, I want to give each student a one-page **character sheet** to on-board them into the roles (stated above). I hope that each character sheet provides several possible talking points to encourage and guide the debate.\n\nEven after I read some materials (below in the \"RAG\" section), I find that I myself am not a writer of debates.  Thus, I shall employ **prompt engineering** to create the bullet-point lists of talking points.  The idea of debate roles lends itself well for the prompt engineering tool.\n\nFollowing materials found in Chapter 6 of [Hands-On Large Language Models](https://github.com/HandsOnLLM/Hands-On-Large-Language-Models) (by Jay Alammar and Maarten Grootendorst), we can construct an LLM prompt with parts like ...\n\n::: {.callout-tip}\n## Persona\n\n[We will provide this below]\n:::\n\n::: {.callout-note}\n## Instruction\n\nProvide possible debate points and *support a stance*.\n:::\n\n::: {.callout-tip}\n## Context\n\nThe list should *express several key points* that can help a debate team and quickly state important information for the scene.\n:::\n\n::: {.callout-note}\n## Format\n\nCreate a bullet-point overview that summarizes the *debate stance* and also provide a concise paragraph that initiates the debate.\n:::\n\n::: {.callout-tip}\n## Audience\n\nWe are at a USA government meeting with a subcommitee of Congress.  We are hoping to inform our Congress members about the issues about *governmental regulation in artificial intelligence*.\n:::\n\n::: {.callout-note}\n## Tone\n\nThe tone should be informative and professional.\n:::\n\n::: {.callout-tip}\n## Data\n\n[https://en.wikipedia.org/wiki/Regulation_of_AI_in_the_United_States](https://en.wikipedia.org/wiki/Regulation_of_AI_in_the_United_States)\n:::\n\nContinuing with the materials found in Chapter 6 of [Hands-On Large Language Models](https://github.com/HandsOnLLM/Hands-On-Large-Language-Models), I used the [Phi-3-mini-4k-instruct](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) LLM (made by Microsoft) and its tokenizer to process these prompts.\n\n\n# Roles (draft)\n\nWe are now ready to see the participants! Here I apply the prompt engineering (structured above) with each of the 4 personas. We hope to have the AI create the rest of the character sheets\n\n::::: {.panel-tabset}\n\n## staffer\n\n::: {.callout-note}\n### persona\n\nYou are a Congressional staffer and college graduate. You have been asked to provide a historical perspective of the issue.\n:::\n\n### character sheet\n\n The United States has been actively engaging in the regulation of artificial intelligence (AI) to ensure the technology's development aligns with national interests, public safety, and ethical standards. The regulatory landscape for AI in the U.S. has evolved through various executive orders, legislative proposals, and state-level initiatives, reflecting a complex interplay between innovation, security, and societal impact.\n\n**Executive Orders and Federal Actions:**\n\n- **2016-2019:** The National Science and Technology Council's report on \"Preparing for the Future of Artificial Intelligence\" laid the groundwork for a balanced approach to AI development, emphasizing the importance of ethical considerations and the potential risks associated with AI technologies.\n- **2019:** The White House Office of Science and Technology Policy (OSTP) issued a draft guidance on the regulation of AI applications, proposing a framework that includes ethical principles and risk assessment methodologies to ensure AI technologies are developed responsibly.\n- **2020:** The National Security Commission on Artificial Intelligence (NSCAI) was established to provide strategic recommendations for AI development in national security contexts, highlighting the importance of AI in maintaining U.S. technological leadership.\n- **2021:** The Biden administration's Executive Order on \"Preventing Weaponization of AI\" aimed to mitigate the risks of AI being used for harmful purposes, emphasizing the need for international cooperation and responsible AI development.\n- **2022:** The National Artificial Intelligence Initiative Office (NAIIO) was created to coordinate federal efforts in AI research and development, focusing on advancing AI technologies that benefit the public and national security.\n- **2023:** The Office of Science and Technology Policy (OSTP) released the \"AI Risk Management Framework (RMF)\" to guide federal agencies in assessing and managing the risks associated with AI applications.\n- **2023:** The National Artificial Intelligence Research and Development Strategic Plan outlined a comprehensive approach to A\n\n## lobbyist\n\n::: {.callout-note}\n### persona\n\nYou are a business lobbyist who seeks to discourage further government regulation in artificial intelligence.  You seek to build upon anti-regulation sentiment.\n:::\n\n### character sheet\n\n* The United States government has been actively engaged in shaping the future of artificial intelligence (AI) through a series of legislative and regulatory efforts aimed at fostering innovation while addressing the potential risks associated with AI technologies. These efforts reflect a balanced approach to AI governance, recognizing the transformative potential of AI across various sectors while also acknowledging the need for safeguards to protect public interests and ensure ethical AI development.\n\n* In the early stages of AI regulation, the National Science and Technology Council (NSTC) in 2018 laid the groundwork for a national AI strategy, emphasizing the importance of research and development in AI technologies. This was followed by the National Artificial Intelligence Research and Development Strategic Plan in 2019, which outlined a vision for the U.S. to lead in AI innovation and application. The Department of Defense (DoD) also played a pivotal role in AI regulation by establishing the National Security Commission on Artificial Intelligence (NSCAI) in 2019, tasked with ensuring the U.S. maintains a competitive edge in AI for national security purposes.\n\n* The Food and Drug Administration (FDA) has been instrumental in addressing the intersection of AI and healthcare, particularly in the regulation of AI applications in medical devices. This includes the development of frameworks to ensure the safety and efficacy of AI-driven diagnostic tools and treatments.\n\n* The Office of Science and Technology Policy (OSTP) has been at the forefront of AI governance, issuing guidelines for the responsible development and deployment of AI technologies. These guidelines emphasize the importance of transparency, accountability, and ethical considerations in AI systems.\n\n* In response to the evolving AI landscape, the National Artificial Intelligence Initiative Office (NAIIO) was established to coordinate federal AI efforts, focusing on research, development, and the responsible use of AI technologies.\n\n* The National AI Initiative (NAII) and the National Artificial Intelligence Research and Development Strategic Plan have been key in setting strategic priorities for AI research and\n\n## NGO\n\n::: {.callout-note}\n### persona\n\nYou are a volunteer at a non-governmental organization (NGO) that raises awareness about data privacy and the corpuses that are used in the training of large language models.\n:::\n\n### character sheet\n\n1. **Regulatory Framework**: The U.S. government's approach to AI regulation has been a mix of federal and state-level initiatives. The National Artificial Intelligence Initiative Office (NAIIO) and the National Artificial Intelligence Research Resource (NAIRR) are federal efforts aimed at advancing AI research and development. However, the Trump administration's Executive Order #14179 and the proposed \"TAKE IT DOWN Act\" reflect a deregulatory stance, emphasizing innovation over risk mitigation. This contrasts with the Biden administration's Executive Order, which seeks to establish a comprehensive AI regulatory framework. The debate should consider the balance between fostering innovation and ensuring public safety and ethical standards.\n\n2. **AI in Healthcare**: The Food and Drug Administration (FDA) has a role in regulating AI applications in healthcare, such as the approval process for AI-driven medical devices. The FDA's involvement ensures that AI applications in healthcare meet safety and efficacy standards, protecting patients from potential harm. This regulatory oversight is crucial in maintaining public trust in AI technologies, especially in sensitive areas like healthcare.\n\n3. **AI and Privacy**: The Biden administration's AI Bill of Rights emphasizes the importance of privacy and data security in AI applications. This includes the requirement for AI systems to be transparent about their use of personal data and the implementation of watermarking to inform users about AI-generated content. These measures are essential in protecting individuals' privacy rights and ensuring that AI technologies are used ethically and responsibly.\n\n4. **State and Local Government Interventions**: The New York City Bias Audit Law and California's Safe and Secure Innovation for Frontier Artificial Intelligence Models Act represent state and local efforts to address AI-related risks and biases. These initiatives highlight the importance of local governance in complementing federal efforts to regulate AI. The debate should consider the effectiveness of these local measures in mitigating AI risks and promoting ethical AI development\n\n## legal\n\n::: {.callout-note}\n### persona\n\nYou are a paralegal at a law firm that has been tasked with explaining the legal subtleties in the current American laws about artificial intelligence.\n:::\n\n### character sheet\n\n**Pro-Regulation Stance:**\n\n1. **Safety and Ethical Concerns:** The rapid advancement of AI technologies, particularly in areas like facial recognition and deepfakes, raises significant safety and ethical concerns. The potential for AI to perpetuate and amplify existing biases, discriminate against marginalized groups, and infringe on privacy rights necessitates a regulatory framework to ensure these technologies are developed and deployed responsibly.\n\n2. **Need for Oversight:** The National Security Commission on Artificial Intelligence's report highlights the importance of establishing a federal agency to oversee AI development, ensuring it aligns with national security and ethical standards. This oversight is crucial to prevent the misuse of AI in ways that could harm individuals or national security.\n\n3. **Public Safety and Trust:** The Executive Order on AI emphasizes the need for AI to advance equity and civil rights, addressing concerns about job quality and labor disruptions. By mandating transparency and accountability in AI systems, the government can foster public trust and ensure AI technologies contribute positively to society.\n\n4. **International Leadership:** The Biden administration's Executive Order and the proposed AI Action Plan demonstrate a commitment to securing U.S. global AI dominance through responsible innovation. This approach not only positions the U.S. as a leader in AI but also ensures that AI advancements are aligned with democratic values and human rights.\n\n**Anti-Regulation Stance:**\n\n1. **Innovation and Competitiveness:** Critics argue that stringent regulations could stifle innovation and hinder the U.S.'s competitive edge in the global AI market. The tech industry often views regulations as barriers to growth, potentially slowing down the development of beneficial AI applications.\n\n2. **State and Local Initiatives:** The diverse approaches taken by states, such as New York's Bias Audit Law and Utah's AI Policy Act, demonstrate a\n\n:::::\n\n\n# RAG\n\nSo far, I have found neat resources that include\n\n* [a brief in the Princeton Legal Journal](https://legaljournal.princeton.edu/regulating-artificial-intelligence-antitrust-and-anti-discrimination-policy/) (Feb 2025)\n* [Congress report](https://www.congress.gov/crs-product/R48555) (June 2025)\n* [article](https://www.whitecase.com/insight-our-thinking/ai-watch-global-regulatory-tracker-united-states) by \"Energy Legal Services Provider\" White & Case (Sept 2025)\n* [state legislation tracker](https://www.ncsl.org/technology-and-communication/artificial-intelligence-2025-legislation) by the NCSL (National Conference of State Legislatures, updated July 2025)\n* [journal report](https://cyber.fsi.stanford.edu/content/regulating-under-uncertainty-governance-options-generative-ai) from the Stanford Cyber Policy Center (updated Sept 2025)\n\n::: {.callout-note collapse=\"true\"}\n## Revisions\n\nTake on a **retrieval-augmented generation** (RAG) mindset by either having each persona ask for more debate points from the specialized literature (i.e. more specialized than just the Wikipedia article) or simply augment the data first and then perform the prompt engineering again.\n:::\n\n# Roles (for activity)\n\nUpon consultation with Dr Steven Kelts, I took his advice and clarified the objective for each role.  The AI LLM created the rest of the character sheets via prompt engineering (outlined above) along with more data (i.e. more text articles) and the objectives\n\n::::: {.panel-tabset}\n\n## staffer\n\n::: {.callout-note}\n### persona\n\nYou are a Congressional staffer and college graduate. You have been asked to provide a historical perspective of the issue.\n:::\n\n::: {.callout-tip}\n### objective\n\nYour goal is to maintain civility in the discussion and hope to keep the discussion on topic to expedite the process (in hopes of proceeding to drafting legislation).\n:::\n\n### character sheet\n\n#### State-Level Legislation\n- **All 50 States, Puerto Rico, the Virgin Islands, and Washington, D.C.** have introduced AI-related legislation in the 2025 legislative session.\n- **Thirty-eight states** have adopted or enacted around 100 measures related to AI.\n\n#### Examples of AI-Related Legislation\n1. **Arkansas**:\n   - Clarifies the ownership of AI-generated content, including content generated by AI models trained with data or input from employers.\n   - Specifies that generated content should not infringe on existing copyright or intellectual property rights.\n\n2. **Montana**:\n   - Introduces the \"Right to Compute\" law, focusing on critical infrastructure controlled by AI systems.\n   - Requires the development of a risk management policy based on specified standards, including the AI risk management framework from the National Institute of Standards and Technology (NIST).\n   - Protects the government's ability to restrict the use of computational resources for lawful purposes unless necessary for a compelling government interest.\n\n3. **New Jersey**:\n   - Adopts a resolution urging generative AI companies to commit to protecting employee whistleblowers.\n\n4. **New York**:\n   - Requires state agencies to publish detailed information about their automated decision-making tools on public websites.\n   - Amends the civil service law to strengthen worker protections, ensuring AI systems do not affect existing rights under collective bargaining agreements or result in displacement or loss of positions.\n   \n\n## lobbyist\n\n::: {.callout-note}\n### persona\n\nYou are a business lobbyist who seeks to discourage further government regulation in artificial intelligence.  \n:::\n\n::: {.callout-tip}\n### objective\n\nYou seek to build upon anti-regulation sentiment and be persuasive to a wider media audience (beyond the Congressional staff)\n:::\n\n### character sheet\n\n#### California AI Acts as a Reference\n- **Senate Bill 942 (California AI Transparency Act):**\n  - Provides for penalties of $5,000 per violation per day.\n  - Enforceable through civil action by the California Attorney General, city attorneys, or county counsel.\n\n- **Assembly Bill 3030 (Health Care Services: AI Act):**\n  - Enforceable by the Medical Board of California and Osteopathic Medical Board of California.\n  - Non-compliance can lead to civil penalties, suspension or revocation of a medical license, and administrative fines.\n\n- **Assembly Bill 2655 (Defending Democracy from Deepfake Deception Act):**\n  - Enforced by the California Attorney General, district attorneys, or city attorneys.\n  - Allows for injunctive relief to compel the removal of materially deceptive content.\n\n#### Enforcement Mechanisms\n- **Civil Penalties:**\n  - Penalties for violations, as seen in SB 942, can be a significant deterrent.\n\n- **Suspension or Revocation of Licenses:**\n  - For healthcare-related AI applications (AB 3030), this mechanism ensures that non-compliant entities cannot operate, protecting public health.\n\n- **Administrative Fines:**\n  - These fines can serve as both a punitive measure and a financial deterrent against non-compliance.\n\n#### Proactive Regulatory Options\n- **Risk Management Policies:**\n  - Mandating the development of risk management policies for critical infrastructure controlled by AI systems, as seen in Montana's \"Right to Compute\" law.\n\n- **Transparency and Disclosure:**\n  - Requiring state agencies to publish detailed information about their automated decision-making tools, enhancing accountability and public trust.\n\n- **Protecting Worker Rights:**\n  - Amending laws to ensure AI systems do not affect existing workers' rights, preserving employment and labor standards.\n\n- **Focusing on Specific AI Technologies:**\n  - Creating targeted regulations for specific AI technologies, such as facial recognition or autonomous vehicles, addressing unique challenges and risks associated with each.\n\n#### Recommendations for Colorado\n- **Adopt a Multifaceted Approach:**\n  - Colorado should consider a combination of civil penalties, licensing actions, and administrative fines to ensure compliance and deter violations.\n\n- **Enhance Transparency:**\n  - Implement mandatory disclosures for AI systems used by public and private entities, focusing on the impact on individuals and communities.\n\n## NGO\n\n::: {.callout-note}\n### persona\n\nYou are a volunteer at a non-governmental organization (NGO) that raises awareness about data privacy and the corpuses that are used in the training of large language models.\n:::\n\n::: {.callout-tip}\n### objective\n\nYou seek to educate the wider public about controversies about artificial intelligence and inspire sympathy for those affected.\n:::\n\n### character sheet\n\n#### 1. Precautionary Principle\nImplementing regulations based on the precautionary principle might be a starting point. This principle suggests that in the absence of scientific consensus, the burden of proof falls on those advocating for an action that may harm the public or the environment. In the context of AI, this could mean strict regulations on the deployment of generative AI until its risks are fully understood.\n\n#### 2. Risk-Based Regulation\nAdopting a risk-based regulatory approach would involve identifying and assessing the risks associated with generative AI and applying regulations proportionally. This could include specific licensing requirements for developers, mandatory impact assessments, and the establishment of AI ethics committees.\n\n#### 3. Self-Regulation and Industry Standards\nEncouraging or mandating self-regulation within the AI industry could be another option. This would involve the development of industry standards and best practices for the responsible use of generative AI. While self-regulation can be effective, it often requires a strong culture of ethics and accountability within the industry.\n\n#### 4. International Cooperation\nGiven the global nature of AI development and its potential impacts, international cooperation and harmonization of regulations could be crucial. This might involve global standards for transparency, accountability, and ethical use of generative AI, facilitated by international organizations or agreements.\n\n#### 5. Adaptive Regulation\nAdopting an adaptive regulatory approach that evolves with the technology could allow for more flexibility and responsiveness to new developments in generative AI. This could involve regular reviews of regulations and the establishment of mechanisms for rapid response to emerging risks.\n\n#### 6. Public Engagement and Transparency\nEngaging the public in discussions about the development and regulation of generative AI can help ensure that the technology aligns with societal values and expectations. Transparency about the capabilities and risks of generative AI, as well as the regulatory processes, can build public trust and facilitate informed decision-making.\n\n### Challenges and Considerations\n\n- **Rapid Technological Advancement**: The pace of AI development, particularly in generative AI, poses a significant challenge to regulators. Regulations may quickly become outdated if they are not adaptable.\n- **Global Impact**: Generative AI has the potential to impact societies worldwide, making international cooperation essential. However, differences in regulatory philosophies and priorities can complicate this process.\n- **Balancing Innovation and Protection**: Regulators must balance the need to protect individuals and society from potential harms with the desire to foster innovation and economic growth.\n- **Ethical and Societal Implications**: Generative AI raises complex ethical and societal questions, including issues related to privacy, consent\n\n\n## legal\n\n::: {.callout-note}\n### persona\n\nYou are a paralegal at a law firm that has been tasked with explaining the legal subtleties in the current American laws about artificial intelligence.\n:::\n\n::: {.callout-tip}\n### objective\n\nYou seek to maintain consistency in legislation based on precedent while acknowledging state versus federal decisions.\n:::\n\n### character sheet\n\n#### California AI Acts as a Reference\n- **Senate Bill 942 (California AI Transparency Act):**\n  - Provides for penalties of $5,000 per violation per day.\n  - Enforceable through civil action by the California Attorney General, city attorneys, or county counsel.\n\n- **Assembly Bill 3030 (Health Care Services: AI Act):**\n  - Enforceable by the Medical Board of California and Osteopathic Medical Board of California.\n  - Non-compliance can lead to civil penalties, suspension or revocation of a medical license, and administrative fines.\n\n- **Assembly Bill 2655 (Defending Democracy from Deepfake Deception Act):**\n  - Enforced by the California Attorney General, district attorneys, or city attorneys.\n  - Allows for injunctive relief to compel the removal of materially deceptive content.\n\n#### 1. Precautionary Principle\nImplementing regulations based on the precautionary principle might be a starting point. This principle suggests that in the absence of scientific consensus, the burden of proof falls on those advocating for an action that may harm the public or the environment. In the context of AI, this could mean strict regulations on the deployment of generative AI until its risks are fully understood.\n\n#### 2. Risk-Based Regulation\nAdopting a risk-based regulatory approach would involve identifying and assessing the risks associated with generative AI and applying regulations proportionally. This could include specific licensing requirements for developers, mandatory impact assessments, and the establishment of AI ethics committees.\n\n#### 3. Self-Regulation and Industry Standards\nEncouraging or mandating self-regulation within the AI industry could be another option. This would involve the development of industry standards and best practices for the responsible use of generative AI. While self-regulation can be effective, it often requires a strong culture of ethics and accountability within the industry.\n\n#### 4. International Cooperation\nGiven the global nature of AI development and its potential impacts, international cooperation and harmonization of regulations could be crucial. This might involve global standards for transparency, accountability, and ethical use of generative AI, facilitated by international organizations or agreements.\n\n#### 5. Adaptive Regulation\nAdopting an adaptive regulatory approach that evolves with the technology could allow for more flexibility and responsiveness to new developments in generative AI. This could involve regular reviews of regulations and the establishment of mechanisms for rapid response to emerging risks.\n\n#### 6. Public Engagement and Transparency\nEngaging the public in discussions about the development and regulation of generative AI can help ensure that the technology aligns with societal values and expectations. Transparency about the capabilities and risks of generative AI, as well as the regulatory processes, can build public trust and facilitate informed decision-making.\n\n### Challenges and Considerations\n\n- **Rapid Technological Advancement**: The pace of AI development, particularly in generative AI, poses a significant challenge to regulators. Regulations may quickly become outdated if they are not adaptable.\n- **Global Impact**: Generative AI has the potential to impact societies worldwide, making international cooperation essential. However, differences in regulatory philosophies and priorities can complicate this process.\n- **Balancing Innovation and Protection**: Regulators must balance the need to protect individuals and society from potential harms with the desire to foster innovation and economic growth.\n- **Ethical and Societal Implications**: Generative AI raises complex ethical and societal questions, including issues related to privacy, consent\n\n\n:::::\n\n\n# Footnotes\n\n::: {.callout-note collapse=\"true\"}\n\n## (optional) Additional Resources and References\n\n\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.5.1 (2025-06-13 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n  LAPACK version 3.12.1\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.5.1    fastmap_1.2.0     cli_3.6.5        \n [5] tools_4.5.1       htmltools_0.5.8.1 rstudioapi_0.17.1 yaml_2.3.10      \n [9] rmarkdown_2.29    knitr_1.50        jsonlite_2.0.0    xfun_0.52        \n[13] digest_0.6.37     rlang_1.1.6       evaluate_1.0.4   \n```\n\n\n:::\n:::\n\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}