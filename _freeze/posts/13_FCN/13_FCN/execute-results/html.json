{
  "hash": "9e9365568be3738e0cdb4783be7d04dc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"13: Fully Connected Networks\"\nauthor: \"Derek Sollberger\"\ndate: \"2025-03-17\"\nformat:\n  html:\n    toc: true\n---\n\n::: {.cell}\n\n:::\n\n\n\n# Session 13: Fully Connected Networks\n\n## Learning objectives:\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- Explore elementary calculations in neural networks\n- Introduce PyTorch code\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n![fully-connected network](FCN_4_2.png)\n:::\n\n::::\n\n# Preprocessing: Min-Max Normalization\n\n## motivation\n\nFor 32-bit float number representation\n\n![floating-point representation](Float_example.svg.png)\n\n* range: $(1.1755 \\times 10^{-38}, 1.7014 \\times 10^{38})$\n\nAnticipating neural networks and performing many calculations, we want to avoid overflow or underflow errors.\n\n::: {.callout-tip}\n\n## Preprocessing Numbers\n\nTo make the numerical inputs and outputs more manageable, we will employ\n\n* min-max normalization\n* softmax\n\n:::\n\n::::: {.panel-tabset}\n\n## scene\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](13_FCN_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n## formula\n\nWe want numerical inputs to be between 0 and 1, so we can employ **min-max normalization**:\n\n$$\\text{scaled}(x) = \\frac{x - \\text{min}}{\\text{max} - \\text{min}}$$\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## rescaled\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](13_FCN_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n## bases\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](13_FCN_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n:::::\n\n# Forward Propogation\n\n## Trained Model\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\n\n```{mermaid}\nflowchart LR\n\ninput_1[flipper length]\ninput_2[body mass]\noutput_1[Adelie]\noutput_2[Chinstrap]\noutput_3[Gentoo]\n\ninput_1 -- -1.16A + 0.38 --> output_1\ninput_1 -- 0.24A - 0.38 --> output_2\ninput_1 -- 1.77A + 0.16 --> output_3\ninput_2 -- -0.05B + 0.38 --> output_1\ninput_2 -- 0.18B - 0.38 --> output_2\ninput_2 -- 1.07B + 0.16 --> output_3\n```\n\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n* input layer size: 2\n* output layer size: 3\n* **fully-connected network**: every output is connected to every input by an edge\n\n    * here: no hidden layer\n    * later: LSTM, transformers, encoder-decorders\n:::\n\n::::\n\n::: {.callout-tip}\n## Where did the coefficients come from?\n\nWe will train the neural network and compute the weights and bias values soon!  For now, let us see how the forward calculations work in this trained model.\n:::\n\n::: {.callout-warning collapse=\"true\"}\n\n## These will be poor results\n\nThe examples in today's session will have poor results (i.e. accuracy values close to 50 percent).  This is because of\n\n* small data sets\n* models run for a limited number of epochs (iterations)\n\n:::\n\n## Parameters\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\n\n```{mermaid}\nflowchart LR\n\ninput_1[flipper length]\ninput_2[body mass]\noutput_1[Adelie]\noutput_2[Chinstrap]\noutput_3[Gentoo]\n\ninput_1 -- -1.16A + 0.38 --> output_1\ninput_1 -- 0.24A - 0.38 --> output_2\ninput_1 -- 1.77A + 0.16 --> output_3\ninput_2 -- -0.05B + 0.38 --> output_1\ninput_2 -- 0.18B - 0.38 --> output_2\ninput_2 -- 1.07B + 0.16 --> output_3\n```\n\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\nThis model has 9 parameters:\n\n* weights\n\n$$\\begin{array}{rcr}\n  w_{1,1} & \\approx & -1.16 \\\\\n  w_{1,2} & \\approx & 0.24 \\\\\n  w_{1,3} & \\approx & 1.77 \\\\\n  w_{2,1} & \\approx & -0.05 \\\\\n  w_{2,2} & \\approx & 0.18 \\\\\n  w_{2,3} & \\approx & 1.07 \\\\\n\\end{array}$$\n\n* bias\n\n$$\\begin{array}{rcr}\n  b_{1} & \\approx & 0.76 \\\\\n  b_{2} & \\approx & -0.76 \\\\\n  b_{3} & \\approx & 0.32 \\\\\n\\end{array}$$\n\n:::\n\n::::\n\n::: {.callout-tip}\n## Split Bias?\n\nEditor's note: for the sake of simplifying the diagram, the bias values were split in half (two input variables).  In practice, the bias values are added after the weight multiplications.  This will be more apparent in the matrix computations later.  Furthermore, the values going into the output nodes are also added together.\n:::\n\n## Obvious Example\n\nWhat will our model predict for a Gentoo penguin whose measurements (after min-max normalization) include a flipper length of 0.75 and a body mass of 0.75?\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\n\n```{mermaid}\nflowchart LR\n\ninput_1[0.75]\ninput_2[0.75]\noutput_1[-0.1475]\noutput_2[-0.4450]\noutput_3[2.4500]\n\ninput_1 -- -1.16*0.75 + 0.38 --> output_1\ninput_1 -- 0.24*0.75 - 0.38 --> output_2\ninput_1 -- 1.77*0.75 + 0.16 --> output_3\ninput_2 -- -0.05*0.75 + 0.38 --> output_1\ninput_2 -- 0.18*0.75 - 0.38 --> output_2\ninput_2 -- 1.07*0.75 + 0.16 --> output_3\n```\n\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\nSince the output value is the largest for the Gentoo label, we predict that such a penguin is of the Gentoo species.\n:::\n\n::::\n\n\n## Tough Classification\n\nWhat will our model predict for a penguin whose measurements (after min-max normalization) include a flipper length of 0.301 and a body mass of 0.301?\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\n\n```{mermaid}\nflowchart LR\n\ninput_1[0.301]\ninput_2[0.301]\noutput_1[0.37579]\noutput_2[-0.63358]\noutput_3[1.17484]\n\ninput_1 -- -1.16*0.301 + 0.38 --> output_1\ninput_1 -- 0.24*0.301 - 0.38 --> output_2\ninput_1 -- 1.77*0.301 + 0.16 --> output_3\ninput_2 -- -0.05*0.301 + 0.38 --> output_1\ninput_2 -- 0.18*0.301 - 0.38 --> output_2\ninput_2 -- 1.07*0.301 + 0.16 --> output_3\n```\n\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\nSince the output value is the largest for the Gentoo label, we predict that such a penguin is of the Gentoo species.\n\nHowever, the output values should have favored the Adelie and Chinstrap penguins here.\n:::\n\n::::\n\n\n# Activation Functions\n\n::::: {.panel-tabset}\n\n## motivation\n\n* Can you name a function whose domain is all real numbers but its range is restricted to $(0,1)$?\n\n* Can you name a function whose domain is all real numbers but its range is restricted to $(-1,1)$?\n\n## sigmoid\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n![sigmoid function](activation_sigmoid.png)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"60%\"}\n\n$$f(x) = \\frac{1}{1 + e^{-x}}$$\n\n* domain: $(-\\infty, \\infty)$\n* range: $(0,1)$\n\nderivative:\n\n$$f'(x) = f(x)[1 - f(x)]$$\n\n* 2 function calls\n* one multiplication, one subtraction\n\n:::\n\n::::\n\n## tanh\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n![hyperbolic tangent](activation_tanh.png)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"60%\"}\n\n$$f(x) = \\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\n\n* domain: $(-\\infty, \\infty)$\n* range: $(-1,1)$\n\nderivative:\n\n$$f'(x) = 1 - \\tanh^{2}(x)$$\n\n* 2 function calls\n* one multiplication, one subtraction\n\n:::\n\n::::\n\n## ReLU\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n![rectified linear unit](activation_ReLU.png)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"60%\"}\n\n$$f(x) = \\begin{cases}x, & x \\geq 0 \\\\ 0, & x < 0\\end{cases}$$\n\n* domain: $(-\\infty, \\infty)$\n* range: $(0,\\infty)$\n\nderivative:\n\n$$f'(x) = \\begin{cases}1, & x > 0 \\\\ 0, & x < 0\\end{cases}$$\n\n:::\n\n::::\n\n\n:::::\n\n\n# Bias\n\n::::: {.panel-tabset}\n\n## scene\n\nSuppose that the high temperature observations at Princeton (in degrees Fahrenheit) are usually between 16 and 90 degrees and that, relatively speaking, local people consider it to be a \"warm day\" if the temperature is at least 60 degrees (\"cold day\" otherwise):\n\n$$f(H) = \\begin{cases}\n  \\text{warm day}, & H \\geq 60 \\\\\n  \\text{cold day}, & H < 60 \\\\\n\\end{cases}$$\n\n## bias\n\nWe can shift the values by a **bias**.  For example, if we use a bias value of $b = 60$, then\n\n$$f(H-60) = \\begin{cases}\n  \\text{warm day}, & H-60 \\geq 0 \\\\\n  \\text{cold day}, & H-60 < 0 \\\\\n\\end{cases}$$\n\nputs the decision boundary at a value of zero in the new units.\n\n## ReLU\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n![rectified linear unit](activation_ReLU.png)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"60%\"}\n\n$$f(x) = \\begin{cases}x, & x \\geq 0 \\\\ 0, & x < 0\\end{cases}$$\n\n* domain: $(-\\infty, \\infty)$\n* range: $(0,\\infty)$\n\nderivative:\n\n$$f'(x) = \\begin{cases}1, & x > 0 \\\\ 0, & x < 0\\end{cases}$$\n\n:::\n\n::::\n\n## float\n\n![floating-point representation](Float_example.svg.png)\n\nA TPU (tensor processing unit) would only have to check one bit!\n\n:::::\n\n\n# Post-Processing: Softmax\n\nAnother way to normalize a vector of numerical observations is to employ the **softmax**\n\n$$\\text{Softmax}(\\vec{x})_{i} = \\frac{e^{x_{i}}}{\\sum_{i=1}^{n}e^{x_{i}}}$$\n\n## Obvious Example\n\nWhat will our model predict for a penguin whose measurements (after min-max normalization) include a flipper length of 0.75 and a body mass of 0.75?\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\n\n```{mermaid}\nflowchart LR\n\ninput_1[0.75]\ninput_2[0.75]\noutput_1[-0.1475]\noutput_2[-0.4450]\noutput_3[2.4500]\n\ninput_1 -- -1.16*0.75 + 0.38 --> output_1\ninput_1 -- 0.24*0.75 - 0.38 --> output_2\ninput_1 -- 1.77*0.75 + 0.16 --> output_3\ninput_2 -- -0.05*0.75 + 0.38 --> output_1\ninput_2 -- 0.18*0.75 - 0.38 --> output_2\ninput_2 -- 1.07*0.75 + 0.16 --> output_3\n```\n\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n$\\frac{e^{-0.1475}}{e^{-0.1475} + e^{-0.4450} + e^{2.4500}} \\approx 0.0659$\n$\\frac{e^{-0.4450}}{e^{-0.1475} + e^{-0.4450} + e^{2.4500}} \\approx 0.0489$\n$\\frac{e^{2.4500}}{e^{-0.1475} + e^{-0.4450} + e^{2.4500}} \\approx 0.8851$\n:::\n\n::::\n\n## Tough Classification\n\nWhat will our model predict for a penguin whose measurements (after min-max normalization) include a flipper length of 0.301 and a body mass of 0.301?\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\n\n```{mermaid}\nflowchart LR\n\ninput_1[0.301]\ninput_2[0.301]\noutput_1[0.37579]\noutput_2[-0.63358]\noutput_3[1.17484]\n\ninput_1 -- -1.16*0.301 + 0.38 --> output_1\ninput_1 -- 0.24*0.301 - 0.38 --> output_2\ninput_1 -- 1.77*0.301 + 0.16 --> output_3\ninput_2 -- -0.05*0.301 + 0.38 --> output_1\ninput_2 -- 0.18*0.301 - 0.38 --> output_2\ninput_2 -- 1.07*0.301 + 0.16 --> output_3\n```\n\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n$$\\begin{array}{c|c|c}\n  \\text{signal} & \\text{exponent} & \\text{softmax} \\\\\n  \\hline\n  0.3758 & e^{0.3758} & 0.2787 \\\\\n  -0.6336 & e^{-0.6336} & 0.1016 \\\\\n  1.1748 & e^{1.1748} & 0.6197 \\\\\n\\end{array}$$\n:::\n\n::::\n\n::: {.callout-caution collapse=\"true\"}\n## Does the softmax output create a probability distribution?\n\nIt's debatable.\n\nWhile the values from a softmax computation are positive sum up to one (up to a rounding error), it is debatable whether or not the softmax result is a probability distribution.\n\n* Nodes earlier in the network (especially in neural networks) can be permuted.  While we might end up with the same distribution, the underlying weights might be quite different.\n* What do these proportions represent?  Later, if we use this softmax output as inputs for another *module*, then we can say something like \"The output signal is about 28 percent Adelie, 10 percent Chinstrap, and 62 percent Gentoo.\"\n* Are these proportions *unbiased estimators* of population proportions?\n\n:::\n\n## Derivative\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\nSoftmax function\n$$\\sigma({\\vec{x}})_{i} = \\frac{e^{x_{i}}}{\\sum_{i=1}^{n}e^{x_{i}}}$$\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\nDerivative\n$$\\frac{\\partial \\sigma_{k}}{\\partial x_{i}} = \\sigma_{k}(\\delta_{ik} - \\sigma_{i})$$\nwhere\n$$\\delta_{ik} = \\begin{cases} 1, & i = k \\\\ 0, & i \\neq k \\\\ \\end{cases}$$\n:::\n\n::::\n\n\n# Loss\n\nToward training and backprogation, for a classification task, we could use the **sum-of-squared residuals** (SSR) to calculate the **loss**. \n\n$$\\text{SSR} = \\sum_{i=1}^{n}(\\text{observed}_{i} - \\text{predicted}_{i})^{2}$$\n\n\n# Back Propogation\n\nHere, we will see how backpropogation works to update *one weight* in our network.\n\n## Initialization\n\nConsider a fully-connected network (FCN) where all of the weights have been initialized to the same uniform value of 0.2 and each bias has been initialized to be zero. What will our model predict for a Gentoo penguin whose measurements (after min-max normalization) include a flipper length of 0.75 and a body mass of 0.75?\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n\n```{mermaid}\nflowchart LR\n\ninput_1[0.75]\ninput_2[0.75]\noutput_1[0.3]\noutput_2[0.3]\noutput_3[0.3]\n\ninput_1 -- 0.2*0.75 + 0 --> output_1\ninput_1 -- 0.2*0.75 + 0 --> output_2\ninput_1 -- 0.2*0.75 + 0 --> output_3\ninput_2 -- 0.2*0.75 + 0 --> output_1\ninput_2 -- 0.2*0.75 + 0 --> output_2\ninput_2 -- 0.2*0.75 + 0 --> output_3\n```\n\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"50%\"}\n$$\\begin{array}{c|c|c}\n  \\text{predicted} & \\text{observed} & \\text{residual} \\\\\n  \\hline\n  0.3333 & 0 & 0.3333 \\\\\n  0.3333 & 0 & 0.3333 \\\\\n  0.3333 & 1 & -0.6667 \\\\\n\\end{array}$$\n\nNote: this initialization assumes that each penguin species is equally likely.\n:::\n\n::::\n\n## Partial Derivatives\n\nHere, we will update weight $w_{2,3} = 0.2$. The composition of the linear transformation, softmax, and SSR loss yields the following partial derivatives:\n\n* linear transformation:\n\n$$\\frac{\\partial L}{\\partial w_{2,3}} = x_{2}$$\n\n* softmax:\n\n$$\\frac{\\partial \\sigma_{3}}{\\partial L_{3}} = (\\sigma_{3})(1 - \\sigma_{3})$$\n\n* loss:\n\n$$\\frac{\\partial\\text{SSR}}{\\partial{\\sigma_{3}}} = -2(\\text{observed}_{3} - \\text{predicted}_{3})$$\n\n## Chain Rule\n\nPutting it all together, we can now compute the overall derviative through the chain rule:\n\n$$\\begin{array}{rcl}\n\\frac{\\partial\\text{SSR}}{\\partial w_{2,3}} & = & \\frac{\\partial\\text{SSR}}{\\partial{\\sigma_{3}}} \\cdot \\frac{\\partial \\sigma_{3}}{\\partial L_{3}} \\cdot \\frac{\\partial L}{\\partial w_{2,3}} \\\\\n~ & = & -2(\\text{observed}_{3} - \\text{predicted}_{3}) \\cdot (\\sigma_{3})(1 - \\sigma_{3}) \\cdot x_{2} \\\\\n~ & = & -2(1 - 0.3333) \\cdot (0.3)(1 - 0.3) \\cdot (0.75) \\\\\n~ & \\approx & -0.2100 \\\\\n\\end{array}$$\n\n::: {.callout-note collapse=\"true\"}\n## Why do we use the softmax?\n\nOne question that may appear here is, \"Why do we use min-max normalization for the input layer but then use a softmax for the output layer?\"\n\n* min-max normalization\n\n    * pro: easier to calculate (more easily vectorized)\n    * con: the derivative is simply just the number one (same if we used an argmax), so applying this normalization does not create information for us for back propogation\n    \n* softmax\n\n    * pro: derivative values between 0 and 1\n    * con: more intense calculation (especially for long vectors)\n:::\n\n::: {.callout-warning collapse=\"true\"}\n## Why don't we use the SSR?\n\nThe derivative values from the SSR are relatively small, and hence the learning is slow.\n:::\n\n## Update\n\nFinally, we can apply a step size\n\n$$\\text{step size} = \\text{derivative} \\cdot \\text{learning rate}$$\nIf we had a learning rate of 0.1 (i.e. as a hyperparameter), then our step size here is\n\n$$\\text{step size} = (-0.2100)(0.1) = -0.0210$$\nOur new weight is\n\n$$\\begin{array}{rcl}\n  \\text{new weight} & = & \\text{old weight} - \\text{step size} \\\\\n  ~ & = & 0.2 - (-0.0210) \\\\\n  ~ & = & 0.2210 \\\\\n\\end{array}$$\n\n## Feed Foward\n\nApplying this new weight, our network now looks like this\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n\n```{mermaid}\nflowchart LR\n\ninput_1[0.75]\ninput_2[0.75]\noutput_1[0.3000]\noutput_2[0.3000]\noutput_3[0.3158]\n\ninput_1 -- 0.2*0.75 + 0 --> output_1\ninput_1 -- 0.2*0.75 + 0 --> output_2\ninput_1 -- 0.2*0.75 + 0 --> output_3\ninput_2 -- 0.2*0.75 + 0 --> output_1\ninput_2 -- 0.2*0.75 + 0 --> output_2\ninput_2 -- 0.2210*0.75 + 0 --> output_3\n```\n\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"50%\"}\n$$\\begin{array}{c|c|c}\n  \\text{predicted} & \\text{observed} & \\text{residual} \\\\\n  \\hline\n  0.3316 & 0 & 0.3316 \\\\\n  0.3316 & 0 & 0.3316 \\\\\n  0.3366 & 1 & -0.6637 \\\\\n\\end{array}$$\n\nNow:\n\n* the prediction moved correctly toward \"Gentoo\"!\n* the residuals decreased!\n:::\n\n::::\n\n# Python Code\n\n## Tensors\n\nWe will use tensors in Pytorch Lightning\n\n![3D tensor](tensor_picture.png)\n\n* image source: [MIT](https://news.mit.edu/2017/faster-big-data-analysis-tensor-algebra-1031)\n\n### What are tensors?\n\nAt first, **tensors** (for machine learning) are multidimensional arrays.\n\n* a value is a 0D tensor\n* an array is a 1D tensor\n* a matrix is a 2D tensor\n\n### Why do we use tensors?\n\n* accelerated computations via graphical processing units (GPUs)\n* automatic differentiation\n* helps with parallelizable processes\n\n![parallel processes](parallel_process.png)\n\n* image source: [Faisal Shahbaz](https://medium.datadriveninvestor.com/python-multiprocessing-pool-vs-process-comparative-analysis-6c03c5b54eec)\n\n## Object-Oriented Programming\n\nFramework choice:\n\n* Keras\n* PyTorch\n* Scikit Learn\n* TensorFlow\n\nWhy Pytorch?\n\n* recency bias (Derek studied these concepts with PyTorch)\n* **object-oriented programming**\n\nWhy Object-Oriented Programming?\n\nLater concepts are probably better understood as *modular* steps in a workflow, which lend themselves to object-oriented programming (OOP)\n\n::::: {.panel-tabset}\n\n## Example 1\n\nFor the penguins examples, we needed a fully-connected network whose input layer size was 2 and whose output layer size was 3\n\n![example 1](FCN_2_3.png)\n\n* Python code: `model = FCN(2,3)`\n\n## Example 2\n\nFor the next example, we need a fully-connected network whose input layer size is 4 and whose output layer size is 2\n\n![example 2](FCN_4_2.png)\n\n* Python code: `model = FCN(4,2)`\n\n## Class Definition\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nclass FCN(pl.LightningModule):\n  # Fully-Connected Network\n  # assumes no hidden layer (i.e. going directly into activation functions)\n\n    def __init__(self, input_layer_size, output_layer_size):\n        super().__init__()\n        self.input_layer_size = input_layer_size \n        self.output_layer_size = output_layer_size \n        self.fc1 = nn.Linear(input_layer_size, output_layer_size)\n        self.test_step_outputs = []\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return x\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        output = self(x)\n        loss = F.cross_entropy(output, y)\n        return {'loss':loss}\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        output = self(x)\n        loss = F.cross_entropy(output, y)\n        self.test_step_outputs.append(loss)\n        return {'loss':loss}\n\n    def on_test_epoch_end(self):\n        epoch_average = torch.stack(self.test_step_outputs).mean()\n        self.log(\"test_epoch_average\", epoch_average)\n        self.test_step_outputs.clear()\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.SGD(self.parameters(), lr=lr)\n        return optimizer\n```\n:::\n\n\n\n:::::\n\n\n# Math Example\n\nWe start with a sequence of natural numbers\n\n$$\\{8, 9, 10, ..., 100\\}$$\nand we will use a neural network to try to classify the numbers as prime numbers or composite numbers. In this simple example, the inputs are indicator variables\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n* `div2`: number is divisible by 2\n* `div3`: number is divisible by 3\n* `div5`: number is divisible by 5\n* `div7`: number is divisible by 7\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\nand our network looks like\n\n\n\n```{mermaid}\nflowchart LR\n\ninput_1[div2]\ninput_2[div3]\ninput_3[div5]\ninput_4[div7]\noutput_1[composite]\noutput_2[prime]\n\ninput_1 --> output_1\ninput_1 --> output_2\ninput_2 --> output_1\ninput_2 --> output_2\ninput_3 --> output_1\ninput_3 --> output_2\ninput_4 --> output_1\ninput_4 --> output_2\n```\n\n\n:::\n\n::::\n\n> What do you think will happen to the *weights* and bias values upon training the network?\n\nAfter running the code for 100 epochs, the network calculations so far look like\n\n$$Wx + b$$\n\n$$\\left[\\begin{array}{rrrr}\n  0.92 & 0.34 & 0.79 & 0.68 \\\\\n  -0.40 & -0.45 & 0.80 & -0.36 \\\\\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n  x_{1} \\\\ x_{2} \\\\ x_{3} \\\\ x_{4} \\\\\n\\end{array}\\right]\n+\n\\left[\\begin{array}{c}\n  0.92 \\\\ 0.42 \\\\\n\\end{array}\\right]\n$$\n\n# Ethics Segment: Voices\n\n::::: {.panel-tabset}\n\n## Scene\n\nIn 2025, New Era unveiled their lineup of fusion caps for baseball franchises.\n\n![New Era 2025 Hats](new_era_hats.png)\n\n## Cases\n\nSome hats (such as this one from 2024) raised some concerns\n\n![New Era 2024 Athletics](new_era_hats_athletics.png)\n\n## Fallout\n\nThe company had to recall many of its products.\n\n![New Era 2025 fall out](new_era_hats_fallout.png)\n\n* image credit: [Sports Illustrated](https://www.si.com/mlb/texas-rangers-new-era-hat-pulled-vulgarity)\n\n:::::\n\n\n# Quo Vadimus?\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n* due this Friday (March 21):\n\n    - Precept 6\n    - Pick project\n    - Art Images Use-Case Collection\n    \n* due next Friday (March 28):\n\n    - Poster Feedback\n    - Precept 7\n    - Literature search\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"50%\"}\n\n\n:::\n\n::::\n\n\n# Footnotes\n\n::: {.callout-note collapse=\"true\"}\n\n## (optional) Additional Resources and References\n\n* [Pytorch history](https://alexmoltzau.medium.com/pytorch-governance-and-history-2e5889b79dc1) by Alex Moltzau\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.3      forcats_1.0.0        stringr_1.5.1       \n [4] dplyr_1.1.4          purrr_1.0.4          readr_2.1.5         \n [7] tidyr_1.3.1          tibble_3.2.1         ggplot2_3.5.1       \n[10] tidyverse_2.0.0      patchwork_1.3.0      palmerpenguins_0.1.1\n[13] ggtext_0.1.2        \n\nloaded via a namespace (and not attached):\n [1] generics_0.1.3    xml2_1.3.6        stringi_1.8.4     lattice_0.22-6   \n [5] hms_1.1.3         digest_0.6.37     magrittr_2.0.3    evaluate_1.0.3   \n [9] grid_4.4.2        timechange_0.3.0  fastmap_1.2.0     jsonlite_1.8.9   \n[13] Matrix_1.7-1      scales_1.3.0      cli_3.6.3         rlang_1.1.5      \n[17] munsell_0.5.1     withr_3.0.2       yaml_2.3.10       tools_4.4.2      \n[21] tzdb_0.4.0        colorspace_2.1-1  reticulate_1.38.0 vctrs_0.6.5      \n[25] R6_2.5.1          png_0.1-8         lifecycle_1.0.4   htmlwidgets_1.6.4\n[29] pkgconfig_2.0.3   pillar_1.10.1     gtable_0.3.6      glue_1.8.0       \n[33] Rcpp_1.0.12       xfun_0.50         tidyselect_1.2.1  rstudioapi_0.17.1\n[37] knitr_1.49        farver_2.1.2      htmltools_0.5.8.1 rmarkdown_2.29   \n[41] labeling_0.4.3    compiler_4.4.2    gridtext_0.1.5   \n```\n\n\n:::\n:::\n\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::",
    "supporting": [
      "13_FCN_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}