{
  "hash": "ca932f4b7fa603e1764e494b56f41618",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"14: Generating Images\"\nauthor: \"Derek Sollberger\"\ndate: \"2025-10-27\"\nformat:\n  html:\n    toc: true\n---\n\n::: {.cell}\n\n:::\n\n\n\n# Session 14: Generating Images\n\n## Learning objectives:\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- Introduce generative adversarial networks\n- Introduce variational autoencoders\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n![Cycle GANs](cycle_gan_paintings.png)\n\n* image credit: [Jun-Yan Zhu, et al.](https://junyanz.github.io/CycleGAN/)\n:::\n\n::::\n\n## Generative AI\n\n::::: {.panel-tabset}\n\n### Definition\n\n:::: {.columns}\n\n::: {.column width=\"75%\"}\n* \"Generative AI is a form of artificial intelligence that is designed to generate content, including text, images, video and music. It uses large language models and algorithms to analyze patterns in datasets to mimic the style or structure of specific types of content.\"\n* [quote and image source](https://www.eweek.com/artificial-intelligence/generative-ai-vs-machine-learning/)\n:::\n\n::: {.column width=\"25%\"}\n![generative AI](generative_ai.png)\n:::\n\n::::\n\n### Dall-E\n\n![June 2022](Dall_e_cheeseburgers.png)\n\n[image source](https://www.onefootdown.com/2022/6/21/23175949/notre-dame-football-immersive-and-horrifying-dall-e-mini-art-experience-gumbo-pitbull-cheeseburgers)\n\n### Stable Diffusion\n\n![August 2023](stable_diffusion.png)\n\n[image source](https://www.geeky-gadgets.com/stable-diffusion-sdxl-beginners-guide/)\n\n:::::\n\n\n\n\n# GANS\n\n## Overview\n\n![GANs overview](DLI_fig_12_1.png)\n\n* image source: [Deep Learning Illustrated](https://www.deeplearningillustrated.com/)\n\n## Discriminator\n\n![Training the discriminator](DLI_fig_12_2.png)\n\n* image source: [Deep Learning Illustrated](https://www.deeplearningillustrated.com/)\n\n## Generator\n\n![Training the generator](DLI_fig_12_3.png)\n\n* image source: [Deep Learning Illustrated](https://www.deeplearningillustrated.com/)\n\n::: {.callout-tip}\n## GAN Objectives\n\n* the goal of a generator is to create better fake images\n* the goal of a discriminator is to better classify fake images\n\n:::\n\n## Adversarial Network\n\n![Adversarial network](DLI_fig_12_4.png)\n\n* image source: [Deep Learning Illustrated](https://www.deeplearningillustrated.com/)\n\n## Initial Results\n\n![Goodfellow, et al., 2014](Goodfellow_2014.png)\n# Activity: Quick Draw!\n\n::::: {.panel-tabset}\n\n## App\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n![Quick Draw, by Google](QuickDraw.png)\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"60%\"}\nTry out Quick Draw!\n\n* [https://quickdraw.withgoogle.com/](https://quickdraw.withgoogle.com/)\n* app will ask you to sketch 6 images\n\n    * and it will try to recognize your artwork\n\n* caution: there is sound\n\n:::\n\n::::\n\n## Lions\n\n![Quick, Draw! Lions](quickdraw_lions.png)\n\n## Tigers\n\n![Quick, Draw! Tigers](quickdraw_tigers.png)\n\n## Penguins\n\n![Quick, Draw! Penguins](quickdraw_penguins.png)\n\n:::::\n\n::: {.callout-warning}\n\n## DCP1\n\n:::\n\n# Conditional GANs\n\n::: {.callout-note}\n## cGANs\n\n\"We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also **learn a loss function** to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations.\"\n\n* [Isola, et al.](https://phillipi.github.io/pix2pix/)\n:::\n\n![cGAN in ecology](cGAN_ecology.png)\n\n* image source: [Hayatbini, et al.](https://www.mdpi.com/2072-4292/11/19/2193#)\n\n## Activity: Pix2Pix Image-to-Image\n\n::::: {.panel-tabset}\n\n## App\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![image-to-image](pix2pix_shoes.png)\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n* [app (link)](https://affinelayer.com/pixsrv/) by Christopher Hesse (no longer active)\n* [GitHub repo](https://github.com/affinelayer/pix2pix-tensorflow)\n* [TensorFlow code](https://www.tensorflow.org/tutorials/generative/pix2pix)\n:::\n\n::::\n\n## ex1\n\n![edges2cats](edges2cats.png)\n\n## ex2\n\n![facades](facades.png)\n\n## ex1\n\n![edges2handbags](edges2handbags.png)\n\n:::::\n\n# Activity: Pix2Pix Instruct\n\n::::: {.panel-tabset}\n\n## App\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![pix2pix](pix2pix.png)\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n* [app (link)](https://huggingface.co/spaces/timbrooks/instruct-pix2pix) by Tim Brooks (no longer active)\n* [GitHub repo](https://github.com/timothybrooks/instruct-pix2pix)\n\n:::\n\n::::\n\n## ex1\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![New Jersey flag](NJ_flag.png)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![convert to grayscale](NJ_flag_grayscale.png)\n:::\n\n::::\n\n## ex2\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![New Jersey flag](NJ_flag.png)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![add more horses](NJ_flag_more_horses.png)\n:::\n\n::::\n\n## ex3\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![New Jersey flag](NJ_flag.png)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![replace humans with ramen](NJ_flag_ramen.png)\n:::\n\n::::\n\n## ex2025\n\nTo jazz up a session about computer vision, I told my students that it used to be quite innovative to prompt a multimodal chatbot with something like \"Turn people into ramen\". Here is what one of our students then made in Copilot.\n\n![Blair Arch Group Photo](blair_arch_ramen.png)\n\n:::::\n\n::: {.callout-warning}\n\n## DCP2\n\n:::\n\n# Stacked GANs\n\n::: {.callout-note}\n## Stacked GANs\n\n*  \"Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images\"\n* \" Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details\"\n\n:::\n\n![Zhang, et al., 2016](stackGAN_stages.png)\n\n## Architecture\n\n![Zhang, et al., 2016](stackGAN_architecture.png)\n\n## Scaling\n\n![Zhang, et al., 2016](stackGAN_scaling.png)\n\n\n# Cycle GANs\n\n![cycle GAN possibilities](cycle_GAN_splash.png)\n\n::: {.callout-note}\n## cGANs\n\n* \"... learning to translate an image from a source domain $X$ to a target domain $Y$ in the absence of paired examples ...\" --- [Jun-Yan Zhu, et al., 2017](https://junyanz.github.io/CycleGAN/)\n\n:::\n\n\n\n## Architecture\n\n![](cycle_GAN_architecture_A2B.png)\n\n![cycle GAN architecture](cycle_GAN_architecture_B2A.png)\n\n* images source: [Bansal and Rathore](https://hardikbansal.github.io/CycleGANBlog/)\n\n::: {.callout-tip}\n## cGAN Objective\n\nIn a cyclic GAN, the generator and discriminator converge toward a Nash equilibrium.\n\n:::\n\n# Ethics Corner: DeepfakeBench\n\n![DeepfakeBench](DeepfakeBench.png)\n\n* image source: [DeepfakeBench](https://github.com/SCLBD/DeepfakeBench)\n\n::: {.callout-warning}\n\n## DCP3\n\n:::\n\n# Variational Autoencoders\n\n::: {.callout-note}\n## Variational Autoencoders\n\n**VAEs** (variational autoencoders) increase probability density around training data to encourage generated information to be more similar to the training data.\n:::\n\nIn AI architecture, **variational autoencoders** simulate the latent space (between the encoder and decoder)---usually with a mixture of Gaussian functions---to maximize the ELBO (**evidence lower bound**)\t\n\n![VAE Gaussians](VAE_gaussians.png)\n\n* [image source](https://www.researchgate.net/publication/379478988_HGMVAE_hierarchical_disentanglement_in_Gaussian_mixture_variational_autoencoder)\n\n## without VAEs\n\n![without VAE](VAE_before.png)\n\n* image source: [Jeff Orchard](https://www.youtube.com/watch?v=FSBLj74Qy4I)\n\n## with VAEs\n\n![with VAE](VAE_after.png)\n\n* image source: [Jeff Orchard](https://www.youtube.com/watch?v=FSBLj74Qy4I)\n\n\n\n\n\n\n\n\n\n# Unsupervised Representation Learning\n\n![modifications to pictures](radford_rooms.png)\n\n* image source: [Radford, et al, 2016](https://arxiv.org/abs/1511.06434)\n\n![trained filters](radford_filters.png)\n\n* image source: [Radford, et al, 2016](https://arxiv.org/abs/1511.06434)\n\n::::: {.panel-tabset}\n\n## Image Space\n\n![vector space of images](image space.png)\n\n* image source: [Deep Learning Illustrated](https://www.deeplearningillustrated.com/)\n\n## ex1\n\n![image space](image_space_ex1.png)\n\n* image source: [Radford, et al, 2016](https://arxiv.org/abs/1511.06434)\n\n## ex2\n\n![image space](image_space_ex2.png)\n\n* image source: [Radford, et al, 2016](https://arxiv.org/abs/1511.06434)\n\n:::::\n\n\n\n\n# Quo Vadimus?\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n* bring a digital image for Wednesday's class\n\n* due this Friday (Oct 31):\n\n    - Precept 7 (1 hour)\n    - Multipass: xLSTM (20 minutes)\n    - Literature Search (10 minutes)\n  \n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![Central Park transformed](central_park_meme.png)\n:::\n\n::::\n\n\n# Footnotes\n\n::: {.callout-note collapse=\"true\"}\n\n## (optional) Additional Resources and References\n\n* [flag of New Jersey](https://www.britannica.com/topic/flag-of-New-Jersey) article at Britannica\n\n:::\n\n## Ethics Corner: Research Consent\n\n![IRB principles](IRB_principles.png)\n\n* image source: [Ross Avilla](https://methods.sagepub.com/video/ethical-guidelines-and-irb)\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.5.1 (2025-06-13 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n  LAPACK version 3.12.1\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.5.1    fastmap_1.2.0     cli_3.6.5        \n [5] tools_4.5.1       htmltools_0.5.8.1 rstudioapi_0.17.1 yaml_2.3.10      \n [9] rmarkdown_2.29    knitr_1.50        jsonlite_2.0.0    xfun_0.52        \n[13] digest_0.6.37     rlang_1.1.6       evaluate_1.0.4   \n```\n\n\n:::\n:::\n\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}