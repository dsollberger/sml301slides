{
  "hash": "fcd385753ad47cdc35d86e36569ef07d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"20: Prompt Engineering\"\nauthor: \"Derek Sollberger\"\ndate: \"2025-11-17\"\nformat:\n  html:\n    toc: true\n---\n\n\n\n\n# 20: Prompt Engineering\n\n## Learning Objectives\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- transformer interpretability\n- text generation hyperparameters\n- intro prompt engineering\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n![prompt engineering](prompt_engineering_overview.png)\n\n* image source: [Cobus Greyling](https://www.humanfirst.ai/blog/12-prompt-engineering-techniques)\n\n:::\n\n::::\n\n# Interpretability\n\n::: {.callout-note}\n## Interpretability\n\nIdeally, we would want our machine learning models to have **interpretability** where we could describe the effect of each coefficient in the model.  However, as technologies advanced, we sacrificed interpretability for deep learning.\n:::\n\n\n## Example: Spam Filters\n\n::::: {.panel-tabset}\n\n### before ML\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n*Keyword* filters\n\n* watches\n* wtches\n* watch3s\n* watchhes\n* ...\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![spam email](spam_watches.png)\n\n* image source: [Secure List](https://securelist.com/spam-in-february-2013/36951/)\n:::\n\n::::\n\n### after ML\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![Naive Bayes Classifier](naive_bayes_spam_filter.png)\n\n* image source: [Arthur V Ratz](https://www.codeproject.com/articles/Na-ve-Bayesian-Anti-Spam-Filter-Using-Node-JS-Java#comments-section)\n\n> In 2002, Paul Graham used Bayes' Rule as part of his algorithms to greatly decrease false positive rates of unwanted e-mails (\"spam\").   Let $H^{c}$ be the event that an e-mail is \"spam\".  Let $W$ be the event that an e-mail contains a trigger word such as \"watches\".  Suppose that\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n\n- the probability that an e-mail contains that word given that it is spam is 17\\%\n- the probability that an e-mail contains that word given that it is not spam is 9\\%\n- the probability that a randomly selected e-mail message is spam is 80\\%\n\n> Find the probability that an e-mail message is spam, given that the trigger word appears.\n\n* across large vocabulary\n* broad movement toward *semantic* associations\n\n:::\n\n::::\n\n:::::\n\n\n## Example: Credit Card Fraud\n\n::::: {.panel-tabset}\n\n### before ML\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\nFinanical companies would send text messages and/or phone calls for *anomalies* or suspicious activity.\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n![fraud notification](credit_card_fraud_notification.png)\n\n* image source: [Merchants & Marine Bank](https://mandmbank.com/introducing-text-alerts/)\n:::\n\n::::\n\n### after ML\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n![financial cybersecurity](Cyber-Security-for-the-Banking.png)\n\n* image source: [Enterprise Edges](https://threatcop.com/blog/cybersecurity-in-banking/)\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\nCybersecurity firms develop robust systems to protect against a variety of attacks.\n:::\n\n::::\n\n:::::\n\n\n## Convolutional Neural Networks\n\n![CNN architecture](cnn_architecture.png)\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\n::: {.callout-tip}\n## Intuitively\n\n* edge detection\n* object detection\n* color scales\n* hue contrast\n* ...\n:::\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n::: {.callout-warning}\n## Actually\n\nbillions of weights and bias parameters trained on thousands of image files\n:::\n:::\n\n::::\n\n* image source: [MK Gurucharan](https://www.upgrad.com/blog/basic-cnn-architecture/)\n\n\n## Transformers\n\n![multi-head attention](multi_head_attention.png)\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\n::: {.callout-tip}\n## Intuitively\n\n* grammar\n* syntax\n* sarcasm\n* jargon\n* ...\n:::\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n::: {.callout-warning}\n## Actually\n\noptimized for training in the encoded reduced dimension space\n:::\n:::\n\n::::\n\n* image source: [Damien Benveniste](https://newsletter.theaiedge.io/p/the-transformer-architecture-v2)\n\n::: {.callout-tip}\n## Cutting Edge Research\n\nInvestigations into the embeddings signals\n\n* representation engineering\n* mechanistic interpretability\n:::\n\n# LLM Hyperparameters\n\n::: {.callout-tip}\n## Hyperparameters\n\nIn machine learning, a hyperparameter is a parameter that can be set in order to define any configurable part of a model's learning process. Hyperparameters can be classified as either model hyperparameters (such as the topology and size of a neural network) or algorithm hyperparameters (such as the learning rate and the batch size of an optimizer). These are named hyperparameters in contrast to parameters, which are characteristics that the model learns from the data. \n---[Wikipedia](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning))\n:::\n\n::: {.callout-warning}\n## DCP1\n:::\n\n\n## Temperature\n\n::: {.callout-note}\n## Temperature\n\nFor this hyperparameter, **temperature** rescales logits before applying a softmax\n\n$$p_{i} = \\frac{e^{z_{i}/T}}{\\sum_{i=1}^{n} e^{z_{i}/T}}$$\n\n* $T = 0$: greedy selection of softmax\n* low temperature (e.g. $T = 0.301$): more deterministic output\n* high temperature (e.g. $T = 1.301$): more creative output\n:::\n\n![temperature sampling](temperature_sampling-1.png)\n\n* image source: [Marcus D.R. Klarqvist](https://www.mdrk.io/temperature-samplig-in-ai/)\n\n::: {.callout-warning}\n## DCP2\n:::\n\n## Hallucinations\n\n::: {.callout-warning}\n## Hallucinations\n\n\"Hallucination happens when an AI model makes stuff up. Itâ€™s a big reason why many companies are hesitant to incorporate LLMs into their workflows.\" --- [Chip Huyen](https://huyenchip.com/2023/05/02/rlhf.html)\n\n:::\n\n![hallucination evals](10-hallucination.png)\n\n* image source: [Ouyang et al, 2022](https://huyenchip.com/2023/05/02/rlhf.html) paper about RLHF (reinforcement learning with human feedback)\n\n\n\n\n\n\n\n## Top-p\n\n::: {.callout-note}\n## Top-p\n\nFor this hyperparameter, **top-p** (aka **nucleus sampling**) restricts the possible outputs to those whose *cumulative softmax* is at most $P_{\\text{top}}$\n\n$$i \\text{ where } \\sum_{j = 1}^{i} \\frac{e^{z_{i}}}{\\sum_{i=1}^{n} e^{z_{i}}} \\leq P_{\\text{top}}$$\n\n* low top-p (e.g. `top_p` = 0.301): more deterministic output\n* high top-p (e.g. `top_p` = 0.903): more creative output\n:::\n\n![top-p sampling](top_p_sampling.png)\n\n* image source: [Daniel Puente Viejo](https://www.linkedin.com/pulse/science-control-how-temperature-topp-topk-shape-large-puente-viejo-u88yf/)\n\n\n## Application\n\n::: {.callout-tip collapse=\"true\"}\n\n## hyperparameter tuning intuition\n\n* high temperature, high `top-p`: brainstorming\n* low temperature, low `top-p`: email generation\n* high temperature, low `top-p`: creative writing\n* low temperature, high `top-p`: translation\n\n**Table 6-1** from *Hands-On Large Language Models* by Jay Alammar and Maarten Grottendorst\n\n:::\n\n\n# Data History: Neural Networks\n\n::::: {.panel-tabset}\n\n## neuron\n\n![neuron](neuron.png)\n\n* image source: [Interactive Biology](https://interactivebiology.com/3439/the-neuron-internal-structure/)\n\n## perceptron\n\n![perceptron](perceptron.png)\n\n* image source: [Muneeb S Ahmad](https://muneebsa.medium.com/deep-learning-101-lesson-7-perceptron-f6a698d81be8)\n\n## net\n\n![neural network](neural_networks-007.png)\n\n* image source: [TikZ](https://tikz.net/neural_networks/)\n\n## eyes\n\n![cat eyes](cat_eyes.png)\n\n* image source: [Bark & Whiskers](https://www.barkandwhiskers.com/2025-06-13-dilated-cat-eyes/)\n\n## vision\n\n![vision signals](visual_cells.png)\n\n* image source: [Why Machines Learn](https://www.penguinrandomhouse.com/books/677608/why-machines-learn-by-anil-ananthaswamy/) by Anil Ananthaswamy\n\n## reception\n\n![Cruelty in Labs](Zak_cruelty_in_labs.png)\n\n* newspaper article: [NY Times](https://www.nytimes.com/1983/05/16/opinion/cruelty-in-labs.html), May 16, 1983\n\n## researchers\n\n![Harvard Medical School](retrospective.png)\n\n* image source: [Harvard Medical School](https://magazine.hms.harvard.edu/articles/our-mind)\n\n:::::\n\n\n# Prompt Engineering\n\n## Complexity\n\n* persona\n* instruction\n* context\n* format\n* audience\n* tone\n* data\n\nfrom *Hands-On Large Language Models* by Jay Alammar and Maarten Grottendorst\n\n\n# Ethics Corner: Research Consent\n\n![IRB principles](IRB_principles.png)\n\n* image source: [Ross Avilla](https://methods.sagepub.com/video/ethical-guidelines-and-irb)\n\n\n# Case Study: Color Sensitivity\n\n* Derek Sollberger, Princeton University\n* Hayley Orndorf, BioMADE\n\n::::: {.panel-tabset}\n\n## Introduction\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- We hope to build a CNN to classify pictures in terms of susceptibility to color blindness\n- Train model on Ishihara data set\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n![301](ishihara_example_301.png)\n:::\n\n::::\n\n## Data Description\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- [Kaggle data set](https://www.kaggle.com/datasets/dupeljan/ishihara-blind-test-cards) by Dupeljan\n- synethetic set of 1400 Ishihara blind test cards\n- digits 0 to 9\n- Google fonts\n- for exploring dichromacy:\n\n    1. protanopia\n    2. deuteranopia\n    3. tritanopia\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n![color vision deficiency](color_vision_deficiency.png)\n\n* image source: [Male, et al., 2022](https://pmc.ncbi.nlm.nih.gov/articles/PMC9498227/)\n:::\n\n::::\n\n## novel\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- Run model on novel images from the Princeton Art Museum\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n![labeling](results_collage.png)\n:::\n\n::::\n\n:::::\n\n\n\n\n\n\n\n\n# Quo Vadimus?\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n* due this Friday (Nov 21):\n\n    - **Precept 10** (2 hours)\n    - surveys:\n        - **Participant Informed Consent** (5 minutes)\n        - **Research Consent** (5 minutes)\n    - group work:\n        - **Abstract (draft)** (30 minutes)\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"30%\"}\n\nOur SML 301 lecture session for **Monday, November 24** will be remote (Zoom).\n\n* guest speaker from AI industry\n\n:::\n\n::::\n\n\n# Footnotes\n\n::: {.callout-note collapse=\"true\"}\n\n## (optional) Additional Resources and References\n\n* [LLM strategies](https://towardsdatascience.com/decoding-strategies-in-large-language-models-9733a8f70539/)\n* [Prompting Guide](https://www.promptingguide.ai/techniques)\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.5.1 (2025-06-13 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n  LAPACK version 3.12.1\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.5.1    fastmap_1.2.0     cli_3.6.5        \n [5] tools_4.5.1       htmltools_0.5.8.1 rstudioapi_0.17.1 yaml_2.3.10      \n [9] rmarkdown_2.29    knitr_1.50        jsonlite_2.0.0    xfun_0.52        \n[13] digest_0.6.37     rlang_1.1.6       evaluate_1.0.4   \n```\n\n\n:::\n:::\n\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}