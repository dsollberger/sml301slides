{
  "hash": "574110555144175c9b9c798921b213c7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"14: Recurrent Neural Networks\"\nauthor: \"Derek Sollberger\"\ndate: \"2025-03-19\"\nformat:\n  html:\n    toc: true\n---\n\n::: {.cell}\n\n:::\n\n\n\n# Session 14: Recurrent Neural Networks\n\n## Learning objectives:\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- Discuss hidden layer size\n- Introduce the notion of memory for neural nets\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n![neural network](NN_4_2_3.png)\n:::\n\n::::\n\n\n# Cross Entropy\n\nRecall, a step size in back propagation is\n\n$$\\text{step size} = \\text{derivative} \\cdot \\text{learning rate}$$\n\n::: {.callout-note}\n## Cross Entropy Loss\n\nWhen classifying among $C$ classes, for an array of predictions (after computing a softmax) $\\vec{s}$ and its associated vector of true observations $\\vec{y}$, the **cross entropy loss** is calculated as\n\n$$L(\\vec{s}, \\vec{y}) = -\\sum_{i=1}^{C} y_{i}\\log s_{i}$$\n:::\n\n![cross entropy loss](cross_entropy_graph.png)\n\n* logarithm of softmax\n* larger derivative values for larger misclassification values\n* larger step sizes for larger misclassification values\n* image source: [Machine Learning Glossary](https://ml-cheatsheet.readthedocs.io/en/latest/index.html)\n\n::: {.callout-tip}\n## Cross Entropy Inference\n\nAs a logarithm (a monotonic transformation) of softmax and similar to the sum-of-squared residuals (SSR), we are likewise seeking lower values of errors.\n\n* smaller cross entropy $\\rightarrow$ better network\n:::\n\n\n\n\n\n\n\n\n\n\n\n# Math Example\n\nWe start with a sequence of natural numbers\n\n$$\\{8, 9, 10, ..., 100\\}$$\nand we will use a neural network to try to classify the numbers as prime numbers or composite numbers. In this simple example, the inputs are indicator variables\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n* `div2`: number is divisible by 2\n* `div3`: number is divisible by 3\n* `div5`: number is divisible by 5\n* `div7`: number is divisible by 7\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\nand our network looks like\n\n\n\n```{mermaid}\nflowchart LR\n\ninput_1[div2]\ninput_2[div3]\ninput_3[div5]\ninput_4[div7]\noutput_1[composite]\noutput_2[prime]\n\ninput_1 --> output_1\ninput_1 --> output_2\ninput_2 --> output_1\ninput_2 --> output_2\ninput_3 --> output_1\ninput_3 --> output_2\ninput_4 --> output_1\ninput_4 --> output_2\n```\n\n\n:::\n\n::::\n\n> What do you think will happen to the *weights* and bias values upon training the network?\n\nAfter running the code for 100 epochs, the network calculations so far look like\n\n$$Wx + b$$\n\n$$\\left[\\begin{array}{rrrr}\n  0.92 & 0.34 & 0.79 & 0.68 \\\\\n  -0.40 & -0.45 & 0.80 & -0.36 \\\\\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n  x_{1} \\\\ x_{2} \\\\ x_{3} \\\\ x_{4} \\\\\n\\end{array}\\right]\n+\n\\left[\\begin{array}{c}\n  0.92 \\\\ 0.42 \\\\\n\\end{array}\\right]$$\n\n\n# Hidden Layer\n\nTo try to capture abstract notions with a neural network, we can employ a **hidden layer** between the input and output layers.\n\n::: {.callout-note collapse=\"true\"}\n## Python code\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nclass NN1H(pl.LightningModule):\n  # Neural Network object\n  # assumes one hidden layer (and default learning rate: 0.01)\n  # inheirited class from LightningModule (for faster computations)\n\n    def __init__(self, input_layer_size, hidden_layer_size,\n                 output_layer_size, learning_rate = None):\n        super().__init__()\n        self.input_layer_size = input_layer_size #input layer size (number of explanatory variables)\n        self.hidden_layer_size = hidden_layer_size\n        self.learning_rate = learning_rate if learning_rate is not None else 0.01\n        self.output_layer_size = output_layer_size #output layer size: 3 (number of penguin species)\n        self.fc1 = nn.Linear(input_layer_size, hidden_layer_size)\n        self.fc2 = nn.Linear(hidden_layer_size, output_layer_size)\n        self.test_step_outputs = []\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n    def training_step(self, batch, batch_idx): # what happen in each train step\n        x, y = batch\n        output = self(x)\n        loss = F.cross_entropy(output, y)\n        # self.log('train_loss', loss, on_epoch=True) # use this for logging (e.g. using TensorBoard)\n        return {'loss':loss}\n\n    def test_step(self, batch, batch_idx): # what happen in each test step\n        x, y = batch\n        output = self(x)\n        loss = F.cross_entropy(output, y)\n        self.test_step_outputs.append(loss)\n        return {'loss':loss}\n\n    def on_test_epoch_end(self):\n        epoch_average = torch.stack(self.test_step_outputs).mean()\n        self.log(\"test_epoch_average\", epoch_average)\n        self.test_step_outputs.clear()  # free memory\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.SGD(self.parameters(), lr = lr)\n        return optimizer\n```\n:::\n\n\n:::\n\nOnce again, let us deploy the `palmerpenguins`:\n\n* $x_{1}$: bill length (mm)\n* $x_{2}$: bill depth (mm)\n* $x_{3}$: flipper length (mm)\n* $x_{4}$: body mass (g)\n\n![neural network](NN_4_2_3.png)\n\n$$\\text{min-max normalization} \\rightarrow Wx + b \\rightarrow \\text{ReLU} \\rightarrow Wx + b \\rightarrow \\text{cross entropy}$$\n\nAfter 100 epochs, our weights and bias values are\n\n$$\\begin{array}{c}\n\\text{min-max normalization} \\\\ \n\\downarrow \\\\\n\\left[\\begin{array}{rrrr}\n  0.04 & -1.19 & 0.96 & 0.91 \\\\\n  -1.59 & 0.65 & 0.02 & 0.16 \\\\\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n  x_{1} \\\\ x_{2} \\\\ x_{3} \\\\ x_{4} \\\\\n\\end{array}\\right]\n+\n\\left[\\begin{array}{r}\n  0.55 \\\\ 0.62 \\\\\n\\end{array}\\right] \\\\\n\\downarrow \\\\\n\\text{ReLU} \\\\\n\\downarrow \\\\\n\\left[\\begin{array}{rr}\n  -1.00 & 1.17 \\\\\n  -1.07 & -1.21 \\\\\n  1.08 & -0.74 \\\\\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n  x_{1} \\\\ x_{2} \\\\ \n\\end{array}\\right]\n+\n\\left[\\begin{array}{r}\n  -0.29 \\\\ 1.28 \\\\ -1.38 \\\\\n\\end{array}\\right] \\\\\n\\downarrow \\\\\n\\text{cross entropy}\n\\end{array}$$\n\n\n# Ethics Segment: How Data Happened\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n![How Data Happened](How_Data_Happened.png)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"50%\"}\n* Chris Wiggins, Princeton PhD (1998) in theoretical physics\n\n* \"Statistical thinking from the 1700s onward rested fundamentally on the explosion of the collection of data about states, their people, and, quite often, people deemed to be deviant\" --- How Data Happened, page 23\n:::\n\n::::\n\n\n# Hidden Layer Size\n\n::: {.callout-note collapse=\"true\"}\n## How do we decide on a hidden layer size?\n\nIndustry professionals and writers suggest powers of two:\n\n$$d_{\\text{model}} = 2, 4, 8, 16, ...$$\n\nand choose a model with a low cross-entropy result (i.e. when increasing the hidden layer size barely reduces the cross-entropy amount).\n\n* MNIST paper: $d_{\\text{model}} = 16$\n* Attention paper: $d_{\\text{model}} = 1024$\n\n:::\n\n::::: {.panel-tabset}\n\n## 2\n\n![h = 2](NN_4_2_3.png)\n\n* cross-entropy: 0.0800\n\n## 4\n\n![h = 4](NN_4_4_3.png)\n\n* cross-entropy: 0.0509\n\n## 8\n\n![h = 8](NN_4_8_3.png)\n\n* cross-entropy: 0.0599\n\n## 16\n\n![h = 16](NN_4_16_3.png)\n\n* cross-entropy: 0.0280\n\n:::::\n\n\n# Deep Learning\n\n::: {.callout-note}\n## Deep Learning\n\nIn our studies of artificial learning, **deep learning** is using multiple hidden layers.\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Python Code\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nclass NN2H(pl.LightningModule):\n  # Neural Network object\n  # assumes two hidden layers (and default learning rate: 0.01)\n  # inheirited class from LightningModule (for faster computations)\n\n    def __init__(self, input_layer_size, h1, h2,\n                 output_layer_size, learning_rate = None):\n        super().__init__()\n        self.input_layer_size = input_layer_size #input layer size (number of explanatory variables)\n        self.h1 = h1\n        self.h2 = h2\n        self.learning_rate = learning_rate if learning_rate is not None else 0.01\n        self.output_layer_size = output_layer_size #output layer size: 3 (number of penguin species)\n        self.fc1 = nn.Linear(input_layer_size, h1)\n        self.fc2 = nn.Linear(h1, h2)\n        self.fc3 = nn.Linear(h2, output_layer_size)\n        self.test_step_outputs = []\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n    def training_step(self, batch, batch_idx): # what happen in each train step\n        x, y = batch\n        output = self(x)\n        loss = F.cross_entropy(output, y)\n        # self.log('train_loss', loss, on_epoch=True) # use this for logging (e.g. using TensorBoard)\n        return {'loss':loss}\n\n    def test_step(self, batch, batch_idx): # what happen in each test step\n        x, y = batch\n        output = self(x)\n        loss = F.cross_entropy(output, y)\n        self.test_step_outputs.append(loss)\n        return {'loss':loss}\n\n    def on_test_epoch_end(self):\n        epoch_average = torch.stack(self.test_step_outputs).mean()\n        self.log(\"test_epoch_average\", epoch_average)\n        self.test_step_outputs.clear()  # free memory\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.SGD(self.parameters(), lr = lr)\n        return optimizer\n```\n:::\n\n\n:::\n\n![deep learning](NN_4_2_4_3.png)\n\n\n# Recurrent Neural Networks\n\n::: {.callout-note}\n## Recurrent Neural Networks\n\nIn order to move toward retaining some *memory* in our inputs to artificial intelligence models, we will here briefly discuss **recurrent neural networks**.\n\n\n\n```{mermaid}\nflowchart LR\n\ninput1[input]\n\nneuron1[neuron]\n\noutput1[output]\n\ninput1 --> neuron1\nneuron1 --> output1\nneuron1 --> neuron1\n```\n\n\n\nAs we *unfold* the network, we use the same weights and bias values between the layers.\n\n:::\n\n## Example: 867-5309\n\nWe start with a sequence that has \n\n$$\\{8, 6, 7, 5, 3, 0, 9, ...\\}$$\nrepeated over and over, and we hope that\n\n* input \"8\" leads to a prediction of \"6\"\n* input \"6\" leads to a prediction of \"7\"\n* input \"7\" leads to a prediction of \"5\"\n* input \"5\" leads to a prediction of \"3\"\n* input \"3\" leads to a prediction of \"0\"\n* input \"0\" leads to a prediction of \"9\"\n\n## Unfolding\n\n\n\n### 8\n\n* Does input \"8\" leads to a prediction of \"6\"?\n\n\n\n```{mermaid}\nflowchart TD\n\ninput1[8]\n\nneuron1[ReLU]\n\noutput1[6.2494]\n\ninput1 -- 0.20x + 0.86 --> neuron1\n\nneuron1 -- 0.91x - 0.25 --> output1\n```\n\n\n\n* SSR:0.0622\n\n### 6\n\n* Does input \"6\" leads to a prediction of \"7\"?\n\n\n\n```{mermaid}\nflowchart TD\n\ninput1[8]\ninput2[6]\n\nneuron1[ReLU]\nneuron2[ReLU]\n\noutput1[NA]\noutput2[5.2948]\n\ninput1 -- 0.20x + 0.86 --> neuron1\ninput2 -- 0.20x + 0.86 --> neuron2\n\nneuron1 -- 0.91x - 0.25 --> output1\nneuron2 -- 0.91x - 0.25 --> output2\n\nneuron1 -- -0.85x + 0.81 --> neuron2\n```\n\n\n\n* SSR: 2.9077\n\n### 7\n\n* Does input \"7\" leads to a prediction of \"5\"?\n\n\n\n```{mermaid}\nflowchart TD\n\ninput1[8]\ninput2[6]\ninput3[7]\n\nneuron1[ReLU]\nneuron2[ReLU]\nneuron3[ReLU]\n\noutput1[NA]\noutput2[NA]\noutput3[6.2882]\n\ninput1 -- 0.20x + 0.86 --> neuron1\ninput2 -- 0.20x + 0.86 --> neuron2\ninput3 -- 0.20x + 0.86 --> neuron3\n\nneuron1 -- 0.91x - 0.25 --> output1\nneuron2 -- 0.91x - 0.25 --> output2\nneuron3 -- 0.91x - 0.25 --> output3\n\nneuron1 -- -0.85x + 0.81 --> neuron2\nneuron2 -- -0.85x + 0.81 --> neuron3\n```\n\n\n\n* SSR: 1.6595\n\n### 5\n\n* Does input \"5\" leads to a prediction of \"3\"?\n\n\n\n```{mermaid}\nflowchart TD\n\ninput1[8]\ninput2[6]\ninput3[7]\ninput4[5]\n\nneuron1[ReLU]\nneuron2[ReLU]\nneuron3[ReLU]\nneuron4[ReLU]\n\noutput1[NA]\noutput2[NA]\noutput3[NA]\noutput4[5.0798]\n\ninput1 -- 0.20x + 0.86 --> neuron1\ninput2 -- 0.20x + 0.86 --> neuron2\ninput3 -- 0.20x + 0.86 --> neuron3\ninput4 -- 0.20x + 0.86 --> neuron4\n\nneuron1 -- 0.91x - 0.25 --> output1\nneuron2 -- 0.91x - 0.25 --> output2\nneuron3 -- 0.91x - 0.25 --> output3\nneuron4 -- 0.91x - 0.25 --> output4\n\nneuron1 -- -0.85x + 0.81 --> neuron2\nneuron2 -- -0.85x + 0.81 --> neuron3\nneuron3 -- -0.85x + 0.81 --> neuron4\n```\n\n\n\n* SSR: 4.3257\n\n### 3\n\n* Does input \"3\" leads to a prediction of \"0\"?\n\n\n\n```{mermaid}\nflowchart TD\n\ninput1[8]\ninput2[6]\ninput3[7]\ninput4[5]\ninput5[3]\n\nneuron1[ReLU]\nneuron2[ReLU]\nneuron3[ReLU]\nneuron4[ReLU]\nneuron5[ReLU]\n\noutput1[NA]\noutput2[NA]\noutput3[NA]\noutput4[NA]\noutput5[5.7430]\n\ninput1 -- 0.20x + 0.86 --> neuron1\ninput2 -- 0.20x + 0.86 --> neuron2\ninput3 -- 0.20x + 0.86 --> neuron3\ninput4 -- 0.20x + 0.86 --> neuron4\ninput5 -- 0.20x + 0.86 --> neuron5\n\nneuron1 -- 0.91x - 0.25 --> output1\nneuron2 -- 0.91x - 0.25 --> output2\nneuron3 -- 0.91x - 0.25 --> output3\nneuron4 -- 0.91x - 0.25 --> output4\nneuron5 -- 0.91x - 0.25 --> output5\n\nneuron1 -- -0.85x + 0.81 --> neuron2\nneuron2 -- -0.85x + 0.81 --> neuron3\nneuron3 -- -0.85x + 0.81 --> neuron4\nneuron4 -- -0.85x + 0.81 --> neuron5\n```\n\n\n\n* SSR: 32.9815\n\n### 0\n\n* Does input \"0\" leads to a prediction of \"9\"?\n\n\n\n```{mermaid}\nflowchart TD\n\ninput1[8]\ninput2[6]\ninput3[7]\ninput4[5]\ninput5[3]\ninput6[0]\n\nneuron1[ReLU]\nneuron2[ReLU]\nneuron3[ReLU]\nneuron4[ReLU]\nneuron5[ReLU]\nneuron6[ReLU]\n\noutput1[NA]\noutput2[NA]\noutput3[NA]\noutput4[NA]\noutput5[NA]\noutput6[4.6333]\n\ninput1 -- 0.20x + 0.86 --> neuron1\ninput2 -- 0.20x + 0.86 --> neuron2\ninput3 -- 0.20x + 0.86 --> neuron3\ninput4 -- 0.20x + 0.86 --> neuron4\ninput5 -- 0.20x + 0.86 --> neuron5\ninput6 -- 0.20x + 0.86 --> neuron6\n\nneuron1 -- 0.91x - 0.25 --> output1\nneuron2 -- 0.91x - 0.25 --> output2\nneuron3 -- 0.91x - 0.25 --> output3\nneuron4 -- 0.91x - 0.25 --> output4\nneuron5 -- 0.91x - 0.25 --> output5\nneuron6 -- 0.91x - 0.25 --> output6\n\nneuron1 -- -0.85x + 0.81 --> neuron2\nneuron2 -- -0.85x + 0.81 --> neuron3\nneuron3 -- -0.85x + 0.81 --> neuron4\nneuron4 -- -0.85x + 0.81 --> neuron5\nneuron5 -- -0.85x + 0.81 --> neuron6\n```\n\n\n\n* SSR: 19.0682\n\n# Vanishing Gradients\n\nIn our recurrent neural network with\n\n* input layer size: 1\n* hidden layer size: 1\n* output layer size: 1\n\n\n\n```{mermaid}\nflowchart LR\n\ninput1[input]\n\nneuron1[neuron]\n\noutput1[output]\n\ninput1 -- w_i,h + b_i,h --> neuron1\nneuron1 -- w_h,0 + b_h,0 --> output1\nneuron1 -- w_h,h + b_h,h --> neuron1\n```\n\n\n\nwe have functions\n\n* $L_{i,h} = w_{i,h}x + b_{i,h}$\n* $L_{h,o} = w_{h,o}x + b_{h,o}$\n* $L_{h,h} = w_{h,h}x + b_{h,h}$\n\nand ReLU activation function\n\n$$\\text{ReLU}(x) = \\text{max}(x,0)$$\nwhose derivative is\n\n$$\\frac{d\\text{ReLU}}{dx} = \\begin{cases} 1, & x \\geq 0 \\\\ 0, & x < 0 \\end{cases}$$\n\n## Chain Rule\n\nToward back propagation, the derivative based on output $k$ given inputs $\\{1, 2, ..., k\\}$ is\n\n$$\\begin{array}{rcl}\n\\frac{\\partial\\text{SSR}}{\\partial x_{k}} & = & \\displaystyle\\sum_{a = 1}^{k}\n\\frac{\\partial\\text{SSR}}{\\partial L_{h,o}}\n\\left(\\displaystyle\\prod_{b=1}^{a-1}\n\\frac{\\partial L_{h,o}}{\\partial\\text{ReLU}}\\cdot\n\\frac{\\partial\\text{ReLU}}{\\partial L_{h,h}}\\cdot\n\\frac{\\partial L_{h,h}}{L_{i,h}}\n\\right)\n\\frac{\\partial L_{i,h}}{\\partial x_{k}} \\\\\n~ & = & \\displaystyle\\sum_{a = 1}^{k}\nw_{i,h}\n\\left(\\displaystyle\\prod_{b=1}^{a-1}\nw_{h,h}\\right)\nw_{h,0} \\\\\n\\end{array}$$\n\n## Example: 867-5309\n\nFor today's example, the derviative is\n\n$$\\frac{\\partial\\text{SSR}}{\\partial x_{k}} = \\sum_{a = 1}^{k}\n(0.20)\n\\left(\\prod_{b=1}^{a-1}\n(-0.85)\n\\right)\n(0.91)$$\n\n::: {.callout-tip}\n## Gradient Propagation\n\n* we have **vanishing gradients** if $|w_{h,h}| < 1$\n* we have **exploding gradients** if $|w_{h,h}| > 1$\n:::\n\n\n# Preview: Word Prediction\n\n\n\n```{mermaid}\nflowchart LR\n\ninput1[eggs]\ninput2[bread]\nstem1[pairs]\nstem2[well]\nstem3[with]\noutput1[bacon]\noutput2[butter]\n\ninput1 --> stem1\ninput2 --> stem1\nstem1 --> stem2\nstem2 --> stem3\nstem3 --> output1\nstem3 --> output2\n```\n\n\n\n* \"eggs pairs well with bacon\"\n* \"bread pairs well with butter\"\n* but how to represent words as numbers?\n\n\n# Derek's Research\n\n::::: {.panel-tabset}\n\n## Paper\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n![pedagogy research](MBVP SABER poster.png)\n:::\n\n::: {.column width=\"50%\"}\n- regression (LASSO penalization)\n- decision trees (XGBoost)\n- K nearest neighbors\n- neural networks\n:::\n\n::::\n\n## Poster\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n![topic modeling](topic_modeling.png)\n:::\n\n::: {.column width=\"50%\"}\n* topic modeling\n* PCA\n* image credit: [Joyce Xu](https://www.joycexu.io/2018/topic-modeling/)\n:::\n\n::::\n\n:::::\n\n\n# Quo Vadimus?\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n* due this Friday (March 21):\n\n    - Precept 6\n    - Pick project\n    - Art Images Use-Case Collection\n  \n\n    \n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n* due next Friday (March 28):\n\n    - Poster Feedback\n    - Precept 7\n    - Literature search\n:::\n\n::::\n\n\n# Footnotes\n\n::: {.callout-note collapse=\"true\"}\n\n## (optional) Additional Resources and References\n\n* derivation of the [derivative of cross entropy](https://shivammehta25.github.io/posts/deriving-categorical-cross-entropy-and-softmax/) by Shivam Mehta\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.3      forcats_1.0.0        stringr_1.5.1       \n [4] dplyr_1.1.4          purrr_1.0.4          readr_2.1.5         \n [7] tidyr_1.3.1          tibble_3.2.1         ggplot2_3.5.1       \n[10] tidyverse_2.0.0      patchwork_1.3.0      palmerpenguins_0.1.1\n\nloaded via a namespace (and not attached):\n [1] Matrix_1.7-1      gtable_0.3.6      jsonlite_1.8.9    compiler_4.4.2   \n [5] Rcpp_1.0.12       tidyselect_1.2.1  png_0.1-8         scales_1.3.0     \n [9] yaml_2.3.10       fastmap_1.2.0     lattice_0.22-6    reticulate_1.38.0\n[13] R6_2.5.1          generics_0.1.3    knitr_1.49        htmlwidgets_1.6.4\n[17] munsell_0.5.1     pillar_1.10.1     tzdb_0.4.0        rlang_1.1.5      \n[21] stringi_1.8.4     xfun_0.50         timechange_0.3.0  cli_3.6.3        \n[25] withr_3.0.2       magrittr_2.0.3    digest_0.6.37     grid_4.4.2       \n[29] rstudioapi_0.17.1 hms_1.1.3         lifecycle_1.0.4   vctrs_0.6.5      \n[33] evaluate_1.0.3    glue_1.8.0        farver_2.1.2      colorspace_2.1-1 \n[37] rmarkdown_2.29    tools_4.4.2       pkgconfig_2.0.3   htmltools_0.5.8.1\n```\n\n\n:::\n:::\n\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}