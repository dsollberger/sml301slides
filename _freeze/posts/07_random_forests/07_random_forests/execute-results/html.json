{
  "hash": "cc4ee5e825e0a794d7111666cb2197be",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"7: Random Forests\"\nauthor: \"Derek Sollberger\"\ndate: \"2025-02-17\"\nformat:\n  html:\n    toc: true\n---\n\n::: {.cell}\n\n:::\n\n\n\n# Session 7: Random Forests\n\n## Learning objectives:\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- Deploy many decision trees\n- Explore ensemble learning:\n\n    - bagging\n    - boosting\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n:::\n\n::::\n\n\n# Bagging\n\n## Bootstrapping\n\n**Bootstrapping** is performed by *resampling with replacement* on the data.\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 9\n     pid species island    bill_length_mm bill_depth_mm flipper_length_mm\n   <int> <fct>   <fct>              <dbl>         <dbl>             <int>\n 1     1 Adelie  Torgersen           39.1          18.7               181\n 2     2 Adelie  Torgersen           39.5          17.4               186\n 3     3 Adelie  Torgersen           40.3          18                 195\n 4     4 Adelie  Torgersen           NA            NA                  NA\n 5     5 Adelie  Torgersen           36.7          19.3               193\n 6     6 Adelie  Torgersen           39.3          20.6               190\n 7     7 Adelie  Torgersen           38.9          17.8               181\n 8     8 Adelie  Torgersen           39.2          19.6               195\n 9     9 Adelie  Torgersen           34.1          18.1               193\n10    10 Adelie  Torgersen           42            20.2               190\n# ℹ 3 more variables: body_mass_g <int>, sex <fct>, year <int>\n```\n\n\n:::\n:::\n\n\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 9\n     pid species   island    bill_length_mm bill_depth_mm flipper_length_mm\n   <int> <fct>     <fct>              <dbl>         <dbl>             <int>\n 1   220 Gentoo    Biscoe              49.5          16.2               229\n 2   141 Adelie    Dream               40.2          17.1               193\n 3   261 Gentoo    Biscoe              43.3          14                 208\n 4   313 Chinstrap Dream               47.6          18.3               195\n 5   261 Gentoo    Biscoe              43.3          14                 208\n 6    47 Adelie    Dream               41.1          19                 182\n 7   282 Chinstrap Dream               45.2          17.8               198\n 8   308 Chinstrap Dream               54.2          20.8               201\n 9     9 Adelie    Torgersen           34.1          18.1               193\n10   127 Adelie    Torgersen           38.8          17.6               191\n# ℹ 3 more variables: body_mass_g <int>, sex <fct>, year <int>\n```\n\n\n:::\n:::\n\n\n:::\n\n::::\n\n> **Bagging** is using the notion of **b**ootstrapping to **agg**regate the data.\n\n## Bags\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n## In-Bag\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  [1]   3   5   7   9  10  11  12  13  14  15  16  22  23  25  26  28  30  32\n [19]  38  41  42  43  47  48  49  51  52  54  55  58  60  63  65  66  68  69\n [37]  70  72  73  75  77  79  80  81  83  84  85  86  87  88  91  94  95  97\n [55]  98  99 100 101 102 104 105 106 107 108 109 111 112 113 116 117 118 119\n [73] 121 122 125 127 128 129 130 131 132 133 134 138 139 140 141 142 143 146\n [91] 150 152 153 155 156 157 158 159 162 167 168 169 170 173 174 175 177 179\n[109] 181 182 183 184 185 186 187 189 192 193 196 199 200 201 202 203 204 206\n[127] 207 208 209 210 211 212 213 215 216 217 219 220 221 223 225 226 227 228\n[145] 229 231 233 235 236 237 238 239 240 241 242 243 244 246 247 250 251 252\n[163] 253 254 255 258 259 260 261 262 263 265 267 269 270 271 272 273 274 275\n[181] 277 278 282 283 284 285 286 287 288 289 292 294 295 296 297 298 300 301\n[199] 302 303 305 306 307 308 309 310 311 313 314 317 318 319 320 323 324 326\n[217] 327 328 329 330 331 332 333 334 335 337 338 339 340 343 344\n```\n\n\n:::\n:::\n\n\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n## Out-of-Bag\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  [1]   1   2   4   6   8  17  18  19  20  21  24  27  29  31  33  34  35  36\n [19]  37  39  40  44  45  46  50  53  56  57  59  61  62  64  67  71  74  76\n [37]  78  82  89  90  92  93  96 103 110 114 115 120 123 124 126 135 136 137\n [55] 144 145 147 148 149 151 154 160 161 163 164 165 166 171 172 176 178 180\n [73] 188 190 191 194 195 197 198 205 214 218 222 224 230 232 234 245 248 249\n [91] 256 257 264 266 268 276 279 280 281 290 291 293 299 304 312 315 316 321\n[109] 322 325 336 341 342\n```\n\n\n:::\n:::\n\n\n:::\n\n::::\n\n* In general, about 1/3 of the data falls into the *out-of-bag* category\n* For each resampling, we can create a decision tree with the *in-bag* sample\n* We can use the out-of-bag data as a testing set\n\n    * **out-of-bag error** (OOB error)\n\n## Classfication Tasks\n\nFor classification tasks, after making a random forest (say, 1000 trees), class labels are assigned by the majority of predictions.\n\n::: {.callout-tip collapse=\"true\"}\n## Variable Importance\n\nSince we have created many trees---each choice order determined by entropy---we can create a list of *variable importance* by reporting which explanatory variables appeared in a higher proportion of trees.  This could aid in variable selection and interpretability.\n\n:::\n\n\n# Activity: Literature Methods\n\nWe will look at the concluding paragraphs for some of the most influential papers in the history of machine learning:\n\n## Adaboost\n\n* A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting (1997—published as abstract in 1995), Freund and Schapire\n\n## AlexNet\n\n* ImageNet Classification with Deep Convolutional Neural Networks (2012)\n\n## DropOut\n\n* Dropout: A Simple Way to Prevent Neural Networks from Overfitting (2014), Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov\n\n## GANs\n\n* General Adversarial Nets (2014), Goodfellow et al.\n\n## TensorFlow\n\n* TensorFlow: A system for large-scale machine learning (2016), Abadi et al.\n\n## Word2Vec\n\n* Efficient Estimation of Word Representations in Vector Space (2013), Mikolov, Chen, Corrado, and Dean\n\n\n# Boosting\n\n## Stratified Samples\n\n> For **stratified sampling**, subsets maintain proportions of categorical data.\n\nFor example, 88 percent of people are right-handed. We assume population proportions\n\n$$p = 0.88, \\quad 1 - p = 0.12$$\n\nIf we employ a training-testing split, each should also have approximately 12 percent representation for left-handed people.\n\n## Rescaling\n\n> If a sample data set exhibits different proportions, we can perform **inverse probability weighting** to try to correct for bias in the sample.\n\nFor example, if our tree model predicts 70 percent right-handed people, then we can apply some weights\n\n* on right-handed: weight = $\\frac{0.88}{0.70} \\approx 1.2571$\n* on left-handed: weight = $\\frac{0.12}{0.30} = 0.4$\n\n## Boosting\n\nFor tree models, **boosting** resamples underrepresented data and applies larger weights to aim toward stratified sampling.\n\n\n# Comparing Methods\n\n![bagging vs boosting](bagging_boosting.png)\n\n* image credit: Roshmita Dey\n\n\n\n\n\n\n\n# Quo Vadimus?\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n* due this Friday (5 PM):\n\n    - Precept 4\n    - Literature Report\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"50%\"}\n\n:::\n\n::::\n\n\n# Footnotes\n\n::: {.callout-note collapse=\"true\"}\n\n## (optional) Additional Resources and References\n\n* [Bagging vs Boosting](https://medium.com/@roshmitadey/bagging-v-s-boosting-be765c970fd1) by Roshmita Dey\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.3      forcats_1.0.0        stringr_1.5.1       \n [4] dplyr_1.1.4          purrr_1.0.4          readr_2.1.5         \n [7] tidyr_1.3.1          tibble_3.2.1         ggplot2_3.5.1       \n[10] tidyverse_2.0.0      palmerpenguins_0.1.1\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6      jsonlite_1.8.9    compiler_4.4.2    tidyselect_1.2.1 \n [5] scales_1.3.0      yaml_2.3.10       fastmap_1.2.0     R6_2.5.1         \n [9] generics_0.1.3    knitr_1.49        htmlwidgets_1.6.4 munsell_0.5.1    \n[13] pillar_1.10.1     tzdb_0.4.0        rlang_1.1.5       utf8_1.2.4       \n[17] stringi_1.8.4     xfun_0.50         timechange_0.3.0  cli_3.6.3        \n[21] withr_3.0.2       magrittr_2.0.3    digest_0.6.37     grid_4.4.2       \n[25] rstudioapi_0.17.1 hms_1.1.3         lifecycle_1.0.4   vctrs_0.6.5      \n[29] evaluate_1.0.3    glue_1.8.0        colorspace_2.1-1  rmarkdown_2.29   \n[33] tools_4.4.2       pkgconfig_2.0.3   htmltools_0.5.8.1\n```\n\n\n:::\n:::\n\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}