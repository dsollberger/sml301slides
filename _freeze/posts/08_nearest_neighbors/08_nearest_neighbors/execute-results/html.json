{
  "hash": "3566217b0592b4bf6b35914953bc2b43",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"8: Nearest Neighbors\"\nauthor: \"Derek Sollberger\"\ndate: \"2025-09-29\"\nformat:\n  html:\n    toc: true\n---\n\n::: {.cell}\n\n:::\n\n\n\n# Session 8: Nearest Neighbors\n\n## Learning objectives:\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- (unsupervised) clustering\n- (supervised) KNN\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n![clustering](Clustering_Data.png)\n\n* image source: [Grammarly](https://www.grammarly.com/blog/ai/what-is-clustering/)\n:::\n\n::::\n\n# Clustering\n\n::: {.callout-note}\n# Clustering\n\n**Clustering** is a technique in *unsupervised learning* that seeks to find structure in unlabeled data.\n\n:::\n\n![clustering](Clustering_Data.png)\n\n* image source: [Grammarly](https://www.grammarly.com/blog/ai/what-is-clustering/)\n\n## Centers\n\n![clustering centers](clustering_centers.png)\n\n* image source: [Paul van der Laken](https://paulvanderlaken.com/2018/12/12/visualizing-the-inner-workings-of-the-k-means-clustering-algorithm/)\n\n## Metric\n\n![within sum of squares](WSS.png)\n\n* image source: [Manik Soni](https://medium.com/swlh/how-to-choose-the-right-number-of-clusters-in-the-k-means-algorithm-9160c57ec760)\n\n::: {.callout-note}\n## SSE Ratio\n\nIdeally, the \"best\" clustering arrangement yields a \"small\" sum-of-squared errors ratio:\n\n$$\\text{SSE Ratio} = \\frac{\\text{total within sum of squares}}{\\text{overall sum of squares}}$$\n* but the number of clusters itself should be \"small\" to aid *interpretability*\n\n:::\n\n::: {.callout-warning}\n## How do we find the optimal clustering?\n:::\n\n## Lloyd's Algorithm\n\n1. pick centers, determine clusters\n2. use cluster means as new centers\n3. (repeat until no change in membership)\n\n![Lloyd's Algorithm](LloydsMethod.png)\n\n* image source: [Wikipedia](https://en.wikipedia.org/wiki/Lloyd%27s_algorithm)\n\n::: {.callout-warning}\n## speed versus complexity tradeoff\n\n:::\n\n::: {.callout-warning}\n# DCP1\n:::\n\n# Parameter Selection\n\nHow do we choose an optimal amount of clusters?\n\n## Scree Plot\n\n![elbow method](clustering_elbow_method.png)\n\n* image source: [Sophie Su](https://sophiesu.net/ch3-1-clustering-machine-learning-with-tensorflow-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/)\n\n::: {.callout-tip}\n## Scree plots are also used with PCA\n\n:::\n\n## Silhouette Analysis\n\n![silhouette analysis](sihouette_analysis.png)\n\n* image source: [Mukesh Chaudhary](https://medium.com/@cmukesh8688/silhouette-analysis-in-k-means-clustering-cefa9a7ad111)\n\n# Modern Approaches\n\n::::: {.panel-tabset}\n\n## batches\n\nTo handle big data jobs, we deploy **batch processing** in the data engineering.\n\n![batch processing](batch_processing.png)\n\n* image source: [Ayar Labs](https://ayarlabs.com/glossary/batch-inference/)\n\n## DBSCAN\n\n\"[Density-Based Spatial Clustering of Applications with Noise](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html). Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density.\"\n\n![DBSCAN](DBSCAN_example.png)\n\n* image source: [Chris Wernst](https://github.com/chriswernst/dbscan-python)\n\n## hierarchical\n\n![hierarchical data](spotify-hierarchical-data-diagram.png)\n\n* image source: [Bayes Rules!](https://www.bayesrulesbook.com/chapter-16)\n\n:::::\n\n::: {.callout-warning}\n# DCP2\n:::\n\n# KNN\n\n## Nearest Neighbors\n\n![KNN example](KNN_example.png)\n\n* image source: [JC Chouinard](https://www.jcchouinard.com/k-nearest-neighbors/dist)\n\n* credited to Evelyn Fix and Joseph L Hodges\n\n## Distances\n\nMathematically, a **natural norm** satisfies the following conditions between any two points\n\n1. reflexive: $d(x,y) \\geq 0$\n2. symmetric: $d(x,y) = d(y,x)$\n3. triangle inequality:\n\n$$d(x,y) \\leq d(x,z) + d(z,y)$$\n\n![types of distances](distances_variety.png)\n\n* image source: [eforebrahim](https://www.reddit.com/r/learnmachinelearning/comments/v13zuw/different_types_of_distances_used_in_ml/)\n\n::: {.callout-note}\n# Minkowski Norm\n\nThe nearest neighbors algorithms in scikit-learn by default use the Minkowski norm with $p=2$ (i.e. Euclidean norm).\n\n$$d(x,y) = \\left[\\sum_{i=1}^{n} (x_{i} - y_{i})^{p}\\right]^{1/p}$$\n\n* $p=1$: Manhattan norm (think: grid of city streets)\n\n$$d(x,y) = \\sum_{i=1}^{n} |x_{i} - y_{i}|$$\n\n* $p=2$: Euclidean norm (from elementary geometry)\n\n$$d(x,y) = \\sqrt{\\sum_{i=1}^{n} (x_{i} - y_{i})^{2}}$$\n\n* $p \\rightarrow \\infty$: Chebychev norm\n\n$$d(x,y) = \\text{max}_{i} |x_{i} - y_{i}|$$\n\n:::\n\n::: {.callout-warning}\n# DCP3\n:::\n\n\n# KNN Imputation\n\nRather than using summary statistics (e.g. mean or median) across the entire data sample, ML analysts recommend using KNN to impute some missing values.\n\n![KNN imputation](KNN_imputation.png)\n\n* image source: [Adrienne Kline](https://towardsdatascience.com/implementation-and-limitations-of-imputation-methods-b6576bf31a6c/)\n\n\n# Curse of Dimensionality\n\n::: {.callout-warning}\n## Curse of Dimensionality\n\nDistance-based algorithms tend to lose usefulness with high-dimensional data due to the **curse of dimensionality**\n\n* coined by Richard Bellman\n:::\n\n![curse of dimensionality](KNN_curse_of_dimensionality.png)\n\n* image source: [Gokcenaz Akyol](https://medium.com/@gokcenazakyol/what-is-curse-of-dimensionality-machine-learning-2-739131962faf)\n\n::: {.callout-tip}\n## On Dimensionality\n\nTo perhaps counter the curse of dimensionality\n\n* dimension reduction (e.g. PCA, LDA)\n* regularization (e.g. Ridge, LASSO)\n* parsimony (i.e. using fewer neighbors)\n:::\n\n# Approximate KNN\n\nApproximate KNN partitions the vector space so that searches takes place only over smaller batches.\n\n![approximate KNN](KNN_ANN.png)\n\n* image source: [Jeremy Jordan](https://www.jeremyjordan.me/scaling-nearest-neighbors-search-with-approximate-methods/)\n\n\n\n# Quo Vadimus?\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n* due this Friday:\n\n    - Precept 5\n    - CuriosityMid (survey)\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"50%\"}\nMidsemester Project\n\n* due Oct 10:\n\n  * report\n  * poster\n  * video\n\n:::\n\n::::\n\n\n# Footnotes\n\n::: {.callout-note collapse=\"true\"}\n\n## (optional) Additional Resources and References\n\n* [clustering with scikit-learn](https://programminghistorian.org/en/lessons/clustering-with-scikit-learn-in-python) by the Programming Historian\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.5.1 (2025-06-13 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n  LAPACK version 3.12.1\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.5.1    fastmap_1.2.0     cli_3.6.5        \n [5] tools_4.5.1       htmltools_0.5.8.1 rstudioapi_0.17.1 yaml_2.3.10      \n [9] rmarkdown_2.29    knitr_1.50        jsonlite_2.0.0    xfun_0.52        \n[13] digest_0.6.37     rlang_1.1.6       evaluate_1.0.4   \n```\n\n\n:::\n:::\n\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}