{
  "hash": "7e7b9b1784d6f1f61a0a7bf6fcd326b5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"21: RAG\"\nauthor: \"Derek Sollberger\"\ndate: \"2025-11-17\"\nformat:\n  html:\n    toc: true\n---\n\n\n\n\n# 21: Retrieval Augmentation Generation\n\n## Learning Objectives\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n- few-shot prompting\n- chain-of-thought\n- intro retrieval augmented generation\n:::\n\n::: {.column width=\"10%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n![RAG applications](Applications-of-Retrieval-Augmented-Generation.png)\n\n* image source: [SoluLab](https://www.solulab.com/what-is-retrieval-augmented-generation/)\n\n:::\n\n::::\n\n\n# Token Cost\n\n::: {.callout-note}\n## Tokens\n\n\"**Tokens** are words, character sets, or combinations of words and punctuation that are generated by large language models (LLMs) when they decompose text. Tokenization is the first step in training. The LLM analyzes the semantic relationships between tokens, such as how commonly they're used together or whether they're used in similar contexts. After training, the LLM uses those patterns and relationships to generate a sequence of output tokens based on the input sequence.\"\n---[Microsoft Learn](https://learn.microsoft.com/en-us/dotnet/ai/conceptual/understanding-tokens)\n:::\n\n## Tokenization\n\n![tokenization](tokenization_examples.png)\n\n* app: [TikTokenizer](https://tiktokenizer.vercel.app/)\n\n## Cost Comparison\n\n![comparing the major LLMs](gpt_claude2_pricing2.png)\n\n* image source: [Merlin Tech](https://www.merlin.tech/llm-costs/), August 28, 2023\n\n## Economies of Scale\n\n![token cost over time](token_cost_over_time.png)\n\n* image source: [Marginal Revolution](https://marginalrevolution.com/marginalrevolution/2024/08/the-falling-cost-of-tokens.html), August 25, 2024\n\n## Prompt Complexity versus Cost\n\n![fine tuning and costs](fine_tuning_cost.png)\n\n* image source: [Mary Schwaber](https://medium.com/@mary.schwaber/few-shot-prompting-vs-fine-tuning-a-cost-efficiency-perspective-5bf00257c3e8)\n\n::: {.callout-warning}\n## DCP1\n:::\n\n# Prompt Augmentation\n\n## In-Context Learning\n\n![in-context learning](In_context_learning.png)\n\n* image source: [PromptHub](https://www.prompthub.us/blog/in-context-learning-guide)\n\n## Chain Prompting\n\n![chain prompting](prompt-chaining-example.png)\n\n* image source: [Gadesha and Kavlakoglu](https://www.ibm.com/think/topics/prompt-chaining)\n\n## Chain of Thought\n\n![chain of thought](chain_of_thought.png)\n\n* image source: [Wei et al](https://arxiv.org/abs/2201.11903)\n\n## Tree of Thought\n\n![tree of thought](tree_of_thought.png)\n\n* image source: [Technische Hochschule Augsburg](https://www.tha.de/en/Types-of-prompting.html)\n\n::: {.callout-warning}\n## DCP2\n:::\n\n# Data Ethics: Social Engineering\n\n::::: {.panel-tabset}\n\n## games\n\n![Whispers from the Star](Whispers_splash.png)\n\n* video game\n* released August 14, 2025\n\n## immersive\n\n![voice communication with AI](Whispers_dialogue.png)\n\n\"We can video call, voice message, or text, and it's all powered by AI-driven dialogue. Right now, you’re the only one I’ve got.\"\n\n## controversy\n\n![asking personal questions](Whispers_meme.png)\n\n* reviewers noted how AI-powered conversations were [co-created with human actors](https://www.notebookcheck.net/Steam-launch-New-interactive-fiction-game-debuts-to-Very-Positive-reviews-may-divide-gamers-over-heavy-AI-integration.1087821.0.html)\n* some [gamers](https://youtu.be/ddbXNHTsKCs?si=HAqjLa7wb8Z5elC0&t=5910) claimed that there was [data harvesting](https://steamcommunity.com/app/3730100/discussions/0/595158249006738933/)\n* producers did revise their [privacy policy](https://www.anuttacon.com/legal/privacy/)\n\n## obituaries\n\n![preserving dead relatives](cheap-ai-clones-dead-relatives.png)\n\n* image source: [Futurism](https://futurism.com/the-byte/cheap-ai-clones-dead-relatives)\n\n## scams\n\n![using dead relatives](ai-obit-scams.png)\n\n* image source: [Blackbird AI](https://blackbird.ai/blog/ai-powered-obituary-scams-targeted-phishing/)\n\n:::::\n\n\n::: {.callout-warning}\n## DCP3\n:::\n\n\n# Retrieval Augmented Generation\n\n::: {.callout-note}\n## RAG\n\n\"**Retrieval-Augmented Generation** (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts.\" --- [AWS](https://aws.amazon.com/what-is/retrieval-augmented-generation/)\n:::\n\n::: {.callout-tip}\n## RAG\n\nInformally, it is said that **Retrieval-Augmented Generation** (RAG) allows users to have an LLM assist in reading a PDF.\n:::\n\n## Example: Film Review\n\nWe will look at an example from [Chapter 8](https://github.com/HandsOnLLM/Hands-On-Large-Language-Models?tab=readme-ov-file) of *Hands-On Large Language Models by Jay Alammar and Maarten Grootenhorst\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n![Interstellar](Interstellar_movie_poster.png)\n\n* image source: [Posterazzi](https://www.posterazzi.com/interstellar-movie-poster-print-27-x-40-item-movab14245/)\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"60%\"}\n> Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan.\nIt stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine.\nSet in a dystopian future where humanity is struggling to survive, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for mankind.\nBrothers Christopher and Jonathan Nolan wrote the screenplay, which had its origins in a script Jonathan developed in 2007.\nCaltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar.\nCinematographer Hoyte van Hoytema shot it on 35 mm movie film in the Panavision anamorphic format and IMAX 70 mm.\nPrincipal photography began in late 2013 and took place in Alberta, Iceland, and Los Angeles.\nInterstellar uses extensive practical and miniature effects and the company Double Negative created additional digital effects.\nInterstellar premiered on October 26, 2014, in Los Angeles.\nIn the United States, it was first released on film stock, expanding to venues using digital projectors.\nThe film had a worldwide gross over $677 million (and $773 million with subsequent re-releases), making it the tenth-highest grossing film of 2014.\nIt received acclaim for its performances, direction, screenplay, musical score, visual effects, ambition, themes, and emotional weight.\nIt has also received praise from many astronomers for its scientific accuracy and portrayal of theoretical astrophysics. Since its premiere, Interstellar gained a cult following,[5] and now is regarded by many sci-fi experts as one of the best science-fiction films of all time.\nInterstellar was nominated for five awards at the 87th Academy Awards, winning Best Visual Effects, and received numerous other accolades\n\n--- [Wikipedia](https://en.wikipedia.org/wiki/Interstellar_(film))\n:::\n\n::::\n\nExperiment: \n\n* treat each sentence as a separate *document* as part of a *corpus*\n* build search database with `faiss`\n* **rerank** with [BM25](https://en.wikipedia.org/wiki/Okapi_BM25) search algorithm\n\n\n\n\n# Quo Vadimus?\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n* due this Friday (Nov 21):\n\n    - **Precept 10** (2 hours)\n    - surveys:\n        - **Participant Informed Consent** (5 minutes)\n        - **Research Consent** (5 minutes)\n    - group work:\n        - **Abstract (draft)** (30 minutes)\n\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"30%\"}\n\nOur SML 301 lecture session for **Monday, November 24** will be remote (Zoom).\n\n* guest speaker from AI industry\n\n:::\n\n::::\n\n\n# Footnotes\n\n::: {.callout-note collapse=\"true\"}\n\n## (optional) Additional Resources and References\n\n* [LLM strategies](https://towardsdatascience.com/decoding-strategies-in-large-language-models-9733a8f70539/)\n* [Prompting Guide](https://www.promptingguide.ai/techniques)\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.5.1 (2025-06-13 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n  LAPACK version 3.12.1\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.5.1    fastmap_1.2.0     cli_3.6.5        \n [5] tools_4.5.1       htmltools_0.5.8.1 rstudioapi_0.17.1 yaml_2.3.10      \n [9] rmarkdown_2.29    knitr_1.50        jsonlite_2.0.0    xfun_0.52        \n[13] digest_0.6.37     rlang_1.1.6       evaluate_1.0.4   \n```\n\n\n:::\n:::\n\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\t\n:::\n\n::: {.column width=\"10%\"}\n\t\n:::\n\n::: {.column width=\"45%\"}\n\n:::\n\n::::\n\n::::: {.panel-tabset}\n\n\n\n:::::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}