---
title: "18: Parsing"
author: "Derek Sollberger"
date: "2025-04-02"
format:
  html:
    toc: true
---

```{r}
#| echo: false
#| message: false
#| warning: false

```

# Session 18: Parsing

## Learning objectives:

:::: {.columns}

::: {.column width="45%"}
- Continue word embedding
- Explore tokenization
- Introduce computer vision
:::

::: {.column width="10%"}

:::

::: {.column width="45%"}
![301](ishihara_example_301.png)
:::

::::

# Word Embedding

## Linear Relationships

:::: {.columns}

::: {.column width="50%"}
$$w_{\text{king}}	- w_{\text{man}} + w_{\text{woman}} \approx w_{\text{queen}}$$
* explore in this [app](https://dash.gallery/dash-word-arithmetic/)!

:::

::: {.column width="10%"}
	
:::

::: {.column width="40%"}

![word vector space](word_vector_network.png)

* image credit: [plotly](https://medium.com/plotly/understanding-word-embedding-arithmetic-why-theres-no-single-answer-to-king-man-woman-cd2760e2cb7f)

:::

::::


![word vector spaces](linear-relationships_orig.png)

* image credit: [Fabian Pfurtscheller](https://www.askyourdata.co/blog/an-introduction-to-natural-language-processing-nlp)


# Tokenization

Andrej Karpathy inspired many machine learning practioners to think about tokenization with the following open problems:

Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.

- Why can't LLM spell words? **Tokenization**.
- Why can't LLM do super simple string processing tasks like reversing a string? **Tokenization**.
- Why is LLM worse at non-English languages (e.g. Japanese)? **Tokenization**.
- Why is LLM bad at simple arithmetic? **Tokenization**.
- Why did GPT-2 have more than necessary trouble coding in Python? **Tokenization**.
- Why did my LLM abruptly halt when it sees the string "<|endoftext|>"? **Tokenization**.
- What is this weird warning I get about a "trailing whitespace"? **Tokenization**.
- Why the LLM break if I ask it about "SolidGoldMagikarp"? **Tokenization**.
- Why should I prefer to use YAML over JSON with LLMs? **Tokenization**.
- Why is LLM not actually end-to-end language modeling? **Tokenization**.

To explore **tokenization**, let us try out the following passages in the [Tiktokenizer app](https://tiktokenizer.vercel.app/) (as seen in Andrej Karpathy's videos):

```
Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.

127 + 677 = 804
1275 + 6773 = 8041

Egg.
I have an Egg.
egg.
EGG.

만나서 반가워요. 저는 OpenAI에서 개발한 대규모 언어 모델인 ChatGPT입니다. 궁금한 것이 있으시면 무엇이든 물어보세요.

for i in range(1, 101):
    if i % 3 == 0 and i % 5 == 0:
        print("FizzBuzz")
    elif i % 3 == 0:
        print("Fizz")
    elif i % 5 == 0:
        print("Buzz")
    else:
        print(i)
```

## Byte Pair Encoding

To compress long text passages, we can employ [byte pair encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding)


# Computer Vision

In this course, we are exploring images in the Ishihara cards (to test for colorblindness).

![301](ishihara_example_301.png)

## Bootstrapping

If we don't have enough images to easily train a convolutional neural network, we can synthetically increase the number of image files by **bootstrapping** the original images with minor alterations such as

* *rotations*
* *reflections*

and we might also want to *resize* the images while drafting a neural net workflow.

# Quo Vadimus?

:::: {.columns}

::: {.column width="45%"}
* due this Friday (April 4):

    - Precept 8
    - Literature Review
  
:::

::: {.column width="10%"}
	
:::

::: {.column width="45%"}

:::

::::


# Footnotes

::: {.callout-note collapse="true"}

## (optional) Additional Resources and References

* [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE) by Andrej Karpathy

    * [notebook](https://colab.research.google.com/drive/1y0KnCFZvGVf_odSfcNAws6kcDD7HsI0L?usp=sharing)

:::

::: {.callout-note collapse="true"}
## Session Info

```{r}
sessionInfo()
```
:::


:::: {.columns}

::: {.column width="45%"}
	
:::

::: {.column width="10%"}
	
:::

::: {.column width="45%"}

:::

::::

::::: {.panel-tabset}



:::::