---
title: "3: Convergence"
author: "Derek Sollberger"
date: "2025-09-10"
format:
  html:
    toc: true
---

```{r}
#| echo: false
#| message: false
#| warning: false
library("gt")
library("tidyverse")
```

# Session 3: Convergence

## Start

:::: {.columns}

::: {.column width="60%"}
* **Goal**: Preview major concepts

* **Objective**: Explore some Python codes about root finding and stochastic processes
:::

::: {.column width="40%"}
![bias-variance tradeoff](biasvariance.png)

image source: [Scott Fortmann-Roe ](http://scott.fortmann-roe.com/docs/BiasVariance.html)
:::

::::

# SGD

Eventually we will discuss upon how neural network weights are found via stochastic gradient descent

* stochastic?
* gradient?
* descent?

# Little Perturbations

::::: {.panel-tabset}

## rounding

:::: {.columns}

::: {.column width="45%"}
### 2 + 2 = 5

... for extremely large values of 2
:::

::: {.column width="10%"}
	
:::

::: {.column width="45%"}
![tshirt](2_2_5.png)
:::

::::

## truncation

![truncation error](truncation-errors.png)

* image source: [BYJUs](https://byjus.com/maths/truncation-errors/)

## consideration

Numerical analysis: many computational algorithms have been designed to mitigate the possible propogation of rounding and truncation errors.

* example: [the quadratic formula](https://zingale.github.io/comp_astro_tutorial/basics/floating-point/numerical_error.html)

:::::

::: {.callout-caution collapse="true"}
## Dev Corner: Binary

Research teams are still affected by these rounding and truncation errors

* different operating systems (Mac, Windows, Linux)
* different processor chips (Intel, AMD)

Tip: data engineers can store computation results in their pure binary representations

:::

# Literature

We will spend some time discussing what makes for a great research paper.  There are many suggestions available, and for brevity, we will focus on these common sections:

* abstract and introduction
* conclusion
* methods
* results

## Activity: Literature Introductions

We will look at the abstracts and introduction paragraphs for some of the [most influential papers](https://github.com/daturkel/learning-papers?tab=readme-ov-file) in the history of machine learning:

::: {.callout-tip}
## Suggested Papers

### AlexNet

* ImageNet Classification with Deep Convolutional Neural Networks (2012)

### GANs

* General Adversarial Nets (2014), Goodfellow et al.

### TensorFlow

* TensorFlow: A system for large-scale machine learning (2016), Abadi et al.

### Word2Vec

* Efficient Estimation of Word Representations in Vector Space (2013), Mikolov, Chen, Corrado, and Dean
:::

::: {.callout-warning}
## DCP1
:::


# Stochastic

* **Stochastic**: word of Greek origin, meaning to guess at

## Sequences

* index: $n \in \mathbb{N} = \{1, 2, 3, 4, 5, ...\}$
* formulaic: f(n) = 2n - 1

$$1, 3, 5, 7, 9, ...$$

* constructive:

$$3, 3.1, 3.14, 3.141, 3.1415, 3.14159, ...$$

* shapes:

![triangular numbers](triangular-numbers.png)

* image source: [BYJUs](https://byjus.com/maths/triangular-numbers/)


## Random Variables

A **random variable** has no set value, but rather represents an element of chance.  We can better understand a random variable through statistics like

* mean
* variance
* distribution

::: {.callout-tip collapse="true"}
## Stochastic Process

A **stochastic process** is a *sequence* of *random variables*
:::


# Gradient

For a multivariate function $f(\vec{x})$, the **gradient** is a vector of partial derivatives

$$\nabla f = \left[ \frac{\partial}{\partial x_{1}}, \frac{\partial}{\partial x_{2}}, \frac{\partial}{\partial x_{3}}, ..., \frac{\partial}{\partial x_{n}}  \right]$$


# Descent

::::: {.panel-tabset}

## Scenario

Find the roots of the function

$$f(x) = x^{3} - 12x^{2} + 44x - 48$$

In other words, find the solutions of the equation

$$f(x) = x^{3} - 12x^{2} + 44x - 48 = 0$$

## Bisection

![Bisection Method](bisection_method.png)

* image source: [Python Numerical Methods](https://pythonnumericalmethods.studentorg.berkeley.edu/notebooks/chapter19.03-Bisection-Method.html)

## code

```
def bisection_method(f, a, b, tol):
  # tries to find a root of f between a and b (i.e. f(root) = 0)
  # Inputs: real numbers a, b; tolerance
  # Output: root

  # check if root is in the interval (i.e. Intermediate Value Theorem)
    if np.sign(f(a)) == np.sign(f(b)):
        raise Exception("The endpoints do not contain a root")

  # initialization
    iter_num = 1

    while (b - a) > tol:
      c = (a + b) / 2

      print("iter_num: " + str(iter_num) + ", midpoint: " + str(c))

      if f(c) * f(a) < 0:
          b = c
      else:
          a = c

      iter_num += 1

    print("Root:", c)

```

```
f = lambda x: x**3 - 12*x**2  + 44*x - 48
```


Try calling the `bisection_method` with different initial values for a and b, such as

*    [0, 3]
*    [3, 7]
*    [5, 10]

## Newton's

![Newton's Method](Newtons_Method.png)

* image source: [Paul's Online Notes](https://tutorial.math.lamar.edu/classes/calci/newtonsmethod.aspx)

## code

```
def Newton_Method(f, f_prime, x_0, tol, max_iter):
  # tries to find a root of f between a and b (i.e. f(root) = 0)
  # Inputs: function f, derivative function f_prime,
  # initial guess x_0, tolerance tol, and maximum number of iterations
  # Output: root

  # initialization
    iter_num = 1
    x_n = x_0

    while (abs(f(x_n)) > tol) & (iter_num <= max_iter):
      print("iter_num: " + str(iter_num) + ", guess: " + str(x_n))
      f_x = f(x_n)
      f_prime_x = f_prime(x_n)

      x_n = x_n - f_x / f_prime_x
      iter_num += 1

    print("Root:", x_n)
```

```
f_prime = lambda x: 3*x**2 - 24*x + 44
```

Try calling Newton_Method with a few different guesses for the initial value

## Cauchy

An infinite sequence $\{x_{i}\}_{i=1}^{\infty}$ is **Cauchy convergent** if

$$\forall \epsilon \exists N \text{ such that } \forall m,n > N$$
$$|x_{m} - x_{n}| < \epsilon$$

:::::

::: {.callout-warning}
## DCP2
:::

# Trade-Off: Speed versus Complexity

Newton's Method

$$x_{n} = x_{n-1} - \frac{f(x_{n-1})}{f'(x_{n-1})}$$

* converges faster
* needs computation of the derivative
* what if $f' \approx 0$?


# Preview: Backpropagation

In this experiment, we will see if we can get some intuition about backpropagation in a relatively simple setting of linear regression

$$\hat{y} = \beta_{0} + \beta_{1}x$$

with loss function

$$L = \sum(y - \hat{y})^{2} = \sum(y - \beta_{0} - \beta_{1}x)^{2}$$

The **gradient** is a vector of partial derivatives

$$\nabla L(\vec{\beta}) = 
\left[\begin{array}{c} \frac{\partial L}{\partial \beta_{0}} \\ \frac{\partial L}{\partial \beta_{1}}\end{array}\right]
= 
\left[\begin{array}{c} 2\sum(y - \beta_{0} - \beta_{1}x)(-1) \\ 2\sum(y - \beta_{0} - \beta_{1}x)(-x)\end{array}\right]$$

::::: {.panel-tabset}

## Setup

:::: {.columns}

::: {.column width="45%"}
![best fit line](backprop_setup.png)
:::

::: {.column width="10%"}
	
:::

::: {.column width="45%"}
Let us explore this gradient in a setup where

$$x \in (-5, 5)$$
$$Y ~ 25 + 20x + N(0,5^{2})$$
:::

::::

## Ex1

:::: {.columns}

::: {.column width="45%"}
![wrong slope](backprop_ex1.png)
:::

::: {.column width="10%"}
	
:::

::: {.column width="45%"}
With initial conditions $\vec{\beta_{0}} = \left[\begin{array}{c} 25 \\ 40 \end{array}\right]$

the gradient is

$$\nabla L(\vec{\beta}) = 
\left[\begin{array}{c} \frac{\partial L}{\partial \beta_{0}} \\ \frac{\partial L}{\partial \beta_{1}}\end{array}\right]
\approx 
\left[\begin{array}{c} 819 \\ 36867 \end{array}\right]$$

which indicates that more effort is needed to adjust the slope than the intercept!

:::

::::

## Ex2

:::: {.columns}

::: {.column width="45%"}
![wrong slope](backprop_ex2.png)
:::

::: {.column width="10%"}
	
:::

::: {.column width="45%"}
With initial conditions $\vec{\beta_{0}} = \left[\begin{array}{c} 40 \\ 20 \end{array}\right]$

the gradient is

$$\nabla L(\vec{\beta}) = 
\left[\begin{array}{c} \frac{\partial L}{\partial \beta_{0}} \\ \frac{\partial L}{\partial \beta_{1}}\end{array}\right]
\approx 
\left[\begin{array}{c} 3819 \\ 2860 \end{array}\right]$$

which indicates that more effort is needed to adjust the intercept than the slope!

:::

::::

:::::






::: {.callout-warning}
# DCP3
:::

# Markov Chains

A **Markov chain** is a stochastic process whose state depends only on the immediate previous iteration.

$$P_{ij} = P(X_{n} = j | X_{n-1} = i)$$

## Application: Dinner Choices

Suppose that we have a Princeton student whose behavior includes eating only three types of dinner: 

$$S = \{\text{ramen}, \text{pizza}, \text{sushi}\}$$

with transition matrix

$$P = \left(\begin{array}{ccc}
0.2 & 0.4 & 0.4 \\
0.3 & 0.4 & 0.3 \\
0.2 & 0.2 & 0.6
\end{array}\right)$$

![dinner choices network](dinner_network_graph.png)

::: {.callout-note collapse="true"}
## Network terminology

* directed versus undirected graphs
* cyclic versus acyclic graphs

Later studies focus on **DAGs**: directed, acyclic network graphs
:::

Suppose that, on a Monday, the student's preferences are

$$x_0 = \left(\begin{array}{ccc} 0.5 & 0.25 & 0.25 \end{array}\right)$$

a. What is the probability that the student will eat ramen on Tuesday (i.e. the next day)?
b. What is the probability that the student will eat pizza on Wednesday (i.e. two days later)?
c. What is the long-term dinner-choice behavior of this student?

::: {.callout-note collapse="true"}
## calculations

a. after one day

$$x_{0}P = \left(\begin{array}{ccc} 0.225 & 0.35 & 0.425 \end{array}\right)$$

The probability of eating ramen the next day is about 22.5 percent.

b. after two days

$$x_{0}P^{2} = \left(\begin{array}{ccc} 0.235 & 0.315 & 0.45 \end{array}\right)$$
The probability of eating pizza two days later is about 31.5 percent.

c. after many days

$$x_{0}P^{100} = \left(\begin{array}{ccc} 0.2307 & 0.3077 & 0.4615 \end{array}\right)$$

The probability of the student being at the sushi place is about 46.15 percent
:::

# Preview: Softmax

In the previous example, we ended with an inference that is similar to what we will encounter with a *softmax*.

$$x_{0}P^{100} = \left(\begin{array}{ccc} 0.2307 & 0.3077 & 0.4615 \end{array}\right)$$

1. the student's first choice is sushi
2. the student's second choice is pizza
3. the student's third choice is ramen

## Hardmax

In contrast, a **hardmax** is one-hot encoded.  

$$\left(\begin{array}{ccc} 0 & 0 & 1 \end{array}\right)$$

In this, we are certain that the student will be at the sushi place.


# Bias-Variance Trade-off

Within a *hypothesis class* of similar modeling functions, we are concerned with the **bias-variance tradeoff** in model selection.

![bias-variance tradeoff](biasvariance.png)

image source: [Scott Fortmann-Roe ](http://scott.fortmann-roe.com/docs/BiasVariance.html)


# Quo Vadimus?

:::: {.columns}

::: {.column width="45%"}
* due this Friday:

    - Precept 2

*  talk to some of your classmates and draft plans to form groups (toward the midterm and semester projects)

:::

::: {.column width="10%"}
	
:::

::: {.column width="45%"}

* tip: talk about your career and research goals with your instructors
* (optional) If you have seen the Bisection and Newton's Methods before, let me know which class(es) cover that material.

:::

::::


# Footnotes

::: {.callout-note collapse="true"}

## (optional) Additional Resources

* [Bisection Method](https://flexiple.com/python/bisection-method-python) by Harsh Pandey
* [Newton's Method](https://flexiple.com/python/newton-raphson-method-python) by Harsh Pandey

:::

::: {.callout-note collapse="true"}
## Session Info

```{r}
sessionInfo()
```
:::


:::: {.columns}

::: {.column width="45%"}
	
:::

::: {.column width="10%"}
	
:::

::: {.column width="45%"}

:::

::::

::::: {.panel-tabset}



:::::