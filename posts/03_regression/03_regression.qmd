---
title: "3: Regression"
author: "Derek Sollberger"
date: "2025-02-03"
format:
  html:
    toc: true
---

```{r}
#| echo: false
#| message: false
#| warning: false

library("tidyverse") #tools for data wrangling and visualization
liquor_df <- data.frame(
  year = 2014:2022,
  LLV = c(129, 54, 103, 50, 15, 10, 18, 49,57))
```

# Session 3: Regression

## Start

:::: {.columns}

::: {.column width="60%"}
* **Goal**: Discuss the bias-variance trade-ff

* **Objective**: Explore linear regression
:::

::: {.column width="40%"}
As we get started, try to install the `ISLP` package in your Python software
:::

::::

# Linear Model

$$\hat{y} = a + bx$$

* $\hat{y}$: predicted value
* $a$: intercept
* $b$: slope

## Residuals

A *residual* is the difference between a predicted value and its true value.

```{r}
#| echo: false
lin_fit <- lm(LLV ~ year, data = liquor_df)
liquor_df <- liquor_df |>
  mutate(predictions = predict(lin_fit,
                               newdata = data.frame(year = liquor_df$year)),
         residuals = predictions - LLV)
```

```{r}
#| echo: false
liquor_df |>
  ggplot(aes(x = year, y = LLV)) +
  geom_segment(aes(x = year, y = predictions, 
                   xend = year, yend = LLV), 
               color = "purple", linewidth = 3) +
  geom_point(size = 4, color = "black") +
  geom_smooth(formula = "y ~ x", method = "lm", se = FALSE,
              color = "blue", linewidth = 2) +
  geom_point(aes(x = year, y = predictions),
             color = "red", size = 4) +
  labs(title = "Linear Regression",
       subtitle = "black: true values\nred: predictions\npurple: residuals",
       caption = "SML 201",
       x = "year", y = "judicial referrals") +
  theme_minimal()
```

## Method of Least Squares

**Idea**: The *best-fit* line is where the sum-of-squared residuals is minimized.

$$E(a,b) = \sum_{i=1}^{n} (y_{i} - a - bx_{i})^{2}$$

**Claim**: $$a = \frac{ (\sum y_{i})(\sum x_{i}^{2}) - (\sum x_{i})(\sum x_{i}y_{i}) }{ n\sum x_{i}^{2} - (\sum x_{i})^{2} }, \quad b = \frac{ n\sum x_{i}y_{i} - (\sum x_{i})(\sum y_{i}) }{ n\sum x_{i}^{2} - (\sum x_{i})^{2} }$$

::: {.callout-note collapse="true"}
## (optional) Proof

Search for a critical point by setting the partial derivatives (along with the Chain Rule) equal to zero.

$$0 = \frac{\partial E}{\partial a} = -2\sum_{i = 1}^{n} (y_{i} - a - bx_{i}) = 2an + 2b\sum_{i = 1}^{n}x_{i} - 2\sum_{i = 1}^{n} y_{i}$$
$$0 = \frac{\partial E}{\partial b} = -2\sum_{i = 1}^{n} (y_{i} - a - bx_{i})x_{i} = 2a\sum_{i = 1}^{n}x_{i} + 2b\sum_{i = 1}^{n}x_{i}^{2} - 2\sum_{i = 1}^{n} x_{i}y_{i}$$

Create a matrix system of equations.

$$\left[  \begin{array}{cc}
  n & \sum_{i = 1}^{n}x_{i} \\
  \sum_{i = 1}^{n}x_{i} & \sum_{i = 1}^{n}x_{i}^{2} \\
  \end{array}\right]
  \left[  \begin{array}{c}  a \\ b \end{array}\right]
  =
  \left[  \begin{array}{c}  \sum_{i = 1}^{n} y_{i} \\ \sum_{i = 1}^{n} x_{i}y_{i} \end{array}\right]
  $$


Employ a matrix inverse.

$$\begin{array}{rcl}
  \left[  \begin{array}{c}  a \\ b \end{array}\right] & = & 
  \left[  \begin{array}{cc}
  n & \sum_{i = 1}^{n}x_{i} \\
  \sum_{i = 1}^{n}x_{i} & \sum_{i = 1}^{n}x_{i}^{2} \\
  \end{array}\right]^{-1}\left[  \begin{array}{c}  \sum_{i = 1}^{n} y_{i} \\ \sum_{i = 1}^{n} x_{i}y_{i} \end{array}\right] \\
  
  ~ & ~ & ~ \\
  
  \left[  \begin{array}{c}  a \\ b \end{array}\right] & = & \frac{1}{n\sum x_{i}^{2} - (\sum x_{i})^{2}} \left[  \begin{array}{cc}
  \sum_{i = 1}^{n}x_{i}^{2} & -\sum_{i = 1}^{n}x_{i} \\
  -\sum_{i = 1}^{n}x_{i} & n \\
  \end{array}\right]  \left[  \begin{array}{c}  \sum_{i = 1}^{n} y_{i} \\ \sum_{i = 1}^{n} x_{i}y_{i} \end{array}\right] \\
  
  ~ & ~ & ~ \\
  
  \left[  \begin{array}{c}  a \\ b \end{array}\right] & = & \frac{1}{n\sum x_{i}^{2} - (\sum x_{i})^{2}} 
  \left[  \begin{array}{c}  (\sum y_{i})(\sum x_{i}^{2}) - (\sum x_{i})(\sum x_{i}y_{i}) \\  n\sum x_{i}y_{i} - (\sum x_{i})(\sum y_{i}) \end{array}\right] \\
\end{array}$$

:::

## Multiple Linear Regression

$$\hat{y} = \beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} + \beta_{3}X_{3} + ...$$

is likewise solved by [ordinary least squares](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf)


# Activity: Literature Introductions

We will look at the abstracts and introduction paragraphs for some of the most influential papers in the history of machine learning:

## Adaboost

* A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting (1997â€”published as abstract in 1995), Freund and Schapire

## AlexNet

* ImageNet Classification with Deep Convolutional Neural Networks (2012)

## DropOut

* Dropout: A Simple Way to Prevent Neural Networks from Overfitting (2014), Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov

## GANs

* General Adversarial Nets (2014), Goodfellow et al.

## TensorFlow

* TensorFlow: A system for large-scale machine learning (2016), Abadi et al.

## Word2Vec

* Efficient Estimation of Word Representations in Vector Space (2013), Mikolov, Chen, Corrado, and Dean


## Bias-Variance Trade-off

Within a *hypothesis class* of similar modeling functions, we are concerned with the **bias-variance tradeoff** in model selection.

![bias-variance tradeoff](biasvariance.png)

image source: [Scott Fortmann-Roe ](http://scott.fortmann-roe.com/docs/BiasVariance.html)


# Quo Vadimus?

:::: {.columns}

::: {.column width="45%"}
* due this Friday (5 PM):

    - Precept 2

:::

::: {.column width="10%"}
	
:::

::: {.column width="45%"}

* tip: talk about your career and research goals with your instructors
* (optional) If you have seen the Bisection and Newton's Methods before, let me know which class(es) cover that material.

:::

::::


# Footnotes

::: {.callout-note collapse="true"}

## (optional) Additional Resources

* [bias-variance demo](https://spotintelligence.com/2023/04/11/bias-variance-trade-off/) by Neet Van Otten
* [bias-variance demo](https://machinelearningmastery.com/calculate-the-bias-variance-trade-off/) by Jason Brownlee
* [OLS](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf) description and proof by Professor Michael J Rosenfeld

:::

::: {.callout-note collapse="true"}
## Session Info

```{r}
sessionInfo()
```
:::


:::: {.columns}

::: {.column width="45%"}
	
:::

::: {.column width="10%"}
	
:::

::: {.column width="45%"}

:::

::::

::::: {.panel-tabset}



:::::