---
title: "9: The Kernel Trick"
author: "Derek Sollberger"
date: "2025-09-30"
format:
  html:
    toc: true
---

```{r}
#| echo: false
#| message: false
#| warning: false

library("DiagrammeR")
library("tidyverse")
```

# Session 9: The Kernel Trick

## Learning objectives:

:::: {.columns}

::: {.column width="60%"}
- Implement a binary classification model using a **maximal margin classifier**.
- Implement a binary classification model using a **support vector classifier**.
- Implement a binary classification model using a **support vector machine** (SVM).
- Generalize SVM models to **multi-class** cases.
:::

::: {.column width="10%"}

:::

::: {.column width="30%"}
![support vector machines!](images/svm_protest.png)
:::

::::

# AUROC

![area under ROC curves](images/roc-curve-v2.png)

* image source: [Rachel Draelos](https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/)

::: {.callout-note}
## AUROC

The **area under an ROC curve** is a popular metric for classification methods.

* a user can change the *cutoff value* and move a decision boundary
* for each cutoff value, compute and record the FPR (false positive rate) and TPR (true positive rate)
* plot TPR vs FPR
* compute the area under the curve: values closer to 1.0 are deemed better models

:::

::: {.callout-warning}
## DCP1
:::


# SVM Overview

::::: {.panel-tabset}

## MMC to SVM

**Support vector machine (SVM)**, an approach for classification developed in 1990. 
SVM is a generalizaion of classifiers methods, in particular:

- **maximal margin classifier** (it requires that the classes be separable by a linear boundary).
- **support vector classifier**
- **support vector machine**: binary classification setting with two classes

```{r}
#| echo: false
#| eval: true
mermaid("
graph TB
  A[SVM<br>support vector machine<br>non-linear class boundaries]

  B[MMC<br>maximal margin classifier<br>linear class boundaries] 
  B-->C[SVC<br>support vector classifier<br>applied in a broader range of cases]
  A-->C

")
```

## Mermaid code

(optional for this course)

```{r}
#| echo: true
#| eval: false

# DiagrammR package
mermaid("
graph TB
  A[SVM<br>support vector machine<br>non-linear class boundaries]

  B[MMC<br>maximal margin classifier<br>linear class boundaries] 
  B-->C[SVC<br>support vector classifier<br>applied in a broader range of cases]
  A-->C

")
```


:::::

# Hyperplane

![image credit: Deep AI](images/hyperplane.png)

- A *hyperplane* is a $p-1$-dimensional flat subspace of a $p$-dimensional space. For example, in a 2-dimensional space, a hyperplane is a flat one-dimensional space: a line. 
- (standard form) Definition of 2D hyperplane in 3D space:
$$\beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} + \beta_{3}X_{3}= 0$$
- (inner products) Any $X$ s.t. $X = (X_{1}, X_{2})^T$ for which the equation above is satisfied is a point on the hyperplane.

Additional resource: [Deep AI](https://deepai.org/machine-learning-glossary-and-terms/hyperplane)


::: {.callout-note collapse="true"}
## Conceptual Task 1A

> *ISLR*, Chapter 9, Conceptual Task 1A

"This problem involves hyperplanes in two dimensions."

(a) Sketch the hyperplane $1 + 3X_{1} - X_{2} = 0$. Indicate the set of
points for which $1 + 3X_{1} - X_{2} > 0$, as well as the set of points
for which $1 + 3X_{1} - X_{2} < 0$.

* blue: $1 + 3X_{1} - X_{2} > 0$
* red: $1 + 3X_{1} - X_{2} < 0$

```{r}
#| message: false
#| warning: false
#| echo: false
#| eval: true
N <- 50 #resolution
x <- seq(-10, 10, length.out = N)
y <- seq(-10, 10, length.out = N)

df <- expand.grid(x,y)
colnames(df) <- c("xval", "yval")

euclidean_distance <- function(x1, y1, x2, y2){
  # computes the Euclidean distance between (x1, y1) and (x2, y2)
  sqrt( (x2 - x1)^2 + (y2 - y1)^2 )
}

accuracy_calculation <- function(confusion_matrix){
  # computes the accuracy revealed by a 2x2 confusion matrix
  Q <- confusion_matrix
  (Q[1,1] + Q[2,2]) / (Q[1,1] + Q[1,2] + Q[2,1] + Q[2,2])
}

df1 <- df |>
  # math function
  mutate(shade = ifelse(yval > 3*xval + 1, "blue", "red"))

df1 |>
  ggplot() +
  
  # shaded regions
  geom_point(aes(x = xval, y = yval, color = shade),
             alpha = 0.5) +
  scale_color_identity() +
  
  # axes
  geom_segment(aes(x = -10, y = 0, xend = 10, yend = 0),
               alpha = 0.25, color = "gray75", linewidth = 2) +
  geom_segment(aes(x = 0, y = -10, xend = 0, yend = 10),
               alpha = 0.25, color = "gray75", linewidth = 2) +
  
  # main line
  geom_segment(aes(x = -(11/3), y = -10, xend = 3, yend = 10),
               color = "black", linewidth = 3) +
  
  # customization
  coord_equal() +
  labs(title = "Separating Hyperplane",
       subtitle = "y = 3x + 1",
       caption = "ISLR",
       x = "X1", y = "X2") +
  theme_minimal()
```



:::


## Separating Hyperplane

- Consider a matrix **X** of dimensions $n*p$, and a $y_{i} \in \{-1, 1\}$. We have a new observation, $x^*$, which is a vector $x^* = (x^*_{1}...x^*_{p})^T$ which we wish to classify to one of two groups.
- We will use a *separating hyperplane* to classify the observation.

![](images/fig9_2.png)
 
- We can label the blue observations as $y_{i} = 1$ and the pink observations as $y_{i} = -1$. 
- Thus, a separating hyperplane has the property s.t. $\beta_{0} + \beta_{1}X_{i1} + \beta_{2}X_{i2} ... + \beta_{p}X_{ip} > 0$ if $y_{i} =1$ and $\beta_{0} + \beta_{1}X_{i1} + \beta_{2}X_{i2} ... + \beta_{p}X_{ip} < 0$ if $y_{i} = -1$.
- In other words, a separating hyperplane has the property s.t. $y_{i}(\beta_{0} + \beta_{1}X_{i1} + \beta_{2}X_{i2} ... + \beta_{p}X_{ip}) > 0$ for all $i = 1...n$.
- Consider also the *magnitude* of $f(x^*)$. If it is far from zero, we are confident in its classification, whereas if it is close to 0, then $x^*$ is located near the hyperplane, and we are less confident about its classification.


::: {.callout-note collapse="true"}
## Conceptual Task 1B

> *ISLR*, Chapter 9, Conceptual Task 1B

(b) On the same plot, sketch the hyperplane $-2 + X_{1} + 2X_{2} = 0$.
Indicate the set of points for which $-2 + X_{1} + 2X_{2} > 0$, as well
as the set of points for which $-2 + X_{1} + 2X_{2} < 0$.

* blue: $-2 + X_{1} + 2X_{2} > 0$
* red: $-2 + X_{1} + 2X_{2} < 0$

```{r}
#| message: false
#| warning: false
#| echo: false
#| eval: true
df1b <- df |>
  # math function
  mutate(shade = ifelse(yval > 1 - 0.5*xval, "blue", "red"))

df1b |>
  ggplot() +
  
  # shaded regions
  geom_point(aes(x = xval, y = yval, color = shade),
             alpha = 0.5) +
  scale_color_identity() +
  
  # axes
  geom_segment(aes(x = -10, y = 0, xend = 10, yend = 0),
               alpha = 0.25, color = "gray75", linewidth = 2) +
  geom_segment(aes(x = 0, y = -10, xend = 0, yend = 10),
               alpha = 0.25, color = "gray75", linewidth = 2) +
  
  # main line
  geom_segment(aes(x = -10, y = 6, xend = 10, yend = -4),
               color = "black", linewidth = 3) +
  
  # customization
  coord_equal() +
  labs(title = "Separating Hyperplane",
       subtitle = "y = 1 - 0.5x",
       caption = "ISLR",
       x = "X1", y = "X2") +
  theme_minimal()
```

:::



## Maximal Margin Classifier

![](images/fig9_3.png)

- Generally, if data can be perfectly separated using a hyperplane, an infinite amount of such hyperplanes exist.
- An intuitive choice is the *maximal margin hyperplane*, which is the hyperplane that is farthest from the training data.
- We compute the perpendicular distance from each training observation to the hyperplane. The smallest of these distances is known as the *margin*.
- The *maximal margin hyperplane* is the hyperplane for which the *margin* is maximized. We can classify a test observation based on which side of the maximal margin hyperplane it lies on, and this is known as the *maximal margin classifier*.
- The maximal margin classifier classifies $x^*$ based on the sign of $f(x^*) = \beta_{0} + \beta_{1}x^*_{1} + ... + \beta_{p}x^*_{p}$.

![](images/fig9_3.png)

- Note the 3 training observations that lie on the margin and are equidistant from the hyperplane. These are the **support vectors** (vectors in $p$-dimensional space; in this case $p=2$).
- They support the hyperplane because if their location was changed, the hyperplane would change. 
- The maximal margin hyperplane depends on these observations, *but not the others* (unless the other observations were moved at or within the margin).


::: {.callout-note collapse="true"}
## Conceptual Task 3

```{r}
#| message: false
#| warning: false
#| echo: false
#| eval: true
obs <- 1:7
xvals <- c(3,2,4,1,2,4,4)
yvals <- c(4,2,4,4,1,3,1)
class_label <- c("Red", "Red", "Red", "Red", "Blue", "Blue", "Blue")
df3 <- data.frame(obs, xvals, yvals, class_label)
df3
```

(a) We are given $n = 7$ observations in $p = 2$ dimensions. For each
observation, there is an associated class label.

```{r}
#| message: false
#| warning: false
#| echo: false
#| eval: true
df3 |>
  ggplot() +
  geom_point(aes(x = xvals, y = yvals, color = class_label),
             size = 5) +
  coord_equal() +
  scale_color_identity() +
  labs(title = "Where to Draw the Separating Hyperplane?",
       # subtitle = "y = x - 0.5",
       caption = "ISLR",
       x = "X1", y = "X2") +
  theme_minimal() +
  xlim(0,5) + ylim(0,5)
```

(b) Sketch the optimal separating hyperplane, and provide the equation
for this hyperplane

```{r}
#| message: false
#| warning: false
#| echo: false
#| eval: true
df3 |>
  ggplot() +
  
  # separating hyperplane
  geom_segment(aes(x = 0.5, y = 0, xend = 5, yend = 4.5),
               color = "black", linetype = 1, linewidth = 3) +
  
  # margins
  geom_segment(aes(x = 0, y = 0, xend = 5, yend = 5),
               color = "black", linetype = 2, linewidth = 2) +
  geom_segment(aes(x = 1, y = 0, xend = 5, yend = 4),
               color = "black", linetype = 2, linewidth = 2) +
  
  geom_point(aes(x = xvals, y = yvals, color = class_label),
             size = 5) +
    coord_equal() +
  scale_color_identity() +
  labs(title = "Separating Hyperplane",
       subtitle = "y = x - 0.5",
       caption = "ISLR",
       x = "X1", y = "X2") +
  theme_minimal() +
  xlim(0,5) + ylim(0,5)
```

(c)

* blue: $0.5 - X_{1} + X_{2} < 0$
* red: $0.5 - X_{1} + X_{2} > 0$

(d) maximal margin in indicated by the dashed lines, with margin

$$M = \frac{0.5}{\sqrt{2}} \approx 0.3536$$

(e) Indicate the support vectors for the maximal margin classifier.

```{r}
#| message: false
#| warning: false
#| echo: false
#| eval: true
df3e <- df3 |>
  mutate(supp_vec = ifelse(obs %in% c(2,3,5,6), 
                           "support vector", "other data"))
  
df3e$supp_vec <- factor(df3e$supp_vec,
                        levels = c("support vector", "other data"))
  
df3e |>  
  ggplot() +
  
  # separating hyperplane
  geom_segment(aes(x = 0.5, y = 0, xend = 5, yend = 4.5),
               color = "black", linetype = 1, linewidth = 3) +
  
  # margins
  geom_segment(aes(x = 0, y = 0, xend = 5, yend = 5),
               color = "black", linetype = 2, linewidth = 2) +
  geom_segment(aes(x = 1, y = 0, xend = 5, yend = 4),
               color = "black", linetype = 2, linewidth = 2) +
  
  geom_point(aes(x = xvals, y = yvals, color = supp_vec),
             size = 5) +
    coord_equal() +
  scale_color_manual(values = c("purple", "gray50")) +
  labs(title = "Separating Hyperplane",
       subtitle = "y = x - 0.5",
       caption = "ISLR",
       x = "X1", y = "X2") +
  theme_minimal() +
  xlim(0,5) + ylim(0,5)
```

(f) Argue that a slight movement of the seventh observation would
not affect the maximal margin hyperplane.

(g) Sketch a hyperplane that is not the optimal separating hyperplane,
and provide the equation for this hyperplane.

```{r}
#| message: false
#| warning: false
#| echo: false
#| eval: true
df3 |>
  ggplot() +
  
  # separating hyperplane
  geom_segment(aes(x = 0, y = 1/4, xend = 5, yend = 16/4),
               color = "black", linetype = 1, linewidth = 3) +
  
  geom_point(aes(x = xvals, y = yvals, color = class_label),
             size = 5) +
    coord_equal() +
  scale_color_identity() +
  labs(title = "Separating Hyperplane (not optimal)",
       subtitle = "y = 0.25(3x+1)",
       caption = "ISLR",
       x = "X1", y = "X2") +
  theme_minimal() +
  xlim(0,5) + ylim(0,5)
```

(h) Draw an additional observation on the plot so that the two
classes are no longer separable by a hyperplane.

```{r}
#| message: false
#| warning: false
#| echo: false
#| eval: true
new_dot <- data.frame(obs = 8, xvals = 0, yvals = 5, class_label = "blue")
df3h <- rbind(df3, new_dot)
df3h |>
  ggplot() +
  geom_point(aes(x = xvals, y = yvals, color = class_label),
             size = 5) +
  coord_equal() +
  scale_color_identity() +
  labs(title = "Where to Draw the Separating Hyperplane?",
       subtitle = "new data at (0,5)",
       caption = "ISLR",
       x = "X1", y = "X2") +
  theme_minimal() +
  xlim(0,5) + ylim(0,5)
```

:::


## Mathematics of the MMC

- Consider constructing an MMC based on the training observations $x_{1}...x_{n} \in \mathbb{R}^p$. This is the solution to the optimization problem:

$$\text{max}_{\beta_{0}...\beta_{p}, M} \space M$$
$$\text{subject to } \sum_{j=1}^{p}\beta_{j}^2 = 1$$
$$y_{i}(\beta_{0} + \beta_{1}X_{i1} + \beta_{2}X_{i2} ... + \beta_{p}X_{ip}) \geq M \quad \forall i = 1...n$$

- $M$ is the *margin*, and the $\beta$ coeffients are chosen to maximize $M$. 
- The constraint (3rd equation) ensures that each observation will be correctly classified, as long as M is positive. 

![](images/fig9_3.png)

- The 2nd and 3rd equations ensure that each data point is on the correct side of the hyperplane and at least M-distance away from the hyperplane.
- The perpendicular distance to the hyperplane is given by $y_{i}(\beta_{0} + \beta_{1}x_{i1} + \beta_{2}x_{i2} ... + \beta_{p}x_{ip})$.

> But what if our data is not separable by a linear hyperplane?

![](images/fig9_4.png)

> Individual data points greatly affect formation of the maximal margin classifier

![](images/fig9_5.png)

::: {.callout-warning}
## DCP2
:::


# Support Vector Classifiers

- We can't always use a hyperplane to separate two classes. 
- Even if such a classifier does exist, it's not always desirable, due to overfitting or too much sensitivity to individual observations.
- Thus, it might be worthwhile to consider a classifier/hyperplane that misclassifies a few observations in order to improve classification of the remaining data points. 
- The *support vector classifier*, a.k.a the *soft margin classifier*, allows some training data to be on the wrong side of the margin or even the hyperplane. 

## Mathematics of the SVC
- The SVC classifies a test observation based on which side of the hyperplane it lies. 

$$\text{max}_{\beta_{0}...\beta_{p}, \epsilon_{1}...\epsilon_{n}, M} \space M$$
$$\text{subject to } \sum_{j=1}^{p}\beta_{j}^2 = 1$$
$$y_{i}(\beta_{0} + \beta_{1}X_{i1} + \beta_{2}X_{i2} ... + \beta_{p}X_{ip}) \geq M(1 - \epsilon_{i})$$
$$\epsilon_{i} \geq 0, \quad \sum_{i=1}^{n}\epsilon_{i} \leq C$$

- $C$ is a nonnegative tuning parameter, typically chosen through cross-validation, and can be thought of as the budget for margin violation by the observations. 
- The $\epsilon_{i}$ are *slack variables* that allow individual observations to be on the wrong side of the margin or hyperplane. The $\epsilon_{i}$ indicates where the $i^{\text{th}}$ observation is located with regards to the margin and hyperplane. 

    - If $\epsilon_{i} = 0$, the observation is on the correct side of the margin.
    - If $\epsilon_{i} > 0$, the observation is on the wrong side of margin
    - If $\epsilon_{i} > 1$, the observation is on the wrong side of the hyperplane. 
    
- Since $C$ constrains the sum of the $\epsilon_{i}$, it determines the number and magnitude of violations to the margin. If $C=0$, there is no margin for violation, thus all the $\epsilon_{1},...,\epsilon_{n} = 0$. 
- Note that if $C>0$, no more than $C$ observations can be on wrong side of hyperplane, since in these cases $\epsilon_{i} > 1$. 

## Tuning Parameter

![](images/fig9_7.png)

- A property of the classifier is that only data points which lie on or violate the margin will affect the hyperplane. These data points are known as *support vectors*.
- $C$ controls the bias-variance tradeoff of the classifier. 

    - When $C$ is large: high bias, low variance
    - When $C$ is small: low bias, high variance

- The property of the SVC solely being dependent on certain observations in classification differs from other classification methods such as LDA (depends on mean of all observations in each class, as well as each class's covariance matrix using all observations). 
- However, logistic regression is more similar to SVC in that it has low sensitivity to observations far from the decision boundary.

## Nonlinear Classification

- Many decision boundaries are not linear.
- We could fit an SVC to the data using $2p$ features (in the case of $p$ features and using a quadratic form).

$$X_{1}, X_{1}^{2}, \quad X_{2}, X_{2}^{2}, \quad\cdots, \quad X_{p}, X_{p}^{2}$$


$$\text{max}_{\beta_{0},\beta_{11},\beta_{12},\dots,\beta_{p1},\beta_{p2} \epsilon_{1},\dots,\epsilon_{n}, M} \space M$$
$$\text{subject to }  y_{i}\left(\beta_{0} + \sum_{j=1}^{p} \beta_{ji}x_{ji} + \sum_{j=1}^{p} \beta_{ji}x_{ji}^{2}\right) \geq M(1 - \epsilon_{i})$$

$$\epsilon_{i} \geq 0, \quad \sum_{i=1}^{n}\epsilon_{i} \leq C, \quad \sum_{j=1}^{p}\sum_{k=1}^{2} \beta_{jk}^{2} = 1$$

- Note that in the enlarged feature space (here, with the quadratic terms), the decision boundary is linear. But in the original feature space, it is quadratic $q(x) = 0$ (in this example), and generally the solutions are not linear.
- One could also include interaction terms, higher degree polynomials, etc., and thus the feature space could enlarge quickly and entail unmanageable computations.

::: {.callout-note collapse="true"}
## Cortes (1995)

The soft margin classifier is credit to Kristin Bennett, Vladimir Vapnik, and Corinna Cortes.
:::


::: {.callout-note collapse="true"}
## Conceptual Task 2

We now investigate a non-linear decision boundary.

* blue: $(1 + X_{1})^{2} + (2 - X_{2})^{2} > 4$
* red: $(1 + X_{1})^{2} + (2 - X_{2})^{2} < 4$

```{r}
#| message: false
#| warning: false
#| echo: false
#| eval: true
df2 <- df |>
  # math function
  mutate(shade = ifelse(euclidean_distance(xval, yval, -1, 2) > 4, 
                        "blue", "red"))

df2 |>
  ggplot() +
  
  # shaded regions
  geom_point(aes(x = xval, y = yval, color = shade),
             alpha = 0.5) +
  scale_color_identity() +
  
  # axes
  geom_segment(aes(x = -10, y = 0, xend = 10, yend = 0),
               alpha = 0.25, color = "gray75", linewidth = 2) +
  geom_segment(aes(x = 0, y = -10, xend = 0, yend = 10),
               alpha = 0.25, color = "gray75", linewidth = 2) +
  
  # customization
  coord_equal() +
  labs(title = "Separating Hyperplane",
       subtitle = "(x+1)^2 + (y-2)^2 = 4",
       caption = "ISLR",
       x = "X1", y = "X2") +
  theme_minimal()
```

(c) To what class is the observation (0, 0) classified? (−1, 1)? (2, 2)? (3, 8)?

```{r}
ifelse(euclidean_distance(0, 0, -1, 2) > 4, "blue", "red")
ifelse(euclidean_distance(-1, 1, -1, 2) > 4, "blue", "red")
ifelse(euclidean_distance(2, 2, -1, 2) > 4, "blue", "red")
ifelse(euclidean_distance(3, 8, -1, 2) > 4, "blue", "red")
```

(d) While the decision boundary

$$(1 + X_{1})^{2} + (2 - X_{2})^{2} = 4$$

is not linear in $X_{1}$ and $X_{2}$, it is linear in terms of $X_{1}$, $X_{1}^{2}$, $X_{2}$, $X_{2}^{2}$

$$\beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} + \beta_{3}X_{1}^{2} + \beta_{4}X_{2}^{2} = 0$$

with $\beta_{0} = 1$, $\beta_{1} = 2$, $\beta_{2} = -4$, $\beta_{3} = 1$, and $\beta_{4} = 1$.
:::


# The Kernel Trick

As presented in *Why Machines Learn* by Anil Ananthaswamy (pages 224 to 241),

::: {.callout-note}
## The Kernel Trick

If we can find a mapping

$$x_{j} \rightarrow \phi(x_{j})$$

where $x_{j} \in \mathbb{R}^{d}$ is in a lower-dimensional space and $\phi(x_{j}) \in \mathbb{R}^{n}$ is in a higher-dimensional space (i.e. $d < n$) and 

$$K(x_{i}, x_{j}) = \phi(x_{i}) \cdot \phi(x_{j})$$

then the **kernel function** allows us to compute inner products in the lower-dimensional space rather than the higher dimensional space.

:::

## Example 1:

$$a = \left[\begin{array}{c}a_{1} \\ a_{2}\end{array}\right], b = \left[\begin{array}{c}b_{1} \\ b_{2}\end{array}\right], \quad K(x,y) = (x \cdot y)^{2}, \quad \phi(x) = \left[\begin{array}{c}x_{1}^{2} \\ x_{2}^{2} \\ \sqrt{2}x_{1}x_{2}\end{array}\right]$$

::: {.callout-note collapse="true"}
## 3D

$$\begin{array}{rcl}
  \phi(a) \cdot \phi(b) & = & \left[\begin{array}{c}a_{1}^{2} \\ a_{2}^{2} \\ \sqrt{2}a_{1}a_{2}\end{array}\right] \cdot \left[\begin{array}{c}b_{1}^{2} \\ b_{2}^{2} \\ \sqrt{2}b_{1}b_{2}\end{array}\right] \\
   & = & a_{1}^{2}b_{1}^{2} + a_{2}^{2}b_{2}^{2} + 2a_{1}a_{2}b_{1}b_{2} \\
   \end{array}$$
:::

::: {.callout-note collapse="true"}
## 2D

$$\begin{array}{rcl}
  K(a,b) & = & (a \cdot b)^{2} \\
  ~ & = & \left(\left[\begin{array}{c}a_{1} \\ a_{2}\end{array}\right] \cdot \left[\begin{array}{c}b_{1} \\ b_{2}\end{array}\right]\right)^{2} \\
  ~ & = & (a_{1}b_{1} + a_{2}b_{2})^{2} \\
  ~ & = & a_{1}^{2}b_{1}^{2} + a_{2}^{2}b_{2}^{2} + 2a_{1}a_{2}b_{1}b_{2} \\
   \end{array}$$
:::

## Example 2:

![kernel trick](images/Kernel_trick_picture.png)

* image source: [Grace Zhang](https://medium.com/@zxr.nju/what-is-the-kernel-trick-why-is-it-important-98a98db0961d)

::: {.callout-note collapse="true"}
## Guyon (1980s)

The kernel trick, its development for SVMs, and synthesis is credited to Isabelle Guyon (Bell Labs)
:::

# Support Vector Machines

- The SVM is an extension of the SVC which results from using *kernels* to enlarge the feature space. A kernel is a function that quantifies the similarity of two data points.
- Essentially, we want to enlarge the feature space to make use of a nonlinear decision boundary, while avoiding getting bogged down in unmanageable calculations.
- The solution to the SVC problem in the SVM context involves only the *inner products* (AKA dot products) of the observations.

$$\langle x_{i}  \; , x_{i'} \; \rangle = \sum_{j=1}^{p}x_{ij}x_{i'j}$$

In the context of SVM, the linear support vector classifier is as follows:

$$f(x) = \beta_{0} + \sum_{i=1}^{n}\alpha_{i}\langle \; x, x_i\; \rangle$$

- To estimate the $n$ $\alpha_{i}$ coefficients and $\beta_{0}$, we only need the $\binom{n}{2}$ inner products between all pairs of training observations.
- Note that in the equation above, in order to compute $f(x)$ for the new point $x$, we need the inner product between the new point and all the training observations. However, $\alpha_{i} = 0$ for all points that are not on or within the margin (i.e., points that are not support vectors). So we can rewrite the equation as follows, where $S$ is the set of support point indices:

$$f(x) = \beta_{0} + \sum_{i \in S}\alpha_{i}\langle \; x, x_{i} \; \rangle$$

- Replace every inner product with $K(x_{i}, x_{i'})$, where $K$ is a kernel function. 
- $K(x_{i}, x_{i'}) = \sum_{j=1}^{p}x_{ij}x_{i'j}$ is the SVC and is known as a linear kernel since it is linear in the features. 
- One could also have kernel functions of the following form, where $d$ is a positive integer:

$$K(x_{i}, x_{i'}) = \left(1 + \sum_{j=1}^{p}x_{ij}x_{i'j}\right)^d$$

- This will lead to a much more flexible decision boundary, and is basically fitting an SVC in a higher-dimensional space involving polynomials of degree $d$, instead of the original feature space. 

::: {.callout-note collapse="true"}
## Poggio (1975)

The polynomial kernels for SVMs are credited to Tomaso Poggio (MIT computational neuroscientist)
:::

- When an SVC is combined with a nonlinear kernel as above, the result is a *support vector machine*.

$$f(x) =  \beta_{0} + \sum_{i \in S}\alpha_{i}K(x, x_{i})$$

## Radial Kernels

![AKA: Gaussians, image credit: Manin Bocss](images/gaussian.png)

- There are other options besides polynomial kernel functions, and a popular one is a *radial kernel*.

$$K(x, x_{i}) = \text{exp}\left(-\gamma\sum_{j=1}^p(x_{ij} - x_{i'j})^2\right), \quad \gamma > 0$$

- For a given test observations $x^*$, if it is far from $x_{i}$, then $K(x^*, x_{i})$ will be small given the negative $\gamma$ and large $\sum_{j=1}^p(x^*_{j} - x_{ij})^2)$. 
- Thus, $x_{i}$ will play little role in $f(x^*)$.
- The predicted class for $x^*$ is based on the sign of $f(x^*)$, so training observations far from a given test point play little part in determining the label for a test observation.
- The radial kernel therefore exhibits local behavior with respect to other observations.

## SVM with Radial Kernels

![image credit: Manin Bocss](images/fig9_9.png)

- The advantage of using a kernel rather than simply enlarging feature space is computational, since it is only necessary to compute $\binom{n}{2}$ kernel functions. 
- For radial kernels, the feature space is implicit and infinite dimensional, so we could not do the computations in such a space anyways.

::: {.callout-warning}
## DCP3
:::


# More than Two Classes

- The concept of separating hyperplanes does not extend naturally to more than two classes, but there are some ways around this.
- A *one-versus-one* approach constructs $K \choose 2$ SVMs, where $K$ is the number of classes. An observation is classified to each of the $K \choose 2$ classes, and the number of times it appears in each class is counted.
- The $k^\text{th}$ class might be coded as +1 versus the $(k')^\text{th}$ class is coded as -1. 
- The data point is classified to the class for which it was most often assigned in the pairwise classifications.
- Another option is *one-versus-all* classification. This can be useful when there are a lot of classes.
- $K$ SVMs are fitted, and one of the K classes to the remaining $K-1$ classes. 
- $\beta_{0k}...\beta_{pk}$ denotes the parameters that results from constructing an SVM comparing the $k$th class (coded as +1) to the other classes (-1).
- Assign test observation $x^*$ to the class $k$ for which $\beta_{0k} + ... + \beta_{pk}x^*_{p}$ is largest.


# SVM Legacy

![SVM inventors](images/BBVA_award_2020.png)

"The BBVA Foundation has awarded Isabelle Guyon, Bernhard Schölkopf and Vladimir Vapnik with the [Frontiers of Knowledge Award in the Information and Communication Technologies category](https://www.bbva.com/en/the-frontiers-of-knowledge-awards-recognize-guyon-scholkopt-and-vapnik-for-teaching-machines-how-to-classify-data/), for helping advance the field of artificial intelligence with their seminal contributions to machine learning."

* Schölkopf coined the phrase "support vector machines" to distance the idea away from neural networks

    * neural networks were distant in the "AI Winter" (little research gains) from 1981 to 2012
    * "machine learning" becomes very popular


# Quo Vadimus?

:::: {.columns}

::: {.column width="40%"}
* due this Friday:

    - Precept 5
    - CuriosityMid (survey)

:::

::: {.column width="10%"}
	
:::

::: {.column width="50%"}
![sarcasm](images/python_import_joke.png)

:::

::::


# Footnotes

::: {.callout-note collapse="true"}

## (optional) Additional Resources

* [SVM in Python](https://medium.com/towards-data-science/support-vector-machines-explained-with-python-examples-cb65e8172c85) by Carolina Bento


:::

::: {.callout-note collapse="true"}
## Session Info

```{r}
sessionInfo()
```
:::


:::: {.columns}

::: {.column width="45%"}
	
:::

::: {.column width="10%"}
	
:::

::: {.column width="45%"}

:::

::::

::::: {.panel-tabset}



:::::