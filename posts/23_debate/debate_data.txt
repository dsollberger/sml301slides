Discussions on regulation of artificial intelligence in the United States have included topics such as the timeliness of regulating AI, the nature of the federal regulatory framework to govern and promote AI, including what agency should lead, the regulatory and governing powers of that agency, and how to update regulations in the face of rapidly changing technology, as well as the roles of state governments and courts.[1]
Federal government regulatory measures
2016-2017

As early as 2016, the Obama administration had begun to focus on the risks and regulations for artificial intelligence. In a report titled Preparing For the Future of Artificial Intelligence,[2] the National Science and Technology Council set a precedent to allow researchers to continue to develop new AI technologies with few restrictions. It is stated within the report that "the approach to regulation of AI-enabled products to protect public safety should be informed by assessment of the aspects of risk".[3]
2018-2020

The first main report was the National Strategic Research and Development Plan for Artificial Intelligence.[4] On August 13, 2018, Section 1051 of the Fiscal Year 2019 John S. McCain National Defense Authorization Act (P.L. 115-232) established the National Security Commission on Artificial Intelligence "to consider the methods and means necessary to advance the development of artificial intelligence, machine learning, and associated technologies to comprehensively address the national security and defense needs of the United States."[5] Steering on regulating security-related AI is provided by the National Security Commission on Artificial Intelligence.[6] The Artificial Intelligence Initiative Act (S.1558) is a proposed bill that would establish a federal initiative designed to accelerate research and development on AI for, inter alia, the economic and national security of the United States.[7][8]

On January 7, 2019, following an Executive Order on Maintaining American Leadership in Artificial Intelligence,[9] the White House's Office of Science and Technology Policy released a draft Guidance for Regulation of Artificial Intelligence Applications,[10] which includes ten principles for United States agencies when deciding whether and how to regulate AI.[11] In response, the National Institute of Standards and Technology has released a position paper,[12] and the Defense Innovation Board has issued recommendations on the ethical use of AI.[13] A year later, the administration called for comments on regulation in another draft of its Guidance for Regulation of Artificial Intelligence Applications.[14]

Other specific agencies working on the regulation of AI include the Food and Drug Administration,[15] which has created pathways to regulate the incorporation of AI in medical imaging.[16] National Science and Technology Council also published the National Artificial Intelligence Research and Development Strategic Plan,[17] which received public scrutiny and recommendations to further improve it towards enabling Trustworthy AI.[18]
2020-2021

In March 2021, the National Security Commission on Artificial Intelligence released their final report.[19] In the report, they stated, "Advances in AI, including the mastery of more general AI capabilities along one or more dimensions, will likely provide new capabilities and applications. Some of these advances could lead to inflection points or leaps in capabilities. Such advances may also introduce new concerns and risks and the need for new policies, recommendations, and technical advances to assure that systems are aligned with goals and values, including safety, robustness and trustworthiness."

In June 2022, Senators Rob Portman and Gary Peters introduced the Global Catastrophic Risk Management Act. The bipartisan bill "would also help counter the risk of artificial intelligence... from being abused in ways that may pose a catastrophic risk".[20][21] On October 4, 2022, President Joe Biden unveiled a new AI Bill of Rights,[22] which outlines five protections Americans should have in the AI age: 1. Safe and Effective Systems, 2. Algorithmic Discrimination Protection, 3.Data Privacy, 4. Notice and Explanation, and 5. Human Alternatives, Consideration, and Fallback. The Bill was formally published in October 2022 by the Office of Science and Technology Policy (OSTP), a U.S. government office that advises the President on science and technology policy matters.[23]
2023-2024

In July 2023, the Biden administration secured voluntary commitments from seven companies – Amazon, Anthropic, Google, Inflection, Meta, Microsoft, and OpenAI – to manage the risks associated with AI. The companies committed to ensure AI products undergo both internal and external security testing before public release; to share information on the management of AI risks with the industry, governments, civil society, and academia; to prioritize cybersecurity and protect proprietary AI system components; to develop mechanisms to inform users when content is AI-generated, such as watermarking; to publicly report on their AI systems' capabilities, limitations, and areas of use; to prioritize research on societal risks posed by AI, including bias, discrimination, and privacy concerns; and to develop AI systems to address societal challenges, ranging from cancer prevention to climate change mitigation. In September 2023, eight additional companies – Adobe, Cohere, IBM, Nvidia, Palantir, Salesforce, Scale AI, and Stability AI – subscribed to these voluntary commitments.[24][25]

In January 2023, the National Institute of Standards and Technology (NIST) released the Artificial Intelligence Risk Management Framework (AI RMF 1.0), providing voluntary guidance for organizations to identify, assess, and manage risks associated with AI systems.[26]

The Biden administration, in October 2023 signaled that they would release an executive order leveraging the federal government's purchasing power to shape AI regulations, hinting at a proactive governmental stance in regulating AI technologies.[27] On October 30, 2023, President Biden released this Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence. The Executive Order addresses a variety of issues, such as focusing on standards for critical infrastructure, AI-enhanced cybersecurity, and federally funded biological synthesis projects.[28]

The Executive Order provides the authority to various agencies and departments of the US government, including the Energy and Defense departments, to apply existing consumer protection laws to AI development.[29] The Executive Order builds on the Administration’s earlier agreements with AI companies to instate new initiatives to "red-team" or stress-test AI dual-use foundation models, especially those that have the potential to pose security risks, with data and results shared with the federal government.

The Executive Order also recognizes AI's social challenges, and calls for companies building AI dual-use foundation models to be wary of these societal problems. For example, the Executive Order states that AI should not “worsen job quality”, and should not “cause labor-force disruptions”. Additionally, Biden’s Executive Order mandates that AI must “advance equity and civil rights”, and cannot disadvantage marginalized groups.[30] It also called for foundation models to include "watermarks" to help the public discern between human and AI-generated content, which has raised controversy and criticism from deepfake detection researchers.[31]
2025

In January 2025, President Donald Trump repealed Executive Order 14110, reflecting his administration's preference for deregulating AI in support of innovation over safeguarding risks.[32]

In early 2025, Congress began advanced bipartisan legislation targeting AI-generated deepfakes, including the "TAKE IT DOWN Act," which would prohibit nonconsensual disclosure of AI-generated "intimate imagery", requiring all platforms to remove such content. Additionally, lawmakers also reintroduced the CREATE AI Act to codify the National AI Research Resource (NAIRR), which aimed to expand public access to computing resources, datasets, and AI testing environments. Additionally, the Trump administration also signed Executive Order #14179 to initiate a national “AI Action Plan”, focusing on securing U.S. global AI dominance in a way in which the White House can seek public input on AI safety and standards. At the state level, new laws have also been passed or proposed to regulate AI-generated impersonations, chatbot disclosures, and even synthetic political content. Meanwhile, the Department of Commerce also expanded export controls on AI technology, and NIST published an updated set of guidances on AI cybersecurity risks.[33]

In March 2025, OpenAI made a policy proposal for the Trump administration to preempt pending AI-related state laws with federal laws.[34] Meta, Google, IBM and Andreessen Horowitz have also pressured the government to adopt national rules that would rein in state laws.[35] States like California, which is considering the most AI regulations of all states would be hit hard. California legislature is considering several AI laws that would require insurance companies to report the use of AI when denying healthcare claims, as well as regulations that requires the AI to be performance tested before making implementation on certain applications.[36] In May 2025, House Republicans inserted into a tax and spending bill a clause banning state AI laws for 10 years,[37] which was met with opposition from more than 100 nonprofit organizations, elected officials, public policy experts, and others.[38][39][40] The Senate voted 99-1 to defeat the ban.[41] In September 2025, Sen. Ted Cruz (a supporter of the AI state moratorium) said that the proposal would return for debate in Congress.[42][43]
State and local government interventions

In January 2023, the New York City Bias Audit Law (Local Law 144[44]) was enacted by the NYC Council in November 2021. Originally due to come into effect on 1 January 2023, the enforcement date for Local Law 144 has been pushed back due to the high volume of comments received during the public hearing on the Department of Consumer and Worker Protection's (DCWP) proposed rules to clarify the requirements of the legislation. It eventually became effective on July 5, 2023.[45] From this date, the companies that are operating and hiring in New York City are prohibited from using automated tools to hire candidates or promote employees, unless the tools have been independently audited for bias.

In February 2024, Senator Scott Wiener introduced the Safe and Secure Innovation for Frontier Artificial Intelligence Models Act to the California legislature. The bill drew heavily on the Biden executive order[46] and had the goal of reducing catastrophic risks by mandating safety tests for the most powerful AI models. If passed, the bill would have established a publicly-funded cloud computing cluster in California.[47] On September 29, 2024. Governor Gavin Newsom vetoed the bill. It is considered unlikely that the legislature will override the governor's veto with a two-thirds vote from both houses.[48]

On March 13, 2024, Utah Governor Spencer Cox signed the S.B 149 "Artificial Intelligence Policy Act". This legislation went into effect on May 1, 2024.[49] It established liability, notably for companies that do not disclose their use of generative AI when required by state consumer protection laws, or when users commit criminal offense using generative AI. It also created the Office of Artificial Intelligence Policy and the Artificial Intelligence Learning Laboratory Program.[50]

On March 21, 2024, the State of Tennessee enacted legislation called the ELVIS Act, aimed specifically at audio deepfakes, and voice cloning.[51] This legislation was the first enacted legislation in the nation aimed at regulating AI simulation of image, voice and likeness.[52] The bill passed unanimously in the Tennessee House of Representatives and Senate.[53] This legislation's success was hoped by its supporters to inspire similar actions in other states, contributing to a unified approach to copyright and privacy in the digital age, and to reinforce the importance of safeguarding artists' rights against unauthorized use of their voices and likenesses.[54][55]

In April 2024, the Federal Trade Commission (FTC) announced Operation AI Comply, a cross-agency initiative to ensure that AI products and marketing claims comply with existing consumer-protection, fair-credit, and truth-in-advertising laws.[56] The FTC stated that companies deploying AI systems must "keep their AI claims in check" and warned that false or misleading representations about AI capabilities would be subject to enforcement under the FTC Act.[57]
Other perspectives

In 2016, Joy Buolamwini, AI researcher at Massachusetts Institute of Technology, shared her personal experiences with discrimination in facial recognition software at a TED Talk conference.[58] Facial recognition software is widely understood to be inaccurate in its identification of darker-skinned peoples, which matters especially in the context of policing, the criminal justice system, healthcare system, and employment sectors.[59]

In 2022, the PEW Research Center's study of Americans revealed that only 18% of respondents are more excited than they are concerned about AI.[60] Biases in AI algorithms and methods that lead to discrimination are causes for concern among many activist organizations and academic institutions. Recommendations include increasing diversity among creators of AI algorithms and addressing existing systemic bias in current legislation and AI development practices.[59][61]

In August 2025, Silicon Valley companies and investors pledged up to $200 million to two new pro-AI super PACs, Meta California (funded by Meta) and Leading the Future.[62][63][64] According to the New York Times, the super PACs criticize politicians who are "insufficiently supportive of the push into artificial intelligence" ahead of the 2026 midterm elections. 

Regulating Artificial Intelligence: Antitrust and Anti-Discrimination Policy

Ava Chen

VOLUME 4

ISSUE 1

Spring 2025

In 2023, the large language model GPT-4 passed the Uniform Bar Examination, scoring in the 90th percentile of real-life test takers.[1] From demonstrating legal expertise to crafting convincing deep-fakes, the capabilities of artificial intelligence are exponentially skyrocketing beyond human intelligence. Artificial intelligence is advancing at a pace that not only challenges traditional legal frameworks but also risks entrenching existing societal inequities and reinforcing monopolistic power structures. While harboring unparalleled potential for positive innovation, the rapid evolution of this technology also carries significant risks—specifically, the corruption by unchecked corporate interests and discriminatory algorithms.

While President Biden’s administration approached AI regulation with a balance between innovation and oversight, the Trump administration has signaled a stark departure from this model. President Trump’s executive order on AI, emphasizing deregulation and American leadership in technological innovation, prioritizes acceleration over accountability. But as an international joint statement with the Department of Justice (DOJ) and Federal Trade Commission (FTC) asserted, “given the speed and dynamism of AI developments…we are committed [to] address any such risks before they become entrenched or irreversible harms.”[2] Contextualized by Trump’s larger attack on DEI, his deregulatory approach to AI risks the unmitigated spread of identity bias in governmental and private spheres. Irresponsible governmental use of AI could violate the Equal Protection Clause under the Fifth and Fourteenth Amendments through inadvertent algorithmic discrimination; already, the Trump administration has been accused of using ChatGPT to generate questionable tariff formulas.[3] The more localized integration of AI into business practices risks violating the Civil Rights Act of 1964, which prohibits discriminatory practices in businesses, employment, public facilities, schools, and more.[4] Moreover, Trump’s overfocus on American “global AI dominance” and revocation of Biden’s orders on maintaining a “fair, open, and competitive ecosystem and marketplace for AI”  may implicitly risk violating the Sherman Antitrust Act by encouraging nationalist technological monopolization.[5],[6] Thus, enforcing antitrust and anti-discrimination law will be crucial to ensure American AI policy respects constitutional and legal precedents, while also protecting the democratic rights of citizens nationwide.

Policy Background

From 2023 to 2025, President Biden signed three executive orders outlining the United States’ federal approach to AI regulation, focusing respectively on high-level responsibility, advancing U.S. leadership in AI infrastructure, and promoting the development of cybersecurity.[7] He pioneered a philosophy emphasizing both innovation and security, embodying how authentic technological leadership does not only come from unfettered development, but also from alignment to democratic values and human rights.

The Trump administration’s recent rescinding of Biden’s foundational AI executive orders, however, suggests a dangerous pivot. Rather than viewing innovation and regulation as mutually reinforcing, Trump’s approach frames them as opposing forces.[8] This false dichotomy risks stripping the United States of the very guardrails needed to manage the existential and ethical risks of AI: in “[revoking] certain existing AI policies and directives that act as barriers to American AI innovation,” it also revokes the necessary democratic tenets to ensure the safety of a society under AI. On the federal level, especially with the increasing consolidation of executive power, Congress will likely only regulate localized AI issues, leaving AI unregulated on the federal scale, which risks spiraling into blatant violations of civil rights statutes and even antitrust law.

The risks posed by misaligned AI—systems whose outputs and objectives diverge from human values—magnify under insufficient equity-based governance, leading to potential violations of both the Equal Protection Clause in government AI usage and civil rights statutes like the Civil Rights Act in private jurisdictions.[9] AI systems, though built by humans, can evolve in ways that are opaque and unpredictable, especially when trained on biased data without accountability mechanisms. Without regulation of AI training, computational power, and implementation, these systems have already exacerbated unconstitutional and discriminatory social constructs—and are likely to continue doing so. Under President Trump’s administration, which has systematically rolled back DEI-based hiring practices and other equity-centered safeguards, the danger of AI systems amplifying racial, gender, and economic bias becomes even more concerning. Anti-discrimination laws thus become especially critical.

Moreover, federal and state governments alike have recognized the growing necessity of maintaining fair competition in the rapidly evolving landscape of artificial intelligence. The Sherman Antitrust Act of 1890 set a seminal legal precedent for protecting fair competition.[10] In line with the Sherman Act, unchecked corporate consolidation poses a threat to innovation, consumer choice, and democratic accountability in all sectors, including AI. In July 2024, the FTC, DOJ, and international antitrust enforcers affirmed their commitment to protecting competition in AI, signaling the importance of preventing monopolistic behavior regarding this skyrocketing technology. Many statewide legislators have passed bills requiring watermarks, disclaimers, and other forms of identification to delineate AI from human-generated material; for instance, California’s Health Care Services Bill ordains AI-generated patient communications regarding clinical services to include a clear AI disclaimer.[11] Meanwhile, current copyright law has generally upheld the “human authorship” standard, denying protection to AI-generated content—yet copyright has become an outsized focus of legislative debates.[12] An overly narrow emphasis on intellectual property distracts from the more pressing need to enforce antitrust laws and computational power limits, which are essential to curbing illegal monopolistic control and ensuring AI develops in the public interest.

In sum, antitrust laws and anti-discrimination safeguards must be proactively addressed and regulated to ensure the responsible, constitutional, and legally sound use of this technology, especially under Trump.

Antitrust Enforcement in AI

Trump’s executive order posits that American AI must be developed free from “engineered social agendas,” yet that goal is incompatible without AI regulation that fundamentally promotes fair competition.[13] If the executive government truly wants to champion human-centered and practical technological development, it must ensure that no single body can dominate the field of AI at the expense of the public good—including the United States as a whole. Two federal agencies, the DOJ and the FTC, are already emphasizing the importance of challenging algorithm-driven collusion, exclusionary conduct, and monopolistic conduct in the AI space.

While the Trump administration and the FTC have recently cracked down on “Big Tech” through various lawsuits—including a ruling in April 2025 that Google illegally maintained a monopoly in online advertising—his overarching promise to deregulate the AI industry presents a long-term antitrust jeopardy that legal experts and investors alike recognize.[14] His deregulatory philosophy bares an unsustainable future, where companies engage in unregulated development of advanced AI models, collectively monopolizing the computational infrastructure required to train and deploy them. Without proper regulation coupled with an explicit regulatory philosophy, Trump’s promise of American industrial “dominance” threatens to perpetuate disruptive innovation, where corporate interests dictate the development of AI instead of social good.

From a policy standpoint, a strong antitrust strategy for AI should include several key provisions, as outlined in  the DOJ and FTC joint statement.[15] First, regulators should consider imposing computational power thresholds to prevent a handful of firms from hoarding the infrastructure necessary for advanced model development. Second, the government should promote interoperability among AI systems, ensuring that smaller firms can build tools that interact with dominant platforms. Third, agencies must scrutinize partnerships that risk undermining healthy competition or co-opting open-source innovations. Fourth, fair use exceptions should be explored as a response to aggressive copyright claims used to block competition.  Finally, public transparency requirements must be established to keep consumers informed about how AI systems operate and interact with personal data.

Combating Algorithmic Discrimination

In a striking contradiction, Trump’s executive order declares that AI systems must be free from ideological bias, while simultaneously rescinding Biden-era directives designed to address algorithmic discrimination. The elimination of diversity, equity, and inclusion (DEI) standards from federal hiring and contracting policy further weakens safeguards against systemic bias, just as these issues become more urgent due to AI’s expanding influence.

AI systems often inherit and amplify biases embedded in training data. Without strict oversight, they can lead to discrimination in hiring, healthcare, law enforcement, and countless other domains. For example, a 2019 study revealed that a hospital’s AI-based triage system disproportionately under-prioritized Black patients, recommending less care for them despite equal or greater health needs.[16] This AI system discriminated on the basis of race, violating Title VI of the Civil Rights Act, which prohibits discrimination in entities that receive federal financial assistance, which includes most American hospitals.[17] Similarly, a 2024 study found that AI-based resume-screening software discriminated against female and minority applicants, favoring white male candidates even with equivalent qualifications.[18] This study exemplifies an AI-based violation of Title VII of the Civil Rights Act, which outlaws employment discrimination on the basis of characteristics like race or sex.[19]

While the current federal environment is not realistically conducive to explicitly enforcing such legislation, state-based initiatives offer a promising path forward. New Jersey’s Law Against Discrimination (LAD), for instance, explicitly prohibits algorithmic discrimination, setting a model that other states can adopt.[20] The FTC has also already taken meaningful action, such as barring Rite Aid from deploying a facial recognition system that disproportionately targeted Black, Latinx, Asian, and female customers in identifying customers who are likely to shoplift.[21] To build on this momentum, policymakers must codify algorithmic fairness requirements into civil rights law and mandate impact assessments for AI systems deployed in sensitive domains. These policies must include strong enforcement mechanisms, independent audits, and clear channels for redress by individuals harmed by discriminatory AI systems—only then can America’s approach towards AI duly respect the constitutional and civil rights every citizen possesses. Otherwise, the Trump administration risks infringing upon the very foundation of equality that the American legal system is purportedly built upon.

Conclusion

The unsettling rhetoric of Trump’s executive order surrounding AI implies a long-term federal approach towards AI that risks undermining constitutional protections for equal rights, not to mention antitrust legal precedent necessary for maintaining healthy economic competition. To ensure the ensuing legal precedents do not continue down an unconstitutional and unsustainable path, policymakers must adopt a regulatory framework that prioritizes responsibility, equity, and human safety over unchecked innovation. To prevent AI from exacerbating inequality, spreading misinformation, or threatening security, legislators must focus on antitrust regulation and combating algorithmic discrimination in particular. While implementing these measures poses challenges, especially under a deregulatory administration, inaction risks a consequence far greater than the technological domain—straying from the foundations of equity that the American society and economy depend upon. State governments must step up in the face of federal retrenchment, while federal agencies like the FTC and DOJ continue rigorous antitrust enforcement.

The U.S. cannot afford to regulate AI in a piecemeal or politically reactive manner. A proactive and equity-centered approach is essential to ensure that this powerful technology strengthens democracy rather than undermines it.

Regulating Artificial Intelligence: U.S. and International Approaches and Considerations for Congress
CRS PRODUCT (LIBRARY OF CONGRESS)Hide Overview
CRS Product Type: 	Reports
CRS Product Number: 	R48555
Topics: 	Foreign Affairs; Science & Technology
Publication Date: 	06/04/2025
Author: 	Harris, Laurie

Download PDF (1MB) | PDF Version History
Listen

Contents

    Introduction
    Defining AI
    Regulatory Considerations
    AI Governance and Regulation in the United States
    Federal Laws Addressing AI
    Congressional AI Activities
    Executive Branch AI Actions
    U.S. Approaches to Regulating AI
    Regulating the AI Technologies
    Regulating the Use of AI Technologies Across Sectors
    Regulating the Use of AI Technologies Within Sectors
    Selected International Approaches to AI Governance and Regulation
    United Kingdom
    European Union
    China
    Multi-Country and Bilateral AI Governance Activities
    Policy Considerations and Options for Congress
    Leveraging Existing Frameworks
    Creating New AI Regulations or Authorities
    Supporting U.S. AI Development and Deployment
    Engaging with International Efforts to Regulate AI

Figures

    Figure 1. European Union AI Act's Risk-Based Regulatory Approach

Summary

Artificial intelligence (AI) presents many potential benefits and challenges in the private and public sectors. No federal legislation establishing broad regulatory authorities for the development or use of AI or prohibitions on AI has been enacted. Recent Congresses have passed primarily more targeted AI provisions. Different Administrations have focused their attention on federal engagement in AI, albeit with somewhat different emphases on specific topics. The focus on AI safety under the Biden Administration appears to be shifting toward security concerns during the second Trump Administration. Stakeholders in the United States have debated how to approach AI innovation and regulation in order to harness the opportunities of AI technologies, such as enhanced government operations and worker efficiency, while minimizing potential problems, such as bias and inaccuracies in AI-generated output.

U.S. AI Governance and Regulation

Outside of broad AI governance frameworks, most of the U.S. regulatory efforts regarding AI have centered on (1) federal agency assessments and enforcement of existing regulatory authorities, (2) exploration of whether individual agencies require additional authorities, and (3) securing voluntary commitments from industry. Much of the legislation proposed in the 118th and 119th Congresses have emphasized the development of voluntary guidelines and best practices and reporting of industry-conducted evaluations of AI systems. The approach of the U.S. federal government as a whole appears to be cautious in regard to regulating AI in the private sector and more focused on oversight of federal government uses of AI. In the absence of federal AI regulations, states have been enacting their own laws. Critics assert that such a patchwork of AI laws creates challenges for companies and that a nationwide regulatory structure may incentivize product development.

Approaches to AI Governance and Regulation, Including International Approaches

Proponents of broad federal AI regulations assert that they would lead to less legal uncertainty for AI developers and improve the public's trust in AI systems, thus supporting AI innovation. Opponents of broad federal AI regulations assert that industry is taking steps to self-regulate and that additional regulation would stifle innovation at a time when international competition in AI is accelerating, which could lead to negative economic and national security outcomes for the United States. Other analysts have criticized such characterizations as presenting a false dichotomy between regulation and innovation and instead support a mixture of targeted, flexible approaches depending on the AI technology and its application.

Similar to the United States, some other countries, including the United Kingdom, have taken to date a measured approach to regulating AI. In contrast, the European Union (EU) has enacted a broad regulatory approach through the EU AI Act, which classifies AI systems into risk categories with different degrees of requirements and obligations. Some analysts have raised concerns that the EU AI Act creates or will create barriers for companies developing and deploying AI. China has enacted targeted AI laws and is working on broader AI regulation, though China's economic and science and technology policies feature a heavy government role in private sector development. China's approach has been characterized as a vertical, technology-specific framework influenced by national security concerns and economic development goals, with the EU's AI Act described as a horizontal, risk-based framework focused on ethical considerations and transparency.

Considerations and Options for Congress

Congress and the Administration might frame AI legislation and policies in various ways. One approach could include actions aimed at promoting AI safety and averting risks to people, another approach may focus more on security, and yet another approach may focus on accelerating innovation potentially accompanied by voluntary commitments from industry. These approaches, among others, may also be combined. Congressional actions might focus on leveraging federal agencies' existing authorities without enacting additional AI-specific laws or on creating new cross-sector authorities or broad regulations to address potential risks from AI, such as transparency and accountability requirements. Additionally, congressional actions might focus on providing federal agencies with authorities or direction to support domestic AI development—such as through public-private partnerships and providing resources for AI research and education—or engaging with international efforts to harmonize AI governance. In a time of rapid AI development, such efforts may need to frequently evolve or incorporate mechanisms for periodic review and flexibility at the state and federal levels.

Introduction

Artificial intelligence (AI) presents many potential benefits and challenges in the private and public sectors.1 In the United States, there has been broad debate about how to harness the opportunities of AI technologies, such as through enhanced operations and worker efficiency, while minimizing potential problems, such as bias and inaccuracies in AI-generated output.

Generally, proponents of comprehensive federal AI regulations assert that such regulations would lead to less legal uncertainty for AI developers and improve the public's trust in AI systems, thus supporting AI innovation. Opponents of broad federal AI regulation assert that the AI industry is already taking steps to self-regulate and that additional regulation would stifle innovation and competitiveness at a time when international competition in AI is accelerating, which could lead to negative economic and national security outcomes. Other analysts have criticized such characterizations as presenting a false dichotomy between regulation and innovation and instead support a mixture of targeted, flexible approaches depending on the AI technology and its application.

This report provides an overview of current federal laws pertaining to AI, approaches to AI regulation and governance efforts in the United States and other selected countries—as well as multi-country governance proposals—and selected policy considerations and legislative options for Congress. The scope of this report does not extend to AI infrastructure or equipment, such as AI computing chips and data centers, or related export controls. For information on AI infrastructure topics, see CRS In Focus IF12899, Data Centers and Cloud Computing: Information Technology Infrastructure for Artificial Intelligence, by Ling Zhu.

Defining AI

There is no single, widely agreed upon definition of AI. However, some common descriptions and features have emerged. Congress has previously enacted laws with definitions of AI, such as through the National AI Initiative Act of 2020 (Division E of the William M. [Mac] Thornberry National Defense Authorization Act [NDAA] of FY2021, P.L. 116-283),2 which defines AI as

a machine-based system that can, for a given set of human-defined objectives, make predictions, recommendations or decisions influencing real or virtual environments. Artificial intelligence systems use machine and human-based inputs to- (A) perceive real and virtual environments; (B) abstract such perceptions into models through analysis in an automated manner; and (C) use model inference to formulate options for information or action.

Federal agencies have put forth definitions of AI systems as well, such as that in the National Institute of Standards and Technology's (NIST's) Artificial Intelligence Risk Management Framework (AI RMF).3 The NIST AI RMF incorporates a definition of AI first put forth by the Organization for Economic Cooperation and Development (OECD) in 2019. While the definition of AI system remains unchanged in the AI RMF, in March 2024, the OECD updated its broad definition of AI to more explicitly reference the generation of content, in part responding to the emergence of general-purpose and generative AI.4 The updated definition is contained in the OECD AI Principles, which have been adopted by 47 countries—including the United States and the EU—as of May 20255:

An AI system is a machine-based system that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments. Different AI systems vary in their levels of autonomy and adaptiveness after deployment.

In an explanatory memorandum about the 2024 updated definition for an AI system, the OECD states, "While the definition is necessarily short and concise, its application in practice depends on a range of complex and technical considerations."6

Along these lines, a variety of factors can complicate defining AI for legislative and regulatory efforts. First, AI can be considered an umbrella term for various technologies (e.g., facial recognition technology, or FRT), and applications (e.g., medical image classification in health care, large language models for text chatbots). Second, AI technologies and applications have been evolving rapidly. Whether and how to craft definitions for AI in legislation necessarily involves consideration of the current and future applicability of a definition. For example, policymakers might consider whether a definition is expansive enough to not hinder the future applicability of a law or regulation as AI develops and evolves while being narrow enough to provide clarity on the entities the law affects.

As state and local governments may use different descriptions of AI, Congress could provide a common definition of AI as part of any federal laws to establish governance and regulatory frameworks for AI technologies. The importance of agreed-upon AI terminologies has been described in U.S.-international efforts. For example, the United States–European Union Trade and Technology Council previously stated

AI terminology is pivotal to cooperation on AI in part due to the present momentum in the field, and due to the broader role of language in constructing and explaining scientific paradigms. Terminology is a necessary basis for technical standards and creates shared frames of reference between like-minded partners and across disciplines. Ultimately, different terminologies express distinct "technological cultures," thus revealing, through both alignment and divergence, the existence of gaps, unnecessary divergences and inconsistencies, and other points of departure for cooperation and collaboration.7

Regulatory Considerations

As with many technologies, AI itself is neither inherently good nor inherently bad. AI technologies can provide many benefits, such as increasing worker efficiency and productivity, accelerating research discoveries, and improving responses to cybersecurity incidents. At the same time, AI can present challenges and risks, such as job loss from work task automation, harms to civil liberties from biases or out-of-scope use, and potential loss of privacy through surveillance with technologies such as FRT.

Proponents of additional AI regulations argue that they are necessary to mitigate potential risks from AI systems that might function dangerously or unpredictably, thereby improving safety. As defined by the International Organization for Standardization, safe AI systems should "not under defined conditions, lead to a state in which human life, health, property, or the environment is damaged."8 Proponents further assert that regulation is needed to, for example, protect against potential discriminatory outcomes that might disproportionately impact marginalized populations and determine liability in the event of AI errors or misuse. Some stakeholders have asserted that well-designed regulations could include "stability and clarity for innovation."9

Opponents of additional regulation assert that such risks can be mitigated by applying current federal laws and agency authorities to AI technologies rather than creating new regulations too soon.10 Some stakeholders have expressed that it is difficult to develop and optimize new regulations for advanced, broadly applicable, rapidly developing technologies, such as AI. Opponents often claim that additional regulations might stifle AI innovation—particularly for smaller companies and startups with fewer resources to use for complying with new regulations—and disincentivize experimentation that could lead to AI systems with fewer risks and improved safety.11 More broadly, stakeholders have raised concerns that potential constraints to U.S. AI innovation from new regulations might lead to the United States losing its historically dominant position in a global "race" to develop advanced AI. For example, according to one academic analysis, "the U.S. still leads in producing top AI models—but China is closing the performance gap."12

Congress and the Administration might frame AI legislation and policies in various ways that reflect different priorities. One approach might include promoting AI safety and averting risks to people, while another approach might focus on security. Still other approaches might focus on accelerating innovation, potentially accompanied by voluntary commitments from industry. These approaches might be combined in various ways in tandem with other forms of AI regulation. Regardless, the technical challenges of how to ensure that AI systems align with human goals and values—however those are defined for specific cultural, legal, and societal contexts—remain,13 and overcoming those challenges in reliable and scalable ways are ongoing areas of research and testing.

A September 2024 report by the Stanford University Cyber Policy Center summarized the authors' views of the debates around regulation of AI as follows:

Regulation [of AI] is both urgently needed and unpredictable. It also may be counterproductive, if not done well. However, governments cannot wait until they have perfect and complete information before they act, because doing so may be too late to ensure that the trajectory of technological development does not lead to existential or unacceptable risk.14

Policymakers may shape the boundaries of AI development—technical, societal, legal, and ethical15—and deployment and may aim to do so in ways that support innovation; enhance benefits broadly; and protect Americans' privacy, civil rights, and civil liberties.16 In a time of rapid AI development, such efforts may need to frequently evolve or incorporate mechanisms for periodic review and flexibility.

AI Governance and Regulation in the United States

U.S. federal laws and policy documents have included language on supporting AI innovation while managing risks. For example, an April 2025 government-wide policy memorandum from the Office of Management and Budget (OMB) directed federal agencies to accelerate their use of AI by focusing on innovation, governance, and public trust; to implement risk management practices; and to "prioritize the use of AI that is safe, secure, and resilient."17 The OMB memorandum describes "risks from the use of AI" as including those "related to efficacy, safety, fairness, transparency, accountability, appropriateness, or lawfulness of a decision or action resulting from the use of AI."18

These overarching aims are reflected in the NIST AI RMF as well. The following subsections provide an overview of federal activities in the legislative and executive branches and U.S. approaches to regulating AI technologies.

Federal Laws Addressing AI

While Members of Congress have introduced hundreds of bills including the term artificial intelligence since the 115th Congress, fewer than 30 have been enacted as of May 2025. Of those, nearly half consisted of AI-focused provisions either in appropriations or national defense authorization legislation. Arguably the most expansive law was the National Artificial Intelligence Initiative Act of 2020 (Division E of the NDAA of FY2021, P.L. 116-283). The act

    codified the American AI Initiative;
    established a National Artificial Intelligence Initiative Office to support federal AI activities, which was launched in January 2021 under the first Trump Administration19;
    established an interagency committee at the Office of Science and Technology Policy to coordinate federal programs and activities in support of the law, and
    established a National AI Advisory Committee, which produced over 30 reports between 2022 and 2024,20 including recommendations for the current Trump Administration.21

The act also directed AI activities at selected federal science agencies, including National Science Foundation (NSF) support for a network of National AI Research Institutes. Other laws have also focused on federal AI research and development (R&D). For example, the CHIPS and Science Act (P.L. 117-167) included numerous AI-related provisions directing certain federal science agencies to support AI R&D activities and the development of technical standards and guidelines related to safe and trustworthy AI systems.22 Since the law was enacted, various federal programs established by the act have provided grants in support of AI R&D, such as the NSF's Regional Innovation Engines program and the Economic Development Administration's Regional Technology and Innovation Hubs program.23

Additional laws have directed individual federal agencies—including the General Services Administration (GSA) and OMB—to support the use of AI across the federal government:

    The AI in Government Act of 2020 (Division U, Title I, of the Consolidated Appropriations Act, 2021, P.L. 116-260) created within GSA an AI Center of Excellence to facilitate federal adoption of AI and collect and make public information regarding federal programs, pilots, and other initiatives.24 The act required OMB to issue a memorandum to federal agencies regarding the development of AI policies; approaches for removing barriers to using AI technologies; and best practices for identifying, assessing, and mitigating any discriminatory impact or bias and any unintended consequences of using AI.25
    The Advancing American AI Act (Subtitle B of Title LXXII of Division G of the FY2023 NDAA, P.L. 117-263) required OMB to (1) incorporate additional considerations when developing guidance for using AI in the federal government26; (2) develop an initial means to ensure that federal contracts for acquiring AI address privacy, civil rights and liberties, and the protection of government data and information27; and (3) require the head of each federal agency (except the Department of Defense) to prepare and maintain an inventory of current and planned AI use cases.28

Other AI-related laws focused on certain aspects of AI training29 and sector-specific use.30 None established broad regulatory authorities for the development or use of AI or prohibitions on AI use.

State AI Laws

According to the National Conference of State Legislatures, as of late April 2025, at least 48 states and Puerto Rico had introduced into their legislatures more than 1,000 bills that include the term AI in the 2025 legislative season.31 Those bills range widely in scope, including mentions of AI in broader legislation (such as commemoration resolutions and commendations), the establishment of task forces to study aspects of AI, and requirements for disclosures regarding AI-generated content (such as in political advertisements and mental health chatbots). Regarding bills focused on AI, in prior years, states have enacted a range of cross-sector AI legislation impacting private sector AI developers and deployers, including the following examples:

    In September 2024, California enacted multiple pieces of AI legislation, including those that require generative AI developers to digitally mark AI outputs (S.B. 942) and require AI developers to disclose information about which data they use to train their models (A.B. 2013).32 Governor Newsom vetoed an AI safety testing bill (S.B. 1047), which would have required developers of certain large-scale AI models (over certain computing power and cost thresholds) to test them before releasing and would have authorized the state attorney general to bring civil actions for specified critical harms (e.g., harms from the creation of chemical, biological, radiological, or nuclear weapons or from cyberattacks on critical infrastructure).33
    In May 2024, Colorado enacted AI legislation (SB 24-205) focused on consumer protections, safety, and disclosure of use.34 Subsequently, Colorado's House Bill 24-1468 created an AI Impact Task Force to consider and propose recommendations regarding protections for consumers and workers from AI systems and automated decision systems. That task force released a report with recommendations to clarify SB 24-205.35
    In March 2024, the State of Washington enacted legislation (ESSB 5838) to establish an AI task force to examine the development and use of AI by public and private sector entities and make recommendations to the Washington state legislature regarding guidelines and potential legislation for the use and regulation of AI systems to protect safety, privacy, and civil and intellectual property rights.36

Congressional AI Activities

Recent Congresses have established various AI-focused caucuses, task forces, and working groups and held numerous hearings on AI topics as part of their AI-focused policymaking activities. For example, as in prior sessions, the 119th Congress has established AI caucuses in both the Senate and the House.37 These caucuses enable Members of Congress to exchange information and ideas with colleagues.38

Leadership in both the Senate and the House previously established groups to inform the development of AI policies and legislation. In the 118th Congress, the Senate's Bipartisan AI Working Group sought to "complement the traditional congressional committee-driven policy process" given the cross-jurisdictional nature of AI.39 After hosting nine AI Insight Forums40 to bring AI expertise from across industry, academia, and civil society to the Senate, the working group released a roadmap for AI policy in the Senate in May 2024.41 The roadmap identified areas of consensus for AI policy development, including in supporting U.S. innovation in AI; AI and the workforce; high impact uses of AI; elections and democracy; privacy and liability; transparency, explainability,42 intellectual property, and copyright; safeguarding against AI risks; and national security.43

In February 2024, House leadership during the 118th Congress announced the establishment of a bipartisan, 24-Member Task Force on Artificial Intelligence "to explore how Congress can ensure America continues to lead the world in AI innovation while considering guardrails that may be appropriate to safeguard the nation against current and emerging threats."44 The task force reported holding multiple hearings and roundtables to engage with experts, after which it released a report in December 2024 with 66 key findings and 89 recommendations organized into 15 chapters.45 The Task Force report also adopted high-level policy considerations to guide future congressional efforts: identify AI issue novelty (i.e., whether a policy issue is "truly new for AI due to capabilities that did not previously exist" in order to avoid duplicative legislative mandates), promote AI innovation, protect against AI risks and harms, empower government with AI, affirm the use of a sectoral regulatory regime, take an incremental approach, and keep humans at the center of AI policy.

Executive Branch AI Actions

The U.S. executive branch has worked to establish federal government-wide AI initiatives and activities, including through executive orders, strategic plans, reports, policy memoranda, and advisory and coordination committees.46 Over multiple Administrations—including both Trump Administrations and the Biden Administration—executive branch policy has been set by a series of executive orders. Subsequent Administrations have rescinded or modified the executive orders or implementation approaches of previous Administrations. As a consequence, agencies in prior Administrations may have engaged in activities not continued in subsequent Administrations due to changing executive direction. Particularly relevant to this discussion is President Trump's rescinding in 2025 of President Biden's executive order of 2023 on AI, as described below.

Both the Trump and Biden Administrations took a variety of actions related to AI, with policy documents including language on supporting AI innovation while assessing for and managing potential risks. For example, the aforementioned government-wide policy memoranda from OMB regarding federal agency use of AI have directed agencies to advance AI governance and innovation while managing the risks of AI.47 In addition to the NIST AI RMF, these overarching aims have been reflected in other documents, such as President Trump's 2025 Executive Order (E.O.) 14179, Removing Barriers to American Leadership in Artificial Intelligence48; President Biden's 2023 E.O. 14110, Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence,49 subsequently rescinded by President Trump50; President Trump's 2020 E.O. 13960, Promoting the Use of Trustworthy Artificial Intelligence in the Federal Government51; and 2019 E.O. 13859, Maintaining American Leadership in Artificial Intelligence.52

During the Biden Administration, E.O. 14110 directed over 50 federal agencies to engage in more than 100 specific actions across eight overarching policy areas.53 The E.O. also used Defense Production Act authorities54 to require (1) companies developing, or intending to develop, certain dual-use AI models to report to the government on model training, testing, and data ownership and (2) entities that acquire, develop, or possess potential large computing infrastructure to report to the government on location and amount of computing power. Those requirements are no longer in force after President Trump's E.O. 14148 revoked E.O. 14110 on January 20, 2025.55 The extent to which these activities may support current aims is to be determined. The Trump Administration has directed a review of any actions taken pursuant to E.O. 14110.56

The Trump Administration's E.O. 14179 set forth a policy "to sustain and enhance America's global AI dominance"57 and called for an "AI Action Plan" by July 30, 2025, inviting public comment on policy ideas for the plan.58 The comment period on the plan ended on March 15, 2025, with more than 8,700 comments reportedly received.59 In February 2025, Vice President Vance, in remarks at the AI Action Summit in Paris, France, stated that the U.S. plan under preparation "avoids an overly precautionary regulatory regime."60 Through such actions, the current Administration has signaled an intention toward what it characterizes as pro-economic-growth AI policies, potentially with a comparatively smaller federal role.

Federal agencies have been reporting their internal AI use cases in what is called the "AI Use Case Inventory" pursuant to E.O. 13906 and the Advancing American AI Act and OMB Memorandum M-25-21.61 As of January 23, 2025, agencies reported over 1,990 current and planned AI use cases (excluding retired AI use cases).62 The top three categories for AI uses in that inventory were mission-enabling (internal agency support), health and medical, and government services (which includes benefits and service delivery).63 This inventory of AI use cases is intended to increase transparency to assist with oversight of agency activities and investments. However, there have been concerns raised about the comprehensiveness and accuracy of the inventories as well as recommendations put forth to improve reporting. For example, in December 2023, the Government Accountability Office (GAO) made 35 recommendations to 19 federal agencies to fully implement federal AI requirements. As of May 2025, 31 of GAO's recommendations were listed as open.64 Additionally, in February 2024, the National AI Advisory Committee recommended that the federal AI use case inventory be expanded by limiting certain categories of exceptions, such as the reporting exceptions for sensitive law enforcement uses and for common commercial products.65

U.S. Approaches to Regulating AI

Beyond broad AI governance frameworks set forth above, most of the U.S. regulatory efforts regarding AI have centered on federal agency assessments and enforcement of existing regulatory authorities, exploration of whether individual agencies require additional authorities, and securing voluntary commitments from industry. Further, many of the proposed bills in the 118th and 119th Congresses have emphasized the development of voluntary guidelines and best practices and reporting of industry-conducted evaluations of AI systems rather than prohibitions or independent evaluation of AI uses and technologies. The approach of the U.S. federal government as a whole appears to be more focused on oversight of federal government uses of AI than regulating AI in the private sector.

In terms of approaches to regulating AI, federal government activities may be broadly thought of in the following categories: regulating the AI technologies directly, regulating the use of the AI across sectors—with a more agnostic approach to the technological cause of the outcome—and regulating the use of AI within particular sectors.

Regulating the AI Technologies

One approach that has been taken by some executive actions and introduced legislation has been regulating AI technologies themselves, such as by using a technical threshold such as computing power or proposing transparency requirements.

For example, as discussed earlier in the report, E.O. 14110, when it was in place, required companies developing or intending to develop certain dual-use AI models to report to the federal government on model training, testing, and data ownership. The initial technical conditions that triggered the reporting requirements included "any model that was trained using a quantity of computing power greater than 10^26 integer or floating-point operations" (FLOPs).66 At the time of introduction of E.O. 14110, the minimum computation threshold that triggered a reporting requirement reportedly exceeded the computational power required for AI models in use at that time.67 According to one analysis updated on May 6, 2025, numerous models now surpass 10^25 FLOPs, and one—xAI's Grok-3—has passed 10^26 FLOPs.68 As an example of transparency requirements for particular AI technologies, the AI Disclosure Act of 2023 (H.R. 3831, 118th) would have required that any output of generative AI include a disclaimer that it was generated by AI, enforced by the Federal Trade Commission (FTC). In the 119th Congress, the Quashing Unwanted and Interruptive Electronic Telecommunications Act (H.R. 1027) would require disclosures with respect to robocalls using AI.

On one hand, a technology-specific approach might result in more targeted accountability and oversight of AI applications. On the other hand, such an approach might be rigid and not fully account, even in the short term, for potential risks, as AI technologies have been evolving rapidly.

Regulating the Use of AI Technologies Across Sectors

Some Members of Congress have proposed to regulate the use and oversight of AI across sectors, sometimes taking a more technology-neutral approach. For example, the Algorithmic Accountability Act (S. 2892 and H.R. 5628, 118th) would have directed the FTC to require impact assessments of automated decision systems (including but not limited to AI) and augmented critical decision processes (ACDP)69 from certain covered commercial entities (e.g., large companies). The bill would have required covered entities to attempt to eliminate or mitigate any negative impacts on a consumer's life from an ACDP. The bill would also have required the FTC to develop a publicly accessible repository to publish a limited subset of information about each automated decision system or ACDP for which the agency received a report. Additionally, the bill would have required the FTC to provide guidance and technical assistance to covered entities and to establish a Bureau of Technology to aid and advise the FTC with respect to enforcement of the act.

While legislation such as the Algorithmic Accountability Act would take a broad approach to regulating AI and algorithmic systems, it would have stipulated evaluation and reporting requirements and did not contain specific prohibitions on use. Some analysts have asserted that focusing on automated systems, rather than AI explicitly, better captures the technical features of concern and avoids the questions of defining and delineating AI from non-AI systems.70 Critiques of an approach that places requirements only on large companies assert that many smaller companies may provide products to a range of customers, including state and local governments, and thus those risks would not be addressed.71 As an alternative, policymakers might implement similar requirements for all companies but provide support (e.g., technical or financial) for small- and medium-size enterprises (SMEs).

Regulating the Use of AI Technologies Within Sectors

Some legislative approaches have focused on the use of AI technologies within particular sectors. For example, Members of Congress have introduced legislation focusing on governance and regulation of AI uses within the financial sector, elections and campaign finance, and health care. Examples of bills pertaining to AI uses in the financial sector include the Preventing Deep Fake Scams Act (H.R. 1734), which would "establish the Task Force on [AI] in the Financial Services Sector"; and the AI PLAN Act (H.R. 2152), which would "require a strategy to defend against the economic and national security risks posed by the use of [AI] in the commission of financial crimes."72 Regarding AI uses in elections and campaign finance, example bills include the Fraudulent Artificial Intelligence Regulations Elections Act of 2024 (S. 4714, 118th) and the AI Transparency in Elections Act of 2024 (H.R. 8668, S. 3875, 118th).73 The 118th Congress also introduced S. 4862 "to ensure that new advances in artificial intelligence are ethically adopted to improve the health of all individuals, and for other purposes" pertaining to health care.74

Selected International Approaches to AI Governance and Regulation

Similar to the U.S. federal government, some other countries, such as the United Kingdom (UK), have taken to date a sector-specific approach to regulating AI. In contrast, the European Union (EU) has enacted a broad regulatory approach through the EU AI Act. China has enacted targeted AI laws and is working on a broader AI regulation, though its economic and science and technology policies feature a heavy government role in private sector development. This section of the report provides high-level summary information on the approaches by the UK, EU, and China—as well as multi-country governance proposals and activities—as compared to U.S. approaches.

United Kingdom

The UK does not have a general statutory regulation for AI. Rather, AI is regulated through existing legal frameworks for sectors in which AI is used.75 The UK government has worked to develop national AI strategy and policy documents, action plans, and research institutes. Depending on control of the UK government, the approaches taken toward AI have varied, particularly with respect to the need for new, broad regulation.

For example, in September 2021, the UK National AI Strategy was published76 under the center-right Conservative Party, followed by a 2023 policy white paper, "AI Regulation: A Pro-Innovation Approach."77 The policy paper laid out the UK government's intention to "put in place a new framework to bring clarity and coherence to the AI regulatory landscape."78 The UK framework is meant to have an "agile and iterative approach, recognizing the speed at which these technologies are evolving," underpinned by five principles: safety, security, and robustness; appropriate transparency and explainability; fairness; accountability and governance; and contestability and redress.79 According to the policy paper, these principles would initially be issued on a non-statutory basis and implemented by existing regulators (e.g., the Health and Safety Executive, Equality and Human Rights Commission, and Competition and Markets Authority) using their existing authorities rather than creating a new regulatory entity. However, the UK government would provide central support functions, including

    monitoring and evaluating the framework's effectiveness and implementation of the principles,
    assessing and monitoring risks across the economy arising from AI,
    conducting horizon scanning and gap analysis—including by convening industry—to inform a coherent response to emerging AI technology trends,
    supporting testbeds and regulatory sandbox initiatives to help AI innovators get new technologies to market,80
    providing education and awareness to give clarity to businesses and empower citizens, and
    promoting interoperability with international regulatory frameworks.81

In response to comments on the 2023 policy white paper, the UK government clarified that, after reviewing an initial period without implementing regulations, it anticipated requiring regulators to "have due regard to the [cross-sectoral AI] principles."82

Since then, the UK government has undertaken efforts to support innovation and work toward broader AI regulations. For example, support for a pro-innovation approach was echoed in the January 2025 "AI Opportunities Action Plan,"83 released under the center-left Labour Party, which won the UK general election in July 2024.84 However, the plan also supports "well-designed and implemented regulation, alongside effective assurance tools," arguing that UK regulators "have an important role in supporting innovation as part of their Growth Duty."85 Further, a bill has been introduced in the House of Lords to make provision for the regulation of AI.86 The bill would direct the UK Secretary of State to create an AI Authority to, among other things, ensure alignment of approach across relevant regulators and accredit independent AI auditors.

One analysis described the UK's approach to AI regulation as a "principles-based, non-statutory, and cross-sector framework" that aims to "balance innovation and safety by applying the existing technology-neutral regulatory framework to AI."87 However, that analysis and others assert that AI regulatory activity is expected to increase since the election of a Labour government88 and cite AI legislation in the King's Speech in July 2024.89

European Union

On April 21, 2021, the European Commission released a proposed regulatory framework for AI—the Artificial Intelligence Act (AI Act).90 (For more information about the EU's components, see the text box on "European Union (EU) Institutions" below.) After revisions and negotiations, the act was formally signed in June 2024 and broadly entered into force on August 1, 2024.91 However, none of the act's prohibitions or requirements began to apply until February 2, 2025, as described below, and final implementation actions stretch through 2030.92

European Union (EU) Institutions

The EU is a political and economic partnership representing a form of cooperation among 27 sovereign member states.93 The EU includes three main institutions involved in proposing, approving/rejecting, and implementing legislation:

    The European Parliament is the only directly elected institution of the EU and includes 705 members.
    The European Commission has 27 commissioners representing the interests of the EU as a whole and functions as the EU's primary executive body.
    The Council of the European Union (or the Council of Ministers) has 27 national ministers (the president or prime minister of every member state) representing the interests of the EU's national governments.94

The AI Act adopts a risk-based approach, classifying AI systems into several risk categories, to which different degrees of requirements and obligations apply (see Figure 1)95:

    Unacceptable risk. AI systems with a clear threat to safety, livelihoods, and rights of people are banned. This includes AI systems used in harmful AI-based manipulation and deception, harmful AI-based exploitation of vulnerabilities, social scoring, individual criminal offense risk assessment or prediction, untargeted collection of internet or closed-circuit television material to create or expand FRT databases, emotion recognition in workplaces and education institutions, biometric categorization to deduce certain protected characteristics, and real-time biometric identification for law enforcement purposes in publicly accessible spaces.96 The ban on AI systems posing unacceptable risks went into effect on February 2, 2025.
    High risk. High-risk AI systems can pose serious risks to health, safety, or fundamental rights. These can include AI safety components in critical infrastructure; AI systems that may determine access to education, employment, public services, or financial services; AI-based safety components of products (e.g., in robot-assisted surgery); remote biometric identification; use by law enforcement that might interfere with fundamental rights; use in migration, asylum, and border control management; and AI systems used in courts or the administration of justice. High-risk AI systems are authorized but subject to assessments before they can be placed on the EU market and to post-market monitoring obligations. Pre- and post-market obligations include having risk assessment and mitigation systems; using high-quality datasets to minimize the risk of discriminatory outcomes; creating detailed documentation; and having a high level of robustness, cybersecurity, and accuracy.97 Obligations for all high-risk systems are to apply beginning in August 2026.
    Transparency risk. AI systems that pose risks of impersonation or deception are subject to information and transparency requirements. For example, users must be made aware when they interact with chatbots, and deployers of AI systems that generate or manipulate image, audio, or video content must make sure the content is identifiable (which could include visual labeling or digital watermarking).98 Most of these requirements are to apply beginning in August 2025.
    Minimal or no risk. For AI systems deemed minimal or no risk, such as AI-enabled video games or spam filters, there are no AI Act requirements.
    General-purpose AI (GPAI).99 All GPAI models will have to maintain up-to-date technical documentation—including summary information on the content used in training the models—and comply with EU copyright law. GPAI models trained using a total computing power exceeding a certain threshold (10^25 FLOPS) are presumed to pose systemic risks. Those GPAI models exceeding that threshold must also constantly assess and mitigate risks, such as through documenting and reporting serious incidents and implementing corrective measures.100 Rules for GPAI systems that must comply with transparency requirements are to apply beginning in August 2025.101

Figure 1. European Union AI Act's Risk-Based Regulatory Approach
media/image4.png

Source: CRS, adapted from Tambiama Madiega, "Artificial Intelligence Act: Briefing—EU Legislation in Progress," European Parliamentary Research Service, September 2024, https://www.europarl.europa.eu/RegData/etudes/BRIE/2021/698792/EPRS_BRI(2021)698792_EN.pdf.

Notes: A general-purpose AI (GPAI) model means "an AI model, including when trained with a large amount of data using self-supervision at scale, that displays significant generality and is capable to competently perform a wide range of distinct tasks regardless of the way the model is placed on the market and that can be integrated into a variety of downstream systems or applications."

The AI Act established an EU AI Office to provide advice on, and monitoring of, the implementation of the act.102 Each EU member state also must establish or designate at least one market surveillance authority and at least one notifying authority to ensure the application and implementation of the act. The act requires member states to provide for financial and non-monetary penalties and other enforcement measures for infringements (i.e., non-compliance by the operator with the prohibitions or obligations in the law, or supplying incorrect, incomplete, or misleading information to authorities).103 Financial penalties are expected to range from €7.5 million (or 1.5% of global annual turnover) to €35 million (or 7% of global annual turnover) depending on the type of infringement and size of the company.104

The AI Act also sets forth measures in support of AI innovation.105 For example, the act requires each member state to establish at least one regulatory sandbox to facilitate development and testing of AI systems in real-world settings while under regulatory oversight before entering the market. It also requires member states to undertake innovation measures to specifically help SMEs and startups.

Some analysts have been broadly supportive of the AI Act's risk-based approach and its recognition that risks posed by AI systems vary with the contexts in which they are used. Critics have asserted that the regulation does not go far enough to protect fundamental rights, recommending stronger risk mitigation measures.106 Other critiques have raised concerns that the act's rules create barriers for companies developing and deploying AI, including high initial compliance costs and prolonged time to market for new AI tools, and recommend fewer requirements and lower fines, regardless of company size, to spur innovation.107 According to recent reports and analyses, the EU may be increasingly emphasizing competitiveness and focusing on potential strategies and financial investments to spur EU AI development in an attempt to help overcome regulatory barriers, building on the aforementioned innovation measures in the AI Act.108 Other analysts have posited that the AI Act may provide benefits to European AI developers. For example, developers might focus more on innovation by navigating one standardized regulation rather than a patchwork from individual EU member countries, and they might have a competitive advantage in being able to assert that their AI products are trustworthy because they are in compliance with the AI Act.109

U.S. tech companies, including Meta and Apple, have reportedly declined to launch some AI products in the EU as a result of the AI Act and related regulations, including the Digital Markets Act and data protection rules, citing "the unpredictable nature of the European regulatory environment" and "regulatory uncertainties."110 Broadly, though, most analysis seems to agree that the ultimate impact of the regulation will depend on the details of its implementation, many of which are to be determined, as the majority of the provisions have yet to be implemented.

China

In July 2017, China's State Council issued A New Generation Artificial Intelligence Development Plan, setting broad goals and plans for developing AI technologies and applications and calling for the country to lead the world in AI by 2030.111 Since then, China's regulatory actions have been targeted to particular AI technologies and sector applications, including through the 2023 Provisions on Management of Deep Synthesis in Internet Information Service ("Deep Synthesis Rule") and the 2023 Interim Measures for the Management of Generative Artificial Intelligence Services ("Generative AI Measures").112 The Deep Synthesis Rule regulates the use of deep fake technologies in generating or changing digital content. The Generative AI Measures aim to mitigate risks associated with public-facing generative AI services—including content security, personal data protection, data security, and intellectual property violations—and sets forth a multi-tiered system of obligations for providers of generative AI services (whether the provider is based within or outside of China) to mitigate such risks.113 One analysis characterized China's approach as a vertical, technology-specific framework influenced by national security concerns and economic development goals, contrasting it with the EU's AI Act, described as a horizontal, risk-based framework focused on ethical considerations and transparency.114 While some have described such regulations as restrictive for Chinese AI development, others have asserted that they "offer little protective value to the Chinese public" and instead send "a strong pro-growth signal" to industry.115 One expert described these regulatory actions as "underscoring the state's interest in controlling online news content" and in keeping with the government's "efforts to prevent what it considers political and social disruption and enforce censorship and content regulations more broadly."116

No draft of a broad AI law has been taken under consideration by China's National People's Congress. In March 2024, an expert group in China released a draft law, Artificial Intelligence Law of the People's Republic of China, which reportedly aims to promote innovation in AI technology, develop a healthy AI industry, and regulate AI products and services in a more comprehensive way than previous governance proposals and with more details on the responsibilities for specific actors.117 Also in March 2024, four Chinese government agencies jointly released the Measures for the Labelling of Artificial Intelligence-Generated and Synthetic Content, which are to come into effect as of September 1, 2025. These rules require AI-generated content to be implicitly labelled (i.e., embedded in a digital file's metadata) or explicitly labelled (i.e., easily perceived by users, added to text, audio, images, video, and virtual scenes).118 Some analysis has described the labeling measures as providing clarity on the implementation of the broader Deep Synthesis Rule and Generative AI Measures described above.119

As part of the aforementioned 2017 New Generation AI Development Plan, the Chinese government called for both state and non-state actors to support the central government in pursuing global leadership in AI. The Chinese government plays a large role in supporting and directing R&D in strategic technologies such as AI through a range of state-led industrial and science and technology policies.120 For example, the Chinese government can assert control and influence over private sector firms through acquiring controlling stakes, directly subsidizing companies, and providing government guidance funds (i.e., state-directed public-private investment funds) to seed early-stage AI firms.121 Broadly, the outsized role of the Chinese government in shaping private sector activities represents an overarching difference in AI development and commercialization as compared to the more independent private sector AI activities in the United States, UK, and EU. Some analysts have asserted that a heavy reliance on government funding can lead to inefficiencies, such as misallocated capital and market distortions.122 According to one analysis, though, while the United States has historically led China in private AI investment and translating AI research into real-world applications, foreign investment is slowly increasing for China's generative AI sector, and China is "rapidly closing the performance gap" with the United States.123 Additionally, the Chinese government is reportedly prioritizing a diversified approach to AI, focusing on basic research, core software and hardware technologies, and AI applications.124

Multi-Country and Bilateral AI Governance Activities

The United States has engaged in multilateral AI activities as well as bilateral activities with the EU, UK, and India. Among the major multi-country AI initiatives are those spearheaded by the OECD, the Group of Seven (G7), and the United Nations. For example, in 2019, the OECD member countries committed to a set of common AI principles, which were updated in 2024 to consider new technological developments, notably in general-purpose and generative AI.125 The principles aim to promote inclusive growth, human-centered values, transparency, safety and security, and accountability, and they encourage investments in R&D. In October 2023, the G7 leaders announced an agreement on guiding principles on AI and a voluntary code of conduct for AI developers under the Hiroshima AI process.126 The 11 guiding principles on AI and code of conduct build on the 2019 OECD AI principles and are meant to inform national regulatory efforts implemented by G7 nations "in line with a risk-based approach."127 The UN General Assembly committed to establishing a multidisciplinary Independent International Scientific Panel on AI and initiate a Global Dialogue on AI Governance, calling for co-facilitators from both a developed and a developing country.128 The United Nations, along with some academic and nonprofit stakeholders, have called for more inclusive discussions of AI governance with countries in the Global South.129

In 2020, 15 countries—including the United States—launched the Global Partnership on AI to bring together expertise from a range of stakeholders focused on project-oriented collaboration.130 As announced in July 2024, it partnered with OECD to bring together 44 countries across six continents "to advance an ambitious agenda for implementing human-centric, safe, secure and trustworthy AI," with nationally funded Expert Support Centres located in Canada, France, and Japan.131

Regarding bilateral activities, in June 2021, the U.S.-EU Trade and Technology Council was established,132 serving as a forum to coordinate U.S. and EU approaches to global trade, economic, and technology issues, with cooperation on "responsible AI innovation … in line with shared democratic values."133 As part of the Indo-U.S. Science and Technology Forum, the U.S.-India Artificial Intelligence Initiative was established in March 2021 to serve as a platform to discuss bilateral AI R&D collaboration between the United States and India.134

In November 2023, the United States participated in the UK's AI Safety Summit, which also included participants from 28 countries across the EU, Africa, the Middle East, and Asia. In tandem with the summit, the United States established a U.S. AI Safety Institute (AISI) to operationalize the NIST AI RMF135 by creating guidelines, tools, benchmarks, and best practices for identifying, evaluating, and mitigating AI risks.136 At the same time, the UK announced its own AISI, and in April 2024, the U.S. and UK AISIs agreed to jointly develop tests for the most advanced AI models.137 As of February 2025, the UK AISI has been renamed the AI Security Institute to "reflect AISI's focus on serious AI risks with security implications," removing prior foci on bias or freedom of speech.138 Similarly in the United States, during the second Trump Administration, NIST has reportedly removed certain skills—including AI safety, responsible AI, and AI fairness—as part of updated cooperative agreement language for scientific partners working with the AISI.139

Policy Considerations and Options for Congress

As Congress continues debating whether and, if so, how to regulate AI technologies or their uses, it might evaluate a range of policy considerations and potential legislative options in addition to maintaining the status quo. This section provides a high-level overview of such considerations from a few overarching topic areas: leveraging existing frameworks, creating new AI regulations or agency authorities, and engaging with international regulatory efforts. This section further provides selected potential policy considerations that are common across AI technologies and sectoral applications and are likely to be considered by policymakers in the near and long terms.

Leveraging Existing Frameworks

Certain federal agencies have stated that their existing legal authorities apply to automated systems and new technologies, including AI technologies.

Applying Existing Federal Agency Authorities to AI

In April 2023, officials at the FTC, Consumer Financial Protection Bureau, Equal Employment Opportunity Commission, and the Department of Justice's Civil Rights Division released a "Joint Statement on Enforcement Efforts Against Discrimination and Bias in Automated Systems."140 The joint statement highlighted the agencies' authorities and reiterated efforts to monitor the development and use of automated systems and to protect individuals' rights "regardless of whether legal violations occur through traditional means or advanced technologies."141 As an example of using those authorities with AI systems, in December 2023, the FTC brought an action against the drugstore chain Rite Aid, alleging it acted unfairly in its use of FRT to surveil customers. Rite Aid agreed to a court-ordered settlement that, among other things, prohibited it from using FRT for a period of five years.142

There have been efforts to better understand whether agencies need additional authorities to effectively address any concerns that are specific to advanced AI technologies and tools. For example, in response to the November 2020 E.O. 13859, OMB provided guidance to federal agencies for regulatory and nonregulatory oversight of AI applications developed and deployed outside of the federal government.143 That memorandum laid out 10 principles for the stewardship of AI applications, and it directed federal agencies to provide plans to conform to the guidance, including any statutory authorities governing agency regulation of AI applications, regulatory barriers to AI applications, and any planned or considered regulatory actions on AI. Few agencies appear to have provided comprehensive, publicly available responses. Congress might consider whether an assessment of agency authorities would be valuable for its oversight and deliberations on legislation on AI. If so, options for Congress could include requesting information from federal agencies, directing an investigative entity such as GAO to conduct an assessment across agencies, drafting legislation directing development and provision of agency assessments, and conducting oversight of agency responses.

Congress might consider maintaining the current regulatory and governance environment, leveraging the authorities of federal agencies without enacting additional laws specific to AI technologies. On one hand, this approach would refrain from placing additional regulatory constraints on AI development and deployment, which has been a concern from those focused on international competition and U.S. leadership in AI innovation. On the other hand, in the absence of federal AI regulations, states have been enacting their own laws (see the "State AI Laws" text box). Critics assert that such a patchwork of AI laws creates challenges for companies, which must evaluate and ensure compliance across states, increasing regulatory burden and costs. Further, such costs may disproportionately impact SMEs and startup companies, which likely do not have the same legal and financial resources as larger companies do.

To support AI development without additional restrictions, Congress might consider codifying existing agency activities, such as those at the NIST AISI. For example, the Future of Artificial Intelligence Innovation Act of 2024 (S. 4178, 118th) would have codified the NIST AISI. Additionally, Congress might consider modifying or updating agencies' existing authorities to support regulatory refinement or providing new authorities to support innovation. Options for Congress might include the following:

    Requiring regulatory agencies to provide AI-specific guidance as it relates to their existing authorities. For example, in early January 2025, under the Biden Administration, the Food and Drug Administration released draft guidance for industry "on the use of AI to produce information or data intended to support regulatory decision-making regarding safety, effectiveness, or quality of drugs," with comments due by April 7, 2025.144
    Requiring an interagency effort to create a comprehensive federal regulatory policy for ensuring AI safety that details each agency's responsibilities. One example might be the Coordinated Framework for Regulation of Biotechnology. As described in the Federal Register, the notice for the Coordinated Framework noted that while existing statutes provided a basic network of agency jurisdiction over both research and products, the Coordinated Framework would help assure reasonable safeguards for the public. The notice further stated that the framework was "expected to evolve in accord with the experiences of the industry and the agencies."145 The framework has been in place since 1986 and most recently updated in 2017.146

Such options for a more robust "whole-of-government" approach to AI governance may help address what some analysts have identified as notable areas for improving implementation of legal and policy requirements, such as greater detail and transparency into Agency Compliance Plans147 with AI requirements and the roles of chief AI officers across federal agencies.148

Creating New AI Regulations or Authorities

Various stakeholders have called for new cross-sector authorities or broad regulations to address potential risks from AI models and tools. Among federal proposals are transparency and accountability requirements, such as requirements for impact assessments of AI models, third-party audits of AI tools, and labels or other disclosures (e.g., digital watermarking) for AI-generated content. For example, the Preventing Algorithmic Collusion Act of 2025 (S. 232, 119th) would prohibit the use of pricing algorithms (including AI algorithms) that can facilitate collusion, and it would create an antitrust law enforcement audit tool to increase transparency. The Content Origin Protection and Integrity from Edited and Deepfaked Media Act of 2025 (S. 1396, 119th) would require transparency with respect to content and content provenance information for AI-generated or algorithmically modified digital content. The Artificial Intelligence Research, Innovation, and Accountability Act of 2024 (S. 3312, 118th) would have required online platform operators to disclose the use of generative AI systems and would have implemented transparency reporting requirements on deployers and developers of high-impact AI systems, akin to the risk-based approach in the aforementioned Algorithmic Accountability Act from the 118th Congress.

Some analysts have asserted a need for government-mandated oversight of AI conducted by professional auditors, which could create an industry of AI auditors to "deliver accountability for AI without disincentivizing innovation."149 Some pro-business groups such as the U.S. Chamber of Commerce have called for "thoughtful laws and rules for the development of responsible AI and its ethical deployment," asserting that "failure to regulate AI will harm the economy, potentially diminish individual rights, and constrain the development and introduction of beneficial technologies."150

Various industry stakeholders have echoed calls for regulating AI technologies, including putting forth recommendations.151 However, some analysts have argued that the calls for regulations from technology firms may be intended to protect companies' interests and may not align with the priorities of other stakeholders. For example, at a Senate Judiciary Committee hearing on May 16, 2023, in response to a request from Senator John Kennedy for recommendations on AI regulations, OpenAI CEO Sam Altman stated that he would "form a new agency that licenses any effort above a certain scale of capabilities and can take that license away and ensure compliance with safety standards," in addition to other recommendations, such as requiring independent audits for compliance with safety standards.152 Analysts have raised concerns that such a plan might make it more difficult for others, such as startups and open-source developers, to enter into developing advanced AI systems.153

Supporting U.S. AI Development and Deployment

In lieu of, or in addition to, new requirements on AI, Congress might consider providing federal agencies with additional authorities or direction to support domestic AI development. As one example, NIST has worked in partnership with private sector groups—including industry, technology trade associations, nonprofits, and civil society groups—to create resources such as the voluntary AI RMF, as directed by law.154 Such efforts might be expanded upon to direct adapting the AI RMF for certain sectors, uses, or groups, such as small businesses. Many comments on an AI Action Plan by the Trump Administration expressed support for NIST and the AI RMF.155 Along these lines, for example, some legislation has been introduced that would have directed NIST to work with other public and private sector organizations and to develop guidance and best practices for AI development—such as dataset and model training documentation; disclosures of security practices such as third-party assessments; and public reporting on AI systems' capabilities, limitations, and appropriate uses.156 In the 119th Congress, the Testing and Evaluation Systems for Trusted AI Act of 2025 (S. 1633) would direct NIST and the Department of Energy to establish testbeds to develop a strategy to assess, and eventually demonstrate, measurement standards for evaluating AI systems used by federal agencies, in coordination with a newly established public-private working group.157

Congress could provide new authorities to create regulatory sandboxes. Such sandboxes are frameworks set up by regulators where firms are exempt from legal risk of certain regulations and allowed to test novel products in the marketplace under close regulatory supervision. For example, the Consumer Financial Protection Bureau has used regulatory sandboxes in financial services.158 In the 118th Congress, bills were introduced that would have created sandboxes for AI projects at financial regulatory agencies (H.R. 9309/S. 4951, 118th) and would have created a new office in OMB to create a universal sandbox (S. 4919, 118th).

Legislation has been introduced in the 119th Congress (H.R. 2385) that seeks to codify and shape the National Artificial Intelligence Research Resource (NAIRR). According to statements from the bill's co-authors,159 the NAIRR, a two-year pilot program launched by NSF in January 2024 as part of an effort to "democratize access to critical resources necessary to power responsible AI discovery and innovation,"160 aims to support researchers and students, as well as small businesses and startups, and might increase domestic innovation and competition. The NAIRR pilot builds off of recommendations in a January 2023 report from a statutorily created NAIRR Task Force161 and previous direction from the revoked E.O. 14110.

Engaging with International Efforts to Regulate AI

Some policy experts and government officials have asserted that the ability of individual countries to support domestic growth of AI technologies may be impacted by the extent to which their regulatory and governance mechanisms are consistent with those of other countries.162 Such international alignment at a broad level may facilitate trade, improve regulatory oversight across countries, and enable international cooperation.163 In May 2023, G7 countries—including the United States, the UK, and the EU (as a "non-enumerated member")—agreed to prioritize AI governance collaborations, emphasizing the importance of forward-looking, risk-based approaches to AI development and deployment.164

Congress might consider whether to pursue collaborative efforts to regulate AI with like-minded countries, as through alignment on policy goals or direction to federal agencies, in line with previous executive orders from the Trump and Biden Administrations. Some experts have referred to an "imperative of global [AI] governance" as "irrefutable," pointing to global sourcing for AI's raw materials, such as critical minerals and training data, and international deployment of GPAI—including beneficial uses and any negative downstream impacts—across borders.165

At the same time as calls for international collaboration have been growing, so too have concerns about international competition in AI R&D and innovation. The laws and regulations that countries create for technologies such as AI can affect a country's competitiveness—a "multi-dimensional and somewhat nebulous concept" that "can include a wide array of context-specific factors"—not only through their presence or absence but through their quality and the effectiveness of their implementation.166 Other considerations can also influence competitiveness, including domestic factors (e.g., R&D funding, education and workforce training, infrastructure availability, and AI adoption by businesses)167 and international factors (e.g., trade agreements, foreign investments). These factors may offset one another to varying extents. For example, while China has implemented numerous AI regulations and is under supply chain constraints for AI infrastructure, the synergy between the government and industrial sector—including government guidance funds supporting AI startups—might be helping to offset potential innovation constraints from those regulations, leading through innovative startups such as DeepSeek.168

Congress could consider whether and how to balance such factors, such as through supporting international collaborations and U.S. AI development while acknowledging concerns about a potential "race to the bottom" where AI systems become unsafe in pursuit of rapid development, as some experts have warned.169 Some bills pertaining to international AI research and activities have been previously introduced, such as the International Artificial Intelligence Research Partnership Act of 2024 (H.R. 8700, 118th) and H.Res. 649 (118th).170 Additionally, the Future of Artificial Intelligence Innovation Act of 2024 (S. 4178, 118th) would have supported domestic AI safety research, as well as international coalitions on AI innovation, development, and alignment of standards development with "like-minded governments" of foreign countries.



AI Watch: Global regulatory tracker - United States

The US relies on existing federal laws and guidelines to regulate AI but aims to introduce AI legislation and a federal regulation authority. Until then, developers and deployers of AI systems will operate in an increasing patchwork of state and local laws, underscoring challenges to ensure compliance.
Insight
24 September 2025
27 min read
Laws/Regulations directly regulating AI (the “AI Regulations”)

Currently, there is no comprehensive federal legislation or regulations in the US that regulate the development of AI or specifically prohibit or restrict their use. President Trump has signaled a permissive approach to AI regulation, issuing an Executive Order for Removing Barriers to American Leadership in AI ("Removing Barriers EO") in January 2025, that rescinds President Biden's Executive Order for the Safe, Secure, and Trustworthy Development and Use of AI ("Biden EO").1 The Removing Barriers EO calls for federal departments and agencies to revise or rescind all policies, directives, regulations, and other actions taken by the Biden administration that are "inconsistent" with "enhanc[ing] America's global AI dominance." Many policies were already in place from the Biden EO and it remains to be seen what the extent of the changes will be.

In July 2025, the Trump administration published the America's AI Action Plan2 ("the Plan"), which identifies more than 90 federal policy actions, with an aim to secure US AI leadership in artificial intelligence and place innovation at the core of US AI policy. This approach contrasts with the more risk-focused approaches adopted by the European Union and certain state-level initiatives such as the Colorado AI Act. While the Plan prescribes various proposed incentives to businesses, the practical impact in states with existing or emerging AI regulatory frameworks remains uncertain.

The US Congress has been considering numerous AI bills covering a wide range of issues. It is unclear if the Republican-held Congress will use this as an opportunity to enact AI legislation or focus on other priorities. That said, many of the proposed bills emphasize the development of voluntary guidelines and best practices for AI systems, reflecting a cautious approach to regulation aimed at fostering innovation without imposing strict mandates. This approach is influenced by concerns over stifling technological progress and maintaining competitiveness, particularly against countries like China (which produces approximately four STEM graduates for every STEM graduate in the US).

Existing US federal laws have limited application to AI. A non-exhaustive list of key examples includes:

    Federal Aviation Administration Reauthorization Act, which includes language requiring review of AI in aviation.3
    National Defense Authorization Act for Fiscal Year 2019, which directed the Department of Defense to undertake various AI-related activities, including appointing a coordinator to oversee AI activities.4
    National AI Initiative Act of 2020, which focused on expanding AI research and development and created the National Artificial Intelligence Initiative Office that is responsible for "overseeing and implementing the US national AI strategy."5
    Nevertheless, various frameworks and guidelines exist to guide the regulation of AI, including:

The AI Action Plan has a deregulation and pro-innovation agenda. It recommends that the Office of Management and Budget work with federal agencies to assess states' AI regulatory environments when making federal funding decisions, ensuring resources are not provided to states with restrictive legal frameworks. However, it is unclear how much impact this recommendation will have or what it will look like in practice. The AI Action Plan also emphasizes the Trump administration's key objective of enhancing the United States' AI infrastructure for geopolitical leadership while protecting against foreign adversary threats. Central to this strategy is the goal of exporting the full AI technology stack, including hardware, models, software and applications to countries willing to join a proposed "AI Alliance." While this would create opportunities for US businesses to expand into new markets, businesses may also need to reevaluate their supply chains, partnership structures and compliance programs to avoid inadvertently granting adversaries or entities of concern access to controlled AI technologies. Another notable aspect of the AI Action Plan (also set forth in the Executive Order "Preventing Woke AI in the Federal Government")6 is the update to the federal procurement guidelines to ensure that only "unbiased" large language models (i.e., considered free from "ideological dogmas such as DEI" and other "partisan or ideological judgments)" be eligible for government use. Therefore, AI companies engaging in government contracting, or whose products may otherwise be evaluated under the forthcoming guidelines on ideological neutrality, should closely monitor developments in this area.

The White House Blueprint for an AI Bill of Rights, issued under Biden, asserts guidance around equitable access and use of AI systems.7 The AI Bill of Rights provides five principles and associated practices to help guide the design, use and deployment of "automated systems" including safe and effective systems; algorithmic discrimination and protection; data privacy; notice and explanation; and human alternatives, consideration and fallbacks. While the Removing Barriers EO did not specifically revoke the AI Bill of Rights, the Trump Administration may be less likely to pursue the development of principles set out in the AI Bill of Rights, to the extent these principles are perceived as "inconsistent" with "enhanc[ing] America's global AI dominance." Nevertheless, AI developers may keep these principles in mind when designing AI systems.

Several leading AI companies – including Adobe, Amazon, Anthropic, Cohere, Google, IBM, Inflection, Meta, Microsoft, Nvidia, Open AI, Palantir, Salesforce, Scale AI, Stability AI – have voluntarily committed to "help move toward safe, secure, and transparent development of AI technology."8 These companies committed to internal/external security testing of AI systems before release, sharing information on managing AI risks and investing in safeguards.

The Federal Communications Commission issued a declaratory ruling stating that the restrictions on the use of "artificial or pre-recorded voice" messages in the 1990s era Telephone Consumer Protection Act include AI technologies that generate human voices, demonstrating that regulatory agencies will apply existing law to AI.9

The Federal Trade Commission (FTC), under the Biden administration, had signaled an aggressive approach to use its existing authority to regulate AI.)10 The FTC issued a warning to market participants that it may violate the FTC Act to use AI tools that have discriminatory impacts, make claims about AI that are not substantiated, or to deploy AI before taking steps to assess and mitigate risks.11 The FTC has already taken enforcement action against various companies that have deceived or otherwise harmed consumers through AI.12 As discussed below, the FTC has notably banned Rite Aid from using AI facial recognition technology without reasonable safeguards.13 That said, the AI Action Plan directs the FTC to review, and, where appropriate, seek to modify or set aside investigations, orders, consent decrees and injunctions from prior administration that may unduly burden AI innovation. Therefore, it remains to be seen how aggressive the FTC will be on AI under the Trump administration.
Status of AI-specific legislation

On September 12, 2023, the US Senate held public hearings regarding AI,14 which laid out potential forthcoming AI regulations. Possible legislation could include requiring licensing and creating a new federal regulatory agency. Additionally, US lawmakers held closed-door listening sessions with AI developers, technology leaders and civil society groups on September 13, 2023 in a continued push to understand and address AI.15

There are several proposed federal laws related to AI. A non-exhaustive list of key examples includes:

SANDBOX Act16, which seeks to establish a federal "regulatory sandbox" for AI developers to apply for waivers or modifications on compliance with federal regulations in order to test, experiment with, or temporarily offer AI products and services.

The SAFE Innovation AI Framework,17 which is a bipartisan set of guidelines for AI developers, companies and policymakers. This is not a law, but rather a set of principles to encourage federal law-making on AI.

The REAL Political Advertisements Act,18 which aims to regulate generative AI in political advertisements.

The Stop Spying Bosses Act,19 which aims to regulate employers surveilling employees with machine learning and AI techniques.

The Draft No FAKES Act,20 which would protect voice and visual likenesses of individuals from unauthorized recreations from Generative AI.

The AI Research Innovation and Accountability Act,21 which calls for greater transparency, accountability and security in AI, while establishing a framework for AI innovation. It would create an enforceable testing and evaluation standard for high-risk AI systems and require companies that use high-risk AI systems to produce transparency reports. It also empowers the National Institute of Standards and Technology to issue sector-specific recommendations to regulate them.

The American Privacy Rights Act, which would create a comprehensive consumer privacy framework.22 The draft bill includes provisions on algorithms, including a right to opt-out of covered algorithms used to make or facilitate consequential decisions.

House Republicans had included a provision in the "One Big Beautiful Bill Act" (enacted July 4, 2025) proposing a 10-year moratorium on state and local AI regulations. This move purportedly aimed to establish uniform federal oversight but sparked bipartisan opposition from many state lawmakers concerned about losing the ability to address AI-related harm locally. Ultimately, on July 1, 2025, the Senate voted 99-1 to remove the proposed moratorium – there was broad agreement among lawmakers and consumer protection advocates that the provision was unreasonably vague and likely to spur a wave of litigation about its scope and impact. Various lawmakers also expressed concerns that the moratorium would undermine efforts to regulate AI for purposes such as improving children's online safety and preventing deceptive trade practices.

Absent federal legislation, we can expect to see more state laws, and state legislatures have already introduced a substantial number of bills aimed at regulating AI, notably:

    On May 17, 2024, Colorado enacted the first comprehensive US AI legislation, the Colorado AI Act. The Act creates duties for developers and for those that deploy AI. Unlike certain state privacy laws, there is no revenue threshold for applicability – the Act applies to all developers and deployers of high-risk AI systems in Colorado. The Act focuses on automated decision-making systems and defines a covered high-risk AI system as one that "when deployed, makes, or is a substantial factor in making a consequential decision" that has a material legal or similarly significant effect on the provision or denial to any consumer of, or the cost or terms of: education, employment, essential government services, healthcare, housing, insurance, and legal services. There is a specific focus on bias and discrimination, and developers and deployers must use reasonable care to avoid discrimination via AI systems that make, or are a substantial factor in making a consequential decision in the above enumerated fields. The Act will go into effect in 2026. Before the law takes effect, the state legislature will host special legislative sessions to address the fiscal impact of the Act and practical considerations relating to its implementation. This will offer an opportunity to respond to industry concerns and clarify some of the legislative requirements.23
    The Colorado AI Act is emerging as a key template for state AI regulation. For example, state legislatures in Connecticut, Massachusetts, New Mexico, New York, and Virginia are considering bills that would generally track the Colorado AI Act and impose safeguards against bias by AI systems.
    In September 2024, California enacted various AI bills relating to transparency, privacy, entertainment, election integrity, and government accountability. Some of the key laws include:
        Assembly Bill 2655 (in effect): Defending Democracy from Deepfake Deception Act:24 requires large online platforms to identify and block the publication of materially deceptive content related to elections in California during specified time periods before and after an election. Additionally, under this Act, large online platforms must label – within 72 hours of notice – certain content as inauthentic, fake, or false during specified time periods before and after an election in California.
        Assembly Bill 1836 (in effect): Use of Likeness: Digital Replica Act:25 establishes a cause of action for beneficiaries of deceased celebrities to recover damages for the unauthorized use of an AI-created digital replica of the celebrity in audiovisual works or sound recordings. This Act requires deployers of AI systems to obtain the consent of a deceased personality's estate before producing, distributing, or making available the digital replica of a deceased personality's voice or likeness in an expressive audiovisual work or sound recording.
        Note: In March 2024, Tennessee enacted a similar bill entitled the Ensuring Likeness, Voice, and Image Security Act ("ELVIS Act") that expands personality rights by prohibiting the unauthorized use of AI to mimic a person's name, photograph, voice, or likeness, without a license.26

        Senate Bill 942: California AI Transparency Act (effective January 1, 2026):27 mandates that "Covered Providers" (AI systems that are publicly accessible within California with more than one million monthly visitors or users) implement comprehensive measures to disclose when content has been generated or modified by AI. This Act outlines requirements for AI detection tools and content disclosures, and establishes licensing practices to ensure that only compliant AI systems are permitted for public use. Covered Providers that violate the Act are liable for a penalty of US$5,000 per violation per day.

        Note: State legislatures in Washington and Virginia are currently considering similar AI transparency bills.
        Assembly Bill 2013 (effective January 1, 2026): Generative AI: Training Data Transparency Act:28 mandates that developers of generative AI systems (GenAI) publish a "high-level summary" of the datasets used to develop and train GenAI systems. For example, developers of GenAI systems would need to publish a summary of the following information, which is non-exhaustive:
            Sources and owners of the datasets
            Description of how the datasets further the intended purpose of the GenAI system
            Whether the datasets include any information protected by IP law
            Whether the datasets include personal information as defined in the CCPA
            Whether the datasets were purchased or licensed by the developer
        Assembly Bill 3030 (in effect): Health Care Services: Artificial Intelligence Act:29 requires health care providers that use GenAI to generate patient communications to (i) disclaim that the communication was generated by a GenAI system, and (ii) provide clear instructions for how the patient can contact a human health care provider for assistance. Where the GenAI communication has been reviewed by a human health care provider, the disclaimer requirements do not apply.
        Other bills governing AI across a range of fields include:
            Assembly Bill 2602 (in effect): Contracts against Public Policy: Personal or Professional Services: Digital Replica Act30
            Bill 896 (in effect): Generative Artificial Intelligence Accountability Act31
            Assembly Bill 2885 (in effect): Unified Definition of Artificial Intelligence
    In May 2025, the California Privacy Protection Agency (CPPA) Board finalized the long-awaited rules on cybersecurity, risk assessments and automated decision-making technologies (ADMT) ("the Proposed CPPA Regulations"32). The CPPA must now submit the Proposed CPPA regulations to the Office of Administrative Law, which has 30 working days to review them for compliance with the Administrative Procedure Act. If approved and finalized, the Proposed CCPA regulations would impose new obligations on businesses, including providing consumers with the right to opt out of ADMT in contexts involving "significant decisions" (e.g., in housing, employment, credit, or healthcare) that replace or substantially replace human decision-making. Businesses would also be required to conduct risk assessments when engaging in certain high-risk data practices, such as selling or sharing personal information, processing sensitive personal information, using ADMT for significant decisions, using personal data to train ADMT, or using profiling technologies in education or employment-related contexts. Notably, the CPPA Board members cautioned that the Proposed CCPA Regulations are intended to be responsive to how technology is currently used and could be revisited if they do not meet the moment.
    Since the August 29, 2025 deadline for state legislature fiscal committees to advance bills has passed, the list of active AI-related bills in California has narrowed. Bills that advanced and remain active include:
        AB 1018: requires developers to conduct performance evaluations of covered automated decision systems (that make consequential decisions) before deployment.
        SB 53: requires developers of "foundation models" deemed to present a "critical risk" to create, follow, and publish a safety and security protocol, including catastrophic risk testing for foundation models and monitoring for critical safety incidents.
        SB 243: aims to counter the marketing of chatbots as a recourse to loneliness and/or mental health struggles.
    Bills that failed at committee stage include:
        SB 420: would have required notification when automated systems made certain decisions about individuals, as well as a right to human review in certain instances.
        AB 412: would have required generative AI developers to disclose copyrighted training data and provide rights owners with a mechanism to request certain information.
    In May 2024, the Utah Artificial Intelligence Policy Act33 went into effect. The Act requires individuals and entities to disclose the use of GenAI in communications with consumers.
        For individuals and entities that engage in "regulated occupations" (i.e., those who must obtain a license or state certification to practice the occupation, such as lawyers or health care providers), the disclosure must be made "prominently" at the beginning of any communication with the consumer, regardless of whether the consumer asks whether they are dealing with a GenAI system (i.e., a proactive disclosure obligation).
        For individuals and entities that do not engage in "regulated occupations," the disclosure must be made "clearly and conspicuously" only if the consumer asks whether they are dealing with a GenAI system (i.e., a reactive disclosure obligation).
        Non-compliant individuals and entities may be fined up to US$2,500 per violation by the Utah Division of Consumer Protection.
        Enactment of the Utah Artificial Policy Act and the California Health Care Services: Artificial Intelligence Act both underscore legislative concern with ensuring that consumers know when they are dealing with GenAI systems in certain settings.
    More than 40 state AI bills were introduced in 2023, with Connecticut34 and Texas35 actually adopting statutes. Both of those enacted statutes establish state working groups to assess state agencies' use of AI systems to ensure they do not result in unlawful discrimination.
    In Texas, Governor Greg Abbott signed the Texas Responsible AI Governance Act (TRAIGA) into law on June 22, 2025. Although the original bill mirrored the more expansive Colorado and EU AI Acts, Texas lawmakers significantly narrowed its scope during the legislative process. The final version eliminates many private sector obligations (e.g., impact assessments and consumer disclosures) and limits most compliance obligations to government use of AI. However, TRAIGA still imposes categorical restrictions on the development and deployment of AI systems for certain purposes such as behavioral manipulation, unlawful discrimination, and infringement of constitutional rights. TRAIGA also provides that AI systems may not be developed or distributed with the sole intent of producing, assisting or aiding in producing, or distributing child pornography or unlawful deepfake videos or images. Intentionally developing or distributing an AI system that engages in explicit text-based conversations while impersonating a child under the age of 18 is also prohibited. TRAIGA takes effect on January 1, 2026.

As for international commitments, on September 5, 2024, the United States joined Andorra, Georgia, Iceland, Norway, the Republic of Moldova, San Marino, the United Kingdom, Israel, and the European Union to sign the Council of Europe's Framework Convention36 on AI. The treaty will enter into force on the first day of the month following three months after five signatories, including at least three Council of Europe Member States, have ratified it. Countries from all over the world will be eligible to join and commit to its provisions. However, given changes to US AI policy under President Trump, the US may withdraw from the Framework Convention on AI or otherwise decline to adhere to the Framework.
Other laws affecting AI

Existing legislation has been the primary way in which the US regulates AI as established law, including privacy and intellectual property laws, which are generally applicable to AI technologies.

Notably, in April 2023, the Federal Trade Commission, Equal Employment Opportunity Commission, Consumer Financial Protection Bureau, and Department of Justice issued a joint statement noting that "existing legal authorities apply to the use of automated systems and innovative new technologies."37 As cited above, in February 2024, the Federal Communications Commission applied restrictions in the Telephone Consumer Protection Act on AI-generated voices.

Several states have enacted comprehensive privacy legislation that can also regulate AI. A non-exhaustive list of notable state legislation includes:

    The California Privacy Protection Act (CPPA), which regulates automated decision-making.38
    The Minnesota Consumer Data Privacy Act (which took effect on July 31, 2025) grants consumers additional rights compared to other state privacy laws, such as the ability to question profiling decisions and access the data used in those decisions. The consumer may also review their data used in the profiling and correct inaccurate data for reevaluation.
    The Biometric Information Privacy Act in Illinois,39 which is very broad and allows for extremely high damages for violations. There is currently pending litigation in the AI context Existing intellectual property laws also apply to AI, both with respect to the data AI technologies are trained upon and the outputs of such technologies. For example, with respect to outputs, the US District Court has held that human authorship is an essential part of a valid copyright claim, and the Copyright Office will refuse to register a work unless it was created by a human being."40 There are also numerous cases before the courts in the US alleging copyright infringement, among other things, with respect to training data. On the product liability front, there is a growing number of lawsuits against large language model developers on issues such as defective design and deceptive business practices.

Definition of “AI” 

There is no single definition of AI.

The National Artificial Intelligence Initiative and Removing Barriers EO define AI as "a machine-based system that can, for a given set of human-defined objectives, make predictions, recommendations, or decisions influencing real or virtual environments. Artificial intelligence systems use machine- and human-based inputs to perceive real and virtual environments; abstract such perceptions into models through analysis in an automated manner; and use model inference to formulate options for information or action."41

Many state privacy bills have different definitions of automated decision-making technology or "profiling":

    A recent Texas statute establishing an AI advisory council (HB 2060) defines an "automated decision system" as "an algorithm, including an algorithm incorporating machine learning or other artificial intelligence techniques, that uses data-based analytics to make or support governmental decisions, judgments or conclusions"42
    Connecticut's Public Act No. 22-15 defines "profiling" as "any form of automated processing performed on personal data to evaluate, analyze or predict personal aspects related to an identified or identifiable individual's economic situation, health, personal preferences, interests, reliability, behavior, location or movements"43
    The CCPA defines "profiling" as "any form of automated processing of personal information, [...] to evaluate certain personal aspects relating to a natural person and in particular to analyze or predict aspects concerning that natural person's performance at work, economic situation, health, personal preferences, interests, reliability, behavior, location, or movements."44
        Additionally, recently enacted California Assembly Bill 10084543 clarifies that the CCPA applies to consumers' "personal information" regardless of its format. Specifically, AB 100845 clarifies that the CCPA encompasses "personal information" contained in "abstract digital formats" (i.e., generative AI systems that are capable of outputting consumers' personal information).
        Further, recently enacted California Senate Bill 122346 clarifies that "sensitive personal information" under the California Privacy Rights Act (CPRA) encompasses consumers' neural data. As with AB 1008, SB 1223 aims to keep pace with emerging technology (in this case, neurotechnology) in an effort to protect information about consumers' brain and nervous system functions. While SB 1223 does not articulate a specific nexus to AI systems, if signed into law, it would constrain developers and deployers from using neural data under the CPRA.
    The recently finalized CPPA's rules on cybersecurity, risk assessments and ADMT defines "ADMT" as "any technology that processes personal information and uses computation to replace human decision-making or substantially replace human decision-making" (note: the rules defines "substantially replace human decision-making" as a business using the technology's output to make a decision without human involvement).

Territorial scope

As noted above, there are currently no comprehensive federal laws that have been enacted to specifically regulate AI. Accordingly, there is no specific territorial scope of federal legislation. However, many existing statutes regulate activities in which AI can be used, and those federal statutes typically apply nationally and, in some cases, extra-territorially. State legislation regulating AI generally has extra-territorial effect as its application typically extends to entities that target its residents from within or outside the state.
Sectoral scope

As noted above, there are currently no comprehensive federal laws that directly regulate AI. Accordingly, there is no specific federal sectoral scope at this stage. Nevertheless, there are certain sector-specific frameworks that have been implemented in the US to regulate the use of AI. A non-exhaustive list of key examples includes:

    In the insurance sector, the National Association of Insurance Commissioners issued a model bulletin47 that focuses on governance frameworks, risk management protocols and testing methodologies that insurers should have in place to govern their use of AI systems that impact insurance consumers. Once adopted by the NAIC (expected early 2024), state insurance departments could use the bulletin at their discretion as the bulletin is not new law, but instead enforces the application of current laws to insurers' use of AI and serves as guidance as to regulatory expectations
    In the employment sector, the City of New York enacted Local Law 144 of 202148 that "prohibits employers and employment agencies [in the city] from using an automated employment decision tool unless the tool has been subject to a bias audit within one year of the use of the tool, information about the bias audit is publicly available, and certain notices have been provided to employees or job candidates"49

Compliance roles

As noted above, there is currently no comprehensive federal legislation in the US that directly regulates AI. Accordingly, there are currently no specific or unique federal obligations imposed on developers, users, operators and/or deployers of AI systems. However, developers, users, operators and deployers of AI systems should anticipate that existing law will apply to any regulated activity that uses AI, and consult legal counsel about the potential liabilities that may arise. While potentially novel, the use of AI does not per se provide a shield from the application of existing law.
Core issues that the AI regulations seek to address

As noted above, there is currently no comprehensive legislation in the US that directly regulates AI. However, proposed legislation at the federal and state level generally seeks to address the following issues:

    Safety and security
    Responsible innovation and development
    Equity and unlawful discrimination
    Protection of privacy and civil liberties

Risk categorization

As noted above, there is currently no comprehensive legislation in the US that directly regulates AI. AI is also not generally classified according to risk in the relevant frameworks and principles.
Regulators

Currently, there is no AI-specific federal regulator in the US. However, in April 2023, the Federal Trade Commission, Equal Employment Opportunity Commission, Consumer Financial Protection Bureau and Department of Justice issued a joint statement clarifying that their authority applies to "software and algorithmic processes, including AI."50

Similarly, state regulators that regulate privacy legislation likely also have the authority to regulate AI vis-à-vis existing privacy provisions. The FTC has been active in this area, and we can expect to see more from them going forward; see discussion of Rite Aid below.
Enforcement powers and penalties

As noted above, there are currently no comprehensive federal laws or regulations in the US that have been enacted specifically to regulate AI. As such, enforcement and penalties relating to the creation, dissemination and/or use of AI are governed by application of existing law to situations involving AI, through regulatory or judicial application of non-AI-specific federal and state statutes or AI-specific state privacy legislation.

In addition, the Federal Trade Commission has evoked an interest in and focus on regulating AI through enforcement. On December 19, 2023, the FTC settled a significant action focused on artificial intelligence bias and discrimination against Rite Aid regarding the company's use of facial recognition technology for retail theft deterrence. This illustrative case provides guidance on the FTC's enforcement on AI systems. For example, the proposed consent order51 between Rite Aid and the FTC:

    Prohibits Rite Aid from using AI facial recognition for five years
    Requires Rite Aid to delete all photos and videos of consumers used in its AI facial recognition
    Specifies that after Rite Aid's ban on using AI facial recognition expires, if Rite Aid operates AI facial recognition technology for surveillance, it must maintain a comprehensive automated biometric security or surveillance system monitoring program that identifies and addresses the risks of such operation and notifies consumers of its use of AI facial recognition. Rite Aid must also provide a means for consumers to lodge complaints, and investigate and respond to all complaints received, among other requirements

With respect to Colorado AI Act, the Colorado Attorney General has rule-making authority to implement, and exclusive authority to enforce, the requirements of the Act.52 A developer or deployer who violates the Act is deemed to engage in unfair or deceptive trade practices.

Enforcement mechanisms and penalties vary under the different California AI bills. Bills that specifically provide for enforcement include:

    Senate Bill 942: California AI Transparency Act: provides for penalties of US$5,000 per violation per day, enforceable through civil action by the California Attorney General, city attorneys, or county counsel
    Assembly Bill 3030: Health Care Services: Artificial Intelligence Act: enforceable by the Medical Board of California and Osteopathic Medical Board of California, with non-compliance punishable by, inter alia, civil penalties, suspension or revocation of a medical license, and administrative fines as set out in the California Health and Safety Code
    Assembly Bill 2655: Defending Democracy from Deepfake Deception Act: the California Attorney General, any district attorney, or any city attorney may seek injunctive relief to compel removal of materially deceptive content

Artificial Intelligence 2025 Legislation
Updated July 10, 2025
Related Topic:

AI—the use of computer systems to perform tasks that normally require human intelligence, such as learning and decision-making—has the potential to spur innovation and transform industry and government. As AI advances and widespread adoption of these tools increase, government, business and the public are exploring the risks and benefits of using the system for different applications.

State governments across the country are starting to use or examine how AI can be used to improve government services such as enhancing customer service; improving health care facility inspections; and improving roadway safety. Legislators, industry and other stakeholders have engaged in robust discussions regarding the concerns about potential misuse or unintended consequences of AI.

More and more states have introduced AI-related legislation over the last few years. In the 2025 legislative session, all 50 states, Puerto Rico, the Virgin Islands, and Washington, D.C., have introduced legislation on this topic this year. Thirty-eight states adopted or enacted around 100 measures this year. Examples of those actions include:

    Arkansas enacted legislation that clarifies who the owner of AI generated content is, which includes the person who provides data or input to train a generative AI model or an employer, if the content is generated as a part of employment duties. The new law specifies that the generated content should not infringe on existing copyright or intellectual property rights.

    Montana's new, "Right to Compute," law sets requirements for critical infrastructure that is controlled by an AI system, such as instructing the deployer to develop a risk management policy that considers guidance from a list of specified standards, such as the latest version of the AI risk management framework from the national institute of standards and technology. The new law also specifies that the government cannot take actions that restrict the ability to privately own or make use of computational resources for lawful purposes, unless deemed necessary to fulfill a compelling government interest.

    New Jersey adopted a resolution urging generative AI companies to make voluntary commitments regarding employee whistleblower protections.

    New York enacted a new law that requires state agencies to publish detailed information about their automated decision-making tools on their public websites through an inventory created and maintained by the Office of Information Technology. The new law also amends the civil service law to strengthen worker protections, such as requiring when an AI system is used by the state government that it cannot affect the existing rights of employees pursuant to an existing collective bargaining agreement and requiring that an AI system does not result in displacement or loss of a position.

    North Dakota's new law prohibits individuals from using an AI-powered robot to stalk or harass other individuals, expanding current harassment and stalking laws.

    Oregon enacted a new law that specifies that a non-human entity including an agent powered by AI cannot use specific licensed and certified medical professionals' titles, such as a registered nurse and certified medication aide.

This webpage covers key legislation related to AI issues generally. Legislation related solely to specific AI technologies, such as facial recognition, deepfakes or autonomous vehicles, is being tracked separately.


REGULATING UNDER UNCERTAINTY: Governance Options for Generative AI
by Florence G'sell

    DOWNLOAD FULL REPORT

 
Image
abstract image of lines swirling
About the Report

 
Although innovation in AI has occurred for many decades, the two years since the release of ChatGPT have been marked by an exponential rise in development and attention to the technology. Unsurprisingly, governmental policy and regulation has lagged behind the fast pace of technological development. Nevertheless, a wealth of laws, both proposed and enacted, have emerged around the world. 

 
The title of this report – “Regulating Under Uncertainty: Governance Options for Generative AI” – seeks to convey the unprecedented position of governments as they confront the regulatory challenges AI poses. Regulation is both urgently needed and unpredictable. It also may be counterproductive, if not done well. However, governments cannot wait until they have perfect and complete information before they act, because doing so may be too late to ensure that the trajectory of technological development does not lead to existential or unacceptable risks. The goal of this report is to present all of the options that are “on the table” now with the hope that all stakeholders can begin to establish best practices through aggressive information sharing. The risks and benefits of AI will be felt across the entire world. It is critical that the different proposals emerging are assembled in one place so that policy proponents can learn from one another and move ahead in a cooperative fashion. 